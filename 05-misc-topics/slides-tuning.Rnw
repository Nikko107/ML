%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{17}{Tuning}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Introduction}

  \begin{itemize}
    \item Many settings of an ML algorithm are not modified by the
      (usually loss-minimizing) fitting procedure.
    \item Our goal is to optimize these w.r.t. the estimated prediction error (often this implies an independent test set), or by cross-validation.
    \item This also applies to preprocessing, feature engineering and any other model-relevant operations. In general, we might be interested in optimizing an entire machine learning \enquote{pipeline}.
  \end{itemize}

  \framebreak

  \begin{blocki}{Model parameters vs. hyperparameters}
    \item \textbf{Model parameters} are optimized during training 
    \item e.g., for a CART, the splits and the leaf predictions are model parameters.
    \item \textbf{Hyperparameters} are parameters that must be specified before training the model
    \item e.g., for a CART, which impurity measure to use to evaluate possible splits or the maximal depth of the tree need to be set before training begins.
    %regularized regresssion is a bad example here because we don't teach it in this course.
  \end{blocki}

\framebreak

Strategies for finding hyperparameter settings:

  \begin{itemize}
    \item If the algorithm is insensitive w.r.t. to changes of a parameter, we don't really have to do anything as long as we stay within a broad range of reasonable values.
    \item Constant default: We can benchmark the algorithm across a broad range of data sets and scenarios and try to find a constant value that works well in many different situations. Quite optimistic?
    \item Dynamic (heuristic) default: We can benchmark the algorithm across a broad range of data sets and scenarios and try to find an easily computable function that sets the parameter in a data dependent way, e.g. \texttt{mtry}$ = p/3$. %or setting the kernel width of an RBF SVM w.r.t. the distance distribution of training data points.\\
      How to find that heuristic function?
    \item Set the hyperparameter by extracting relevant info from the fitted model, e.g.
      %early stopping in boosting or 
      varying \texttt{ntree} for a random forest (does OOB error increase or stagnate?)
      %, or some regularized models allow full-path computation for whole sequences of $\lambda$ values.
      %regularized regresssion, boosting bad examples here because we don't teach it in this course.
  \end{itemize}

  \framebreak

  \begin{blocki}{Why tuning is important:}
  \item Hyperparameters control the capacity of a model, i.e., how flexible the model is, how many degrees of freedom it has to adapt to the training data.
  \item If our model is too flexible and adapts to the training data too much
  we will overfit.
  \item Hence, control of capacity, i.e., proper setting of hyperparameters
  prevents overfitting the model w.r.t. the training set.
  \item Many other choices like the type of kernel, preprocessing, etc., can
    heavily influence model performance in non-trivial ways. It is often impossible to guess the optimal settings.
  \end{blocki}

  \framebreak

  \begin{blocki}{Types of hyper parameters:}
    \item Numerical parameters (real valued / integers)
    \begin{itemize}
      \item $k$ for kNN
      \item Depth, node-size of a tree
    \end{itemize}
    \item Categorical parameters:
    \begin{itemize}
      \item Which split criterion for trees?
      \item Which distance measure to use for kNN?
    \end{itemize}
    \item Ordinal parameters:
    \begin{itemize}
      \item $\{$\texttt{low}, \texttt{medium}, \texttt{high}$\}$
    \end{itemize}
    \item Dependent parameters: Hyperparameters whose possible values / existence depends on other hyperparameters.
    %\begin{itemize}
    %  \item Kernel parameter, according to the kernel
    %\end{itemize}
  \end{blocki}

  \framebreak

  \begin{blocki}{What our tuning problem consists of:}
    \item Our learning method %(or are there actually several?)
    \item The performance measure. Determined by the application. Challenging if problem requires non-standard measure unlike, say, classification error or MSE. In general, we could be interested in multiple measures at once.
    \item Resampling procedure for measuring the performance. How do we choose it?
    \item All (hyper)parameters plus their regions-of-interest for optimization.
  \end{blocki}

  \framebreak

  \begin{blocki}{Some general remarks on tuning}
    \item Lots of literature exists for models, far less on efficient tuning.
    \item This optimization problem is derivative-free, we can only evaluate the
      performance of selected hyperparameter configurations (black-box problem).
    \item This optimization problem is stochastic -- we want to optimize expected performance and use resampling to approximate this.
    \item Evaluation of the target function will usually take quite some time:
      we are cross-validating complex models on large data sets.
    \item Categorical and dependent hyperparameters increase the prioblem's difficulty: complex search space.
    \item For large/difficult problems, parallelizing the computation seems relevant.
  \end{blocki}

\end{vbframe}


% \begin{vbframe}{Offline- vs. online-tuning}
% 
%   \begin{blocki}{Offline-tuning:}
%     \item Learn optimal parameter settings before solving an instance.
%     \item Tuning on training instances
%   \end{blocki}
% 
%   \lz
% 
%   \begin{blocki}{Online-tuning:}
%     \item Learn optimal parameter settings during solving.
%     \item No training phase
%   \end{blocki}
% 
% \end{vbframe}
% 
% \begin{vbframe}{Offline configuration}
% \begin{center}
% \includegraphics{figure_man/offline_configuration.png}
% 
% {\tiny St√ºtzle and Lopez-Ibanez, Automatic (Offline) Configuration of Algorithms, 2014.}
% \end{center}
% \end{vbframe}
 
% does this belong here? seems fairly irrelevant to what comes before/after?

\begin{vbframe}{Tuning with grid search}

  \begin{itemize}
    \item Offline tuning technique which is still quite popular.
    \item For each parameter a finite set of candidates is predefined.
    \item Then, one we simply search the Cartesian product of all possible combinations.
    \item All solutions are searched in an arbitrary order.
    \item Extension: Start with a coarse grid and iteratively refine it around a detected optimum.
  \end{itemize}

\framebreak

<<>>=
lrn = makeLearner("classif.ksvm", predict.type = "prob")
# ps = makeParamSet(
#   makeDiscreteParam("C", values = 2^(-3:3)),
#   makeDiscreteParam("sigma", values = 2^(-3:3)))
set.seed(1)

ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlGrid()

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

caption_grid <- sprintf("Grid search over %d grid points. Best MMCE: %1.2f",
                        nrow(opt.grid), min(opt.grid$mmce.test.mean))

gridSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
gridSearch +  
  geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + 
  scale_size(range = c(3,15)) + 
  scale_x_continuous("Parameter 1", limits = c(-3, 3)) + 
  scale_y_continuous("Parameter 2", limits = c(-3, 3)) + 
  labs(caption = caption_grid)
@

\framebreak

  \begin{blocki}{Advantages:}
    \item Very easy to implement, therefore very popular.
    \item All parameter types possible.
    \item Parallelization is trivial.
  \end{blocki}

  \begin{blocki}{Disadvantages}
    \item Combinatorial explosion, inefficient
    \item Searches large irrelevant areas.
    \item Which values / discretization?
  \end{blocki}

\end{vbframe}

\begin{vbframe}{Tuning with random search}

  \begin{itemize}
    \item Small variation of grid search.
    \item Instead of evaluating all parameter configurations,
      we randomly sample from the region-of-interest.
    \item This often drastically reduces optimization time.
    \item Performs just as well as normal grid search in surprisingly
    many applications.
  \end{itemize}

\framebreak

<<>>=
set.seed(1)
ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlRandom(maxit = 50)

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

caption_rnd <- sprintf("Random search over %d grid points. Best MMCE: %1.2f",
                        nrow(opt.grid), min(opt.grid$mmce.test.mean))

rndSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
rndSearch + 
  geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + 
  scale_size(range = c(3,15)) + 
  scale_x_continuous("Parameter 1", limits = c(-3, 3)) + 
  scale_y_continuous("Parameter 2", limits = c(-3, 3)) + 
  labs(caption = caption_rnd)
@


  % \framebreak
  %
  % \begin{blocki}{Why it works:}
  %   \item Imagine the $5\%$ interval around the optimum.
  %   \item Now we sample points out of the parameter space.
  %   \item Each point has a $5\%$-chance of falling into this interval.
  %   \item If we draw $n$-points, then the probability that at least one of the points falls into the desired interval is:
  %   $$ 1 - (1 - 0.95)^n $$
  %   \item We can calculate the number of draws it takes to have a $95\%$-chance of success:
  %   $$
  %   1 - (1 - 0.95)^n > 0.95 \quad \Longleftrightarrow \quad n \geq 60
  %   $$
  % \end{blocki}

\end{vbframe}

\endlecture
