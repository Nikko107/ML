% ml - bagging, random forest
\newcommand{\bl}[1][m]{b^{[#1]}} % baselearner, default m
\newcommand{\blh}[1][m]{\hat{b}^{[#1]}} % estimated base learner, default m 
\newcommand{\blx}[1][m]{b^{[#1]}(\xv)} % baselearner, default m

\newcommand{\blfmh}{\hat{f}^{[m]}} % estimated baselearner: scores
\newcommand{\blfmhx}{\blfmh(\xv)} % estimated baselearner: scores of x
\newcommand{\blhmh}{\hat{h}^{[m]}} % estimated baselearner: hard labels
\newcommand{\blhmhx}{\blhmh(\xv)} % estimated baselearner: hard labels of x
\newcommand{\blpmh}{\hat \pi^{[m]}} % estimated baselearner: probabilities
\newcommand{\blpmhxk}{\hat \pi_{k}^{[m]}(\xv)} % estimated baselearner: probabilities of x for class k

\newcommand{\fM}{f^{[M]}(\xv)} % ensembled predictor
\newcommand{\fMh}{\hat f^{[M]}(\xv)} % estimated ensembled predictor
\newcommand{\ambifM}{\Delta\left(\fM\right)} % ambiguity/instability of ensemble
\newcommand{\betam}[1][m]{\beta^{[#1]}} % weight of basemodel m
\newcommand{\betamh}[1][m]{\hat{\beta}^{[#1]}} % weight of basemodel m with hat
\newcommand{\betaM}{\beta^{[M]}} % last baselearner
\newcommand{\summM}{\sum_{m=1}^M} % sum over m=1 to M baselearners
\newcommand{\avgmM}{\frac{1}{M} \sum_{m=1}^M} % averaging over m=1 to M baselearners

\newcommand{\ib}{\mathrm{IB}} % In-Bag (IB)
\newcommand{\ibm}{\ib^{[m]}} % In-Bag (IB) for m-th bootstrap
\newcommand{\oob}{\mathrm{OOB}} % Out-of-Bag (OOB)
\newcommand{\oobm}{\oob^{[m]}} % Out-of-Bag (OOB) for m-th bootstrap


% ml - boosting
\newcommand{\fm}[1][m]{f^{[#1]}} % prediction in iteration m
\newcommand{\fmh}[1][m]{\hat{f}^{[#1]}} % prediction in iteration m
\newcommand{\fmd}[1][m]{f^{[#1-1]}} % prediction m-1
\newcommand{\fmdh}[1][m]{\hat{f}^{[#1-1]}} % prediction m-1
\newcommand{\errm}[1][m]{\text{err}^{[#1]}} % weighted in-sample misclassification rate
\newcommand{\wm}[1][m]{w^{[#1]}} % weight vector of basemodel m
\newcommand{\wmi}[1][m]{w^{[#1](i)}} % weight of obs i of basemodel m
\newcommand{\thetam}[1][m]{\thetab^{[#1]}} % parameters of basemodel m
\newcommand{\thetamh}[1][m]{\hat{\thetab}^{[#1]}} % parameters of basemodel m with hat
\newcommand{\blxt}[1][m]{b(\xv, \thetab^{[#1]})} % baselearner, default m
\newcommand{\ens}{\sum_{m=1}^M \betam \blxt} % ensemble
\newcommand{\rmm}[1][m]{\tilde{r}^{[#1]}} % pseudo residuals
\newcommand{\rmi}[1][m]{\tilde{r}^{[#1](i)}} % pseudo residuals
\newcommand{\Rtm}[1][m]{R_{t}^{[#1]}} % terminal-region
\newcommand{\Tm}[1][m]{T^{[#1]}} % terminal-region
\newcommand{\ctm}[1][m]{c_t^{[#1]}} % mean, terminal-regions
\newcommand{\ctmh}[1][m]{\hat{c}_t^{[#1]}} % mean, terminal-regions with hat
\newcommand{\ctmt}[1][m]{\tilde{c}_t^{[#1]}} % mean, terminal-regions
\newcommand{\Lp}{L^\prime}
\newcommand{\Ldp}{L^{\prime\prime}}
\newcommand{\Lpleft}{\Lp_{\text{left}}}

% ml - boosting iml lecture
\newcommand{\ts}{\thetab^{\star}} % theta*
\newcommand{\bljt}{\bl[j](\xv, \thetab)} % BL j with theta
\newcommand{\bljts}{\bl[j](\xv, \ts)} % BL j with theta*
