\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
% \input{common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/reg_class_task_1}
\newcommand{\learninggoals}{
\item Understand the definition of multiclass classification
\item Understand how to extend logistic regression to softmax regression}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Einführung in das Statistische Lernen}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








%! includes: basics-supervised, basics-datatask

\lecturechapter{Multiclass Classification}
\lecture{Einführung in das Statistische Lernen}

\begin{vbframe}{From logistic regression ...}

    Remember \textbf{logistic regression} ($\Yspace = \{0, 1\}$): We combined the hypothesis space of linear functions, transformed by the logistic function $s(z) = \frac{1}{1 + \exp(- z)}$

    \vspace*{-0.3cm}

    \begin{eqnarray*}
      \Hspace = \left\{\pi: \Xspace \to \R ~|~\pix = s(\thetab^\top \xv)\right\}
    \end{eqnarray*}

    with the Bernoulli (logarithmic) loss:

    \begin{eqnarray*}
      L(y, \pix) = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right).
    \end{eqnarray*}

    \vfill

    \begin{footnotesize}
      \textbf{Remark:} We suppress the intercept term for better readability. The intercept term can be easily included via $\thetab^\top \tilde\xv$, $\thetab \in \R^{p + 1}$, $\tilde\xv = (1, \xv)$.
    \end{footnotesize}

    \end{vbframe}

    \begin{vbframe}{... to softmax regression}

    There is a straightforward generalization to the multiclass case:

    \begin{itemize}
      \item Instead of a single linear discriminant function we have $g$ linear discriminant functions
        $$
          f_k(\xv) = \thetab_k^\top \xv, \quad k = 1, 2, ..., g,
        $$
      each indicating the confidence in class $k$.
      \item The $g$ score functions are transformed into $g$ probability functions by the \textbf{softmax} function $s:\R^g \to \R^g$

      $$
        \pi_k(\xv) = s(\fx)_k = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) }
      $$
      instead of the \textbf{logistic} function for $g = 2$. The probabilities are well-defined: $\sum \pi_k(\xv) = 1$ and $\pi_k(\xv) \in [0, 1]$ for all $k$.

      \item The softmax function is a generalization of the logistic function. For $g = 2$, the logistic function and the softmax function are equivalent.

      \item Instead of the \textbf{Bernoulli} loss, we use the multiclass \textbf{logarithmic loss}
       $$
        L(y, \pix) = - \sum_{k = 1}^g \mathds{1}_{\{y = k\}} \log\left(\pi_k(\xv)\right).
      $$
        \item Note that the softmax function is a \enquote{smooth} approximation of the arg max operation,
            so $s((1, 1000, 2)^T) \approx (0, 1, 0)^T$ (picks out 2nd element!).
        \item Furthermore, it is invariant to constant offsets in the input:
          \end{itemize}
        $$
        s(\fx + \mathbf{c}) = \frac{\exp(\thetab_k^\top \xv + c)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv + c)} =
        \frac{\exp(\thetab_k^\top \xv)\cdot \exp(c)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) \cdot \exp(c)} =
        s(\fx)
        $$


    \end{vbframe}

    \begin{vbframe}{Logistic vs. softmax regression}

    \begin{scriptsize}
    \begin{table}[]
    \bgroup
    \def\arraystretch{2}%  1 is the default, change whatever you need
    \begin{tabular}{ccc}
    & Logistic Regression & Softmax Regression \\ \hline
    $\Yspace$ & $\{0, 1\}$ & $\{1, 2, ..., g\}$ \\[0.5cm]
    Discriminant fun. & $f(\xv) = \thetab^\top \xv$ & $f_k(\xv) = \thetab_{k}^{\top} \xv, k = 1, 2, ..., g$ \\[0.5cm]
    Probabilities & $\pi(\xv) = \frac{1}{1 + \exp\left(-\thetab^\top \xv\right)}$ & $\pi_k(\xv) = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) }$ \\[0.5cm]
    $L(y, \pix)$ & Bernoulli / logarithmic loss & Multiclass logarithmic loss\\[-0.3cm]
    & $-y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)$  & $ - \sum_{k = 1}^g [y = k] \log\left(\pi_k(\xv)\right)$ \\
    \end{tabular}
    \egroup
    \end{table}
    \end{scriptsize}

    \end{vbframe}

    \frame{
    \frametitle{Logistic vs. softmax regression}
    We can schematically depict softmax regression as follows:

    \only<1>{
    \begin{center}
      \includegraphics[width=0.6\textwidth]{figure_man/softmax2.png}
    \end{center}
    }

    \only<2>{
    \begin{center}
      \includegraphics[width=0.6\textwidth]{figure_man/softmax1.png}
    \end{center}
    }
    }

    \begin{vbframe}{Logistic vs. softmax regression}

    Further comments:

    \begin{itemize}

    \item We can now, for instance, calculate gradients and optimize this with standard numerical optimization software.

    % \item For linear $\fxt = \theta^T \xv$ this is also called \emph{softmax regression}.

    \item Softmax regression has an unusual property in that it has a \enquote{redundant} set of parameters. If we subtract a fixed vector
      from all $\thetab_k$, the predictions do not change at all.
      I.e.,  our model is \enquote{over-parameterized}. For any hypothesis we might fit,
      there are multiple parameter vectors that give rise to exactly the same hypothesis function.
      This also implies that the minimizer of $\risket$ above is not unique (but $\risket$ is convex)!
      Hence, a numerical trick is to set $\theta_g = 0$ and only optimize the other $\theta_k$.

    \item A similar approach is used in many ML models: multiclass LDA, naive Bayes, neural networks and boosting.

    \end{itemize}

    \end{vbframe}

    \begin{vbframe}{Softmax: Linear discriminant functions}

    Softmax regression gives us a \textbf{linear classifier}.

    \begin{itemize}
      \item The softmax function $s(\bm{z})_k = \frac{\exp(\bm{z}_k)}{\sum_{j = 1}^g \exp \left(\bm{z}_j\right)}$ is

    \begin{itemize}
      \item a rank-preserving function, i.e. the ranks among the elements of the vector $\bm{z}$ are the same as among the elements of $s(\bm{z})$. This is because softmax transforms all scores by taking the $\exp(\cdot)$ (rank-preserving) and divides each element by \textbf{the same} normalizing constant.
    \end{itemize}

    Thus, the softmax function has a unique inverse function $s^{-1}: \R^g \to \R^g$ that is also monotonic and rank-preserving. Applying $s_k^{-1}$ to $\pi_k(\xv) = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^n \thetab_j^\top \xv}$ gives us $f_k(\xv) = \theta_k^\top \xv$. Thus softmax regression is a linear classifier.
    \end{itemize}

    \end{vbframe}

    \endlecture

\end{document}
