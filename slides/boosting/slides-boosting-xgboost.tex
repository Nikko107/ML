\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure_man/split-finding02.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{XGBoost}
\lecture{Introduction to Machine Learning}

% sources: https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
% sources: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d
% sources: https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda/

\begin{vbframe}{Motivation}

\pkg{XGBoost} (short for eXtreme Gradient Boosting) is an open-source and scalable tree boosting system \textbf{(Chen and Guestrin 2016)}.

\lz

\begin{itemize}
  \item Efficient implementation in \emph{C++} with interfaces to many other programming languages.
  \item Parallel approximate split finding.
  \item Additional regularization techniques.
  \item Feature constraints.
  \item Cluster and GPU support.
\end{itemize}

\lz

\pkg{XGBoost} (and related implementations) are highly optimized and powerful machine learning frameworks and often achieve top performance in benchmarks and machine learning challenges.


\end{vbframe}

\begin{vbframe}{Loss minimization}

\pkg{XGBoost} uses a risk function with 3 regularization terms:

\begin{multline*}
  \riskr^{[m]} = \sum_{i=1}^{n} L\left(\yi, \fmd(\xi) + \bmm(\xi)\right)\\
   + \lambda_1 J_1(\bmm) + \lambda_2 J_2(\bmm) + \lambda_3 J_3(\bmm),
\end{multline*}

\lz

with $J_1(\bmm) = T^{[m]}$ the number of leaves in the tree to penalize tree depth.

\lz

$J_2(\bmm) = \left\|\mathbf{c}^{[m]}\right\|^2_2$ and $J_3(\bmm) = \left\|\mathbf{c}^{[m]}\right\|_1$ are $L2$ and $L1$ penalties of the terminal region values $c_t^{[m]}, t=1,\dots,T^{[m]}$.

\lz

We define $J(\bmm) := \lambda_1 J_1(\bmm) + \lambda_2 J_2(\bmm) + \lambda_3 J_3(\bmm)$.

\framebreak

%Recall the way a tree base learner $\bmm(\xv)$ can be fitted loss-optimally in gradient boosting:

%$$
%\tilde{\mathbf{c}}^{[m]} = \argmin_{(c_1,\dots,c_{T^{[m]}})}\sum_{i = 1}^n L(\yi, \fmd(\xi) + \bmm(\xi, c_1,\dots,c_{T^{[m]}})).
%$$

%The direction of steepest descent for the update is then

%$$
%-\frac{\partial L(y, \fmd(\xv) + \bmm(\xv))}{\partial \bmm(\xv)}
%$$

%for each $\xi, i = 1,\dots, n$.

%\lz

%\textbf{Note:} $J(\bmm)$ is omitted for now but will be re-introduced later.

%\framebreak

To approximate the loss in iteration $m$, a second-order Taylor expansion around $\fmd(\xv)$ is computed:
\vskip -1em
\begin{align*}
  &L(y, \fmd(\xv) + \bmm(\xv)) \approx \\
  &\qquad L(y, \fmd(\xv)) + g^{[m]}(\xv)\bmm(\xv) + \frac12 h^{[m]}(\xv)\bmm(\xv)^2,
\end{align*}

with gradient

$$
g^{[m]}(\xv) = \frac{\partial L(y, \fmd(\xv))}{\partial \fmd(\xv)}
$$

and Hessian

$$
h^{[m]}(\xv) = \frac{\partial^2 L(y, \fmd(\xv))}{\partial {\fmd(\xv)}^2}.
$$

\textbf{Note:} $g^{[m]}(\xv)$ are the negative pseudo-residuals $-\rmm$ we use in standard gradient boosting to determine the direction of the update.

\framebreak

Since $L(y, \fmd(\xv))$ is constant, the optimization simplifies to

\begin{align*}
\riskr^{[m]} = &\sum_{i = 1}^n g^{[m]}(\xi)\bmm(\xi) + \frac12 h^{[m]}(\xi)\bmm(\xi)^2 + J(\bmm) + const\\
\propto&\sum_{t=1}^{T^{[m]}}\sum_{\xi\in R_t^{[m]}} g^{[m]}(\xi)c^{[m]}_t + \frac12 h^{[m]}(\xi)(c^{[m]}_t)^2 + J(\bmm) \\
=& \sum_{t=1}^{T^{[m]}}G^{[m]}_t c^{[m]}_t+\frac12 H^{[m]}_t (c^{[m]}_t)^2 + J(\bmm).
\end{align*}

Where $G^{[m]}_t$ and $H^{[m]}_t$ are the accumulated gradient and Hessian values in terminal node $t$.



\framebreak

Expanding $J(\bmm)$:
\begin{align*}
\riskr^{[m]} = &\sum_{t=1}^{T^{[m]}}\left(G^{[m]}_t c^{[m]}_t+\frac12 H^{[m]}_t (c^{[m]}_t)^2 + \frac12\lambda_2(c^{[m]}_t)^2 + \lambda_3 |c^{[m]}_t|\right) + \lambda_1 T^{[m]}\\
=&\sum_{t=1}^{T^{[m]}}\left(G^{[m]}_t c^{[m]}_t+\frac12 (H^{[m]}_t + \lambda_2) (c^{[m]}_t)^2 + \lambda_3 |c_t^{[m]}|\right) + \lambda_1 T^{[m]}.
\end{align*}

\lz

\textbf{Note:} The factor $\frac12$ is added to the $L2$ regularization to simplify the notation as shown in the second step.
This does not impact estimation since we can just define $\lambda_2 = 2\tilde\lambda_2$.

\framebreak

Computing the derivative for a terminal node constant value $c^{[m]}_t$ yields

$$
\frac{\partial \riskr^{[m]}}{\partial c^{[m]}_t} = (G^{[m]}_t + \sign{(c^{m}_t)}\lambda_3) + (H^{[m]}_t + \lambda_2) c^{m}_t.
$$

The optimal constants $\hat{c}^{[m]}_1,\dots, \hat{c}^{[m]}_{T^{[m]}}$ can then be calculated as

\lz
$$
\hat{c}^{[m]}_t = - \frac{t_{\lambda_3}\left(G^{[m]}_t\right)}{H^{[m]}_t + \lambda_2}, t=1,\dots T^{[m]},
$$
with $$t_{\lambda_3}(x) = \begin{cases}
  x + \lambda_3 &\text{ for } x < - \lambda_3 \\
  0  &\text{ for } |x| \leq \lambda_3 \\
  x - \lambda_3 &\text{ for } x > \lambda_3.
\end{cases}$$

\end{vbframe}

\begin{vbframe}{Loss minimization - split finding}

To evaluate the performance of a candidate split that divides the instances in region $R_t^{[m]}$ into a left and right node we use the \textbf{risk reduction} achieved by that split:
$$
\tilde S_{LR} =
 \frac12 \left[
 \frac{t_{\lambda_3} \left( G^{[m]}_{tL} \right)^2}{H^{[m]}_{tL} + \lambda_2} + \frac{t_{\lambda_3}\left(G^{[m]}_{tR}\right)^2}{H^{[m]}_{tR} + \lambda_2} - \frac{t_{\lambda_3}\left(G^{[m]}_{t}\right)^2}{H^{[m]}_{t} + \lambda_2}
 \right] - \lambda_1,
$$
where the subscripts $L$ and $R$ denote the left and right leaves after the split.

% derivation for this: write out change in loss function, case distinction acording to sign of t_a(G)/(H + lambda_2)

\lz

\framebreak

\begin{algorithm}[H]

\begin{footnotesize}
\begin{center}

  \begin{algorithmic}[1]
    \State \textbf{Input} $I$: \emph{instance set of current node}
    \State \textbf{Input} $p$: \emph{dimension of feature space}
    \State $gain \gets 0$
    \State $G \gets \sum_{i \in I} g(\xi), {H} \gets \sum_{i \in I} h(\xi)$
    \For{$j = 1 \to p$}
      \State $G_L \gets 0, {H}_L \gets 0$
      \For{$i$ in sorted($I$, by $x_{j}$)}
        \State ${G}_L \gets {G}_L + g(\xi), {H}_L \gets {H}_L + h(\xi)$
        \State ${G}_R \gets G - {G}_L, {H}_R \gets {H} - {H}_L$
        \State compute $\tilde S_{LR}$
      \EndFor
    \EndFor
    \State \textbf{Output} Split with maximal $\tilde S_{LR}$
  \end{algorithmic}
\end{center}
\end{footnotesize}
\caption{(Exact) Algorithm for split finding}
\end{algorithm}

\end{vbframe}

\begin{vbframe}{Tree Growing}

  \pkg{XGBoost} grows \emph{balanced} trees with a maximum depth \texttt{max\_depth}.

  \lz

  This means that no comparison of risk reduction between different nodes is done and every leaf is split until \texttt{max\_depth} is reached.

  \lz

  Leaves are split even if no positive $\tilde S_{LR}$ can be found.

  \lz

  Only after the tree is fully expanded each split that did not improve $\tilde S_{LR}$ is pruned.

\end{vbframe}


\begin{vbframe}{Approximate split-finding algorithms}

To speed up the tree building for large datasets (in $n$) it is possible to speed up the split finding:

\lz

Instead of iterating over all possible split points (exact greedy algorithm) only a predefined number of splits $l$ per feature is considered.

\lz

Usually these are percentiles of the empirical distribution of each feature, hence referred to as \textbf{Histogram-based Gradient Boosting}.

\lz

These percentiles can be computed once (global approximate algorithm) or recomputed after each split (local approximate algorithm).

\lz

To speed up the computation for the percentiles a \emph{weighted quantile-sketch} can be used as an efficient approximation.


\framebreak

Comparison of global (left) and local (right) histogram-based approximate split finding for feature $x_1$.

\lz

\begin{small}
\begin{minipage}[b]{0.49\textwidth}
\begin{figure}
  \includegraphics[width=\textwidth]{figure_man/split-finding01.png}
  \caption*{Global approximation.}
\end{figure}

\end{minipage}
\begin{minipage}[b]{0.49\textwidth}
  \begin{figure}
    \includegraphics[width=\textwidth]{figure_man/split-finding02.png}
    \caption*{Local approximation.}
  \end{figure}
\end{minipage}
\end{small}

\lz

Blue lines indicate percentiles that are considered as split points and the red line is the selected split.

\framebreak

\begin{algorithm}[H]
\begin{footnotesize}
\begin{center}
  \begin{algorithmic}[1]
    \For{$j = 1 \to p$}
      \State Define possible split proposals $S_j = \{s_{j}^{(1)}, s_{j}^{(2)}, \hdots, s_{j}^{(l)}\}$ by percentiles on feature $j$.
      \State Proposal can be done once per tree (global), or in each node (local).
    \EndFor
    \For{$j = 1 \to p$}
      \State ${G}_{kv} \gets \sum_{i \in \{i|s_j^{(v)} \geq x_j^{(i)} > s_{k}^{(v - 1)}\}} g(\xi)$
      \State ${H}_{kv} \gets \sum_{i \in \{i|s_j^{(v)} \geq x_j^{(i)} > s_{k}^{(v - 1)}\}} h(\xi)$
    \EndFor
    \State Follow same steps as exact algorithm to find max score only among proposed splits.
  \end{algorithmic}
\end{center}
\end{footnotesize}
\caption{Approximate algorithm for split finding}
\end{algorithm}

\end{vbframe}

\begin{vbframe}{Feature Subsampling}

Similar to the \texttt{mtry} hyperparameter in a random forest feature subsampling can be used for gradient boosting as well.

\lz

The fraction of features for a split can be randomly sampled for each


\begin{enumerate}
  \item tree,
  \item level of a tree, or
  \item split.
\end{enumerate}

\lz

\begin{itemize}
  \item Feature subsampling speeds up training even further and can creates a more diverse ensembles that will often perform better.
  \item Subsampling rates are usually defined as a fraction of features $[0,1]$ instead of an absolute number.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Dropout Additive-Regression Trees}

ToDo

\end{vbframe}


\begin{vbframe}{Parallelism and GPU Computation}

ToDo

\end{vbframe}

%\begin{vbframe}{Summary}
%
%XGBoost is an extremely powerful method, but also hard to configure correctly.
%Overall, eight hyperparameters have to be set, which is difficult to do in practice and almost always requires tuning.
%
%\lz
%
%Different split finding algorithms can be selected, which allows XGBoost to be efficient even on very large datasets.
%
%\lz
%
%A large number of different regularization strategies is included to prevent overfitting.
%
%
%\end{vbframe}
%
%\begin{vbframe}{Comparison of major boosting systems}
%
%\begin{tiny}
%\begin{table}[]
%\centering
%\begin{tabular}{l|c|c|c|c|c|c}
%System       & Exact algo. & Approx. algo. & Sparsity-aware & Variable importance & Parallel & Language   \\
%\hline
%ada          & yes         & no            & no             & no                  & no       & R          \\
%GBM          & yes         & no            & partially      & yes                 & no       & R          \\
%mboost       & yes         & no            & no             & no                  & no       & R          \\
%compboost    & yes         & no            & yes            & yes                 & yes      & R          \\
%H2O          & no          & yes           & partially      & yes                 & yes      & R (Java)   \\
%XGBoost      & yes         & yes           & yes            & yes                 & yes      & R + Python \\
%lightGBM     & no          & yes           & yes            & yes                 & yes      & R + Python \\
%catboost     & no          & yes           & no             & yes                 & yes      & R + Python \\
%scikit-learn & yes         & no            & no             & yes                 & no       & Python     \\
%pGBRT        & no          & no            & no             & no                  & yes      & Python     \\
%Spark MLLib  & no          & yes           & partially      & yes                 & yes      & R, Python, \\
%             &             &               &                &                     &          & Java, Scala\\
%
%\end{tabular}
%\label{my-label}
%\end{table}
%\end{tiny}
%
%\lz
%
%\textbf{Note:} H2O is a commercial software written in Java with a solid R interface.
%In the free version only two CPUs can be used.
%
%%\framebreak
%%
%%We compare the performance in terms of accuray and runtime on five example data sets from OpenML.
%%
%%\lz
%%
%%All boosting algorithms use $100$ iterations, a learning rate of $0.1$ and a maximum tree depth of $4$ (except for mboost which uses linear models as base-learner).
%%
%%\lz
%%
%%We also compare to a random forest as a base-line.
%%
%%\framebreak
%%<<echo=FALSE, fig.height=5>>=
%%load("rsrc/benchmark.RData")
%%plotBMRBoxplots(bmr, facet.wrap.ncol = 4)
%%plotBMRBoxplots(bmr, measure = timetrain, facet.wrap.ncol = 4)
%%@
%
%%\framebreak
%
%%Overall XGBoost performs well and is on par with commercial software like H2O.
%
%%\lz
%
%%The random forest is hard to beat in this benchmark. This is due to the fact that we did not do any tuning of the boosting hyperparameters.
%%While this is quite important for boosting algorithms, a random forest is not as sensitive to its hyperparameters.
%
%%\lz
%
%%mboost with boosted linear models is overall worse than the other algorithms, but has the advantage of better interpretability.
%
%\end{vbframe}
%
\endlecture
\end{document}
