\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\input{../../latex-math/ml-ensembles.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Feature Importance
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/forest-fimp_perm.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand that the goal of defining feature importance is to enhance interpretability of the random forest
  \item Understand feature importance based on improvement in split criterion
  \item Understand feature importance based on permutations of OOB observations
}

\begin{vbframe}{Feature Importance}

RFs improve accuracy by aggregating multiple decision trees but \textbf{lose interpretability} compared to a single tree. \textbf{Feature importance} helps rectify lost interpretability.
\\
\begin{itemize}
  \item We can obtain a $\widehat{\mathrm{GE}}$ of an RF by its performance on OOB samples.
  \item Measuring how much the performance \textit{decreases} if a specific feature were removed or rendered useless is called \textbf{permutation importance}.
\end{itemize}

\vspace{-1ex}
\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
\includegraphics[width=0.8\textwidth]{figure_man/forest-fimp_idea.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Permutation importance}

\includegraphics[width = 0.95\textwidth]{figure_man/forest-permutation-imp.png}
\vspace{-1.2ex}
\begin{algorithm}[H]
\small
\caption*{Measure based on permutations of OOB obs.}
\begin{algorithmic}[1]
  \State Calculate $\widehat{\mathrm{GE}}_{\text{OOB}}$ using set-based metric $\rho$
  \For{features $x_j$, $j = 1 \to p$}
    \State {Distort feature-target relation: permute $x_j$ with $\psi_j$}
	  \For{distorted observations $(\xi_{\psi_j}, \yi), i = 1 \to n$}
		\State Compute {$\yih_{\text{OOB},\psi_j}$}
	  \EndFor
    \State {Compute $\widehat{\mathrm{GE}}_{\text{OOB},\psi_j} = \rho(\yv, \hat{\yv}_{\text{OOB},\psi_j})$}
  \State {Estimate importance of $j$-th feature: $\widehat{\text{FI}_j} = \widehat{\mathrm{GE}}_{\text{OOB},\psi_j} - \widehat{\mathrm{GE}}_{\text{OOB}}  $}
  \EndFor
\end{algorithmic}
\end{algorithm}
\vspace{-3ex}
\end{vbframe}

\begin{vbframe}{Impurity importance}

{\small Another way to measure feature importance is to add up all \textit{improvements} in the splitting criteria (e.g., Gini index) for nodes where feature $x_j$ is used.}

\vspace{-1ex}
\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
\includegraphics[width=0.8\textwidth]{figure_man/forest-fimp_impurity.png}
\end{center}

\vspace{-3ex}
\begin{algorithm}[H]
\small
\caption*{Measure based on improvement in split criterion}
\begin{algorithmic}[1]
  \For{features $x_j$, $j = 1 \to p$}
  \For{tree base learners $\blh$, $m = 1 \to M$}
  \State {Find all nodes $\Np$ in $\blh$ that use $x_j$.} 
  \State {Compute improvement in splitting criterion achieved by them.}
  \State {Add up these improvements.}
  \EndFor
  \State {Add up improvements over all trees to get feature importance of $x_j$.}
  \EndFor
\end{algorithmic}
\end{algorithm}

\end{vbframe}

\begin{vbframe}{In practice}

{\small
Comparing permutation importance, i.e., the average increase of MSE for predictions of OOB observations after permuting the $j$-th feature of mtcars with the total decrease in node impurity:
}
% \vspace{-1ex}
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-fimp_gini.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-fimp_perm.png}
\end{minipage}
\end{figure}
% \vspace{-1ex}

{\small
In practice, both methods are \textbf{biased toward features with more levels} (i.e., continuous or categorial with many categories) for tasks with predictor variables of varying types. \citebutton{C. Strobl, A.-L. Boulesteix, et. al., 2007}{https://pubmed.ncbi.nlm.nih.gov/17254353/}
}

\end{vbframe}

\endlecture
\end{document}
