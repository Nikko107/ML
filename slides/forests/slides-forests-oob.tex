\documentclass[11pt,compress,t,notes=noshow,xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest.png}
\newcommand{\learninggoals}{
\item Understand the concept of out-of-bag and in-bag observations
\item Learn how out-of-bag error provides an unbiased estimate of the generalization error during training}
\usepackage{../../style/lmu-lecture}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Random Forest: Out-of-Bag Error Estimate}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Out-of-bag vs in-bag observations}
  
\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
  \includegraphics[width=0.9\textwidth]{figure_man/forest-oob.png}
\end{center}

\begin{itemize}
  \item Let $\text{OOB}^{[m]}$ denote the set 
  $\left \{i \in \nset | (\xv^{(i)}, \yi) \text{ is OOB for } \bl \right \}$.
  \item The number of trees for which the $i$-th observation is OOB is then 
  given by $S_{\text{OOB}}^{(i)} = 
  \sum_{m = 1}^M \I(i \in \text{OOB}^{[m]})$.
  \item OOB size: $\P(i \in \text{OOB}^{[m]}) = \left(1 - \frac{1}{n}\right)^n 
  \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx 0.37$ for 
  $i \in \nset$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{the Out-of-Bag Error Estimate}
  
\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
  \includegraphics[width=0.75\textwidth]{figure_man/forest-oob-error.png}
\end{center}

\begin{itemize}
  \item We can estimatate the generalization error $\widehat{\mathrm{GE}}$ by using the $i$-th observation as test data for all trees $\blh$ for which it is OOB. 
  \item The proportion of OOB samples that were incorrectly classified by our RF is our $\widehat{\mathrm{GE}}$!
\end{itemize}

\end{vbframe}

\begin{vbframe}{Out-Of-Bag error Pseudo Code}
\begin{algorithm}[H]
  \footnotesize
  \caption*{Out-Of-Bag error calculation}
  \begin{algorithmic}[1]
    \State {\bf Input: } $\text{OOB}^{[m]}$, ensemble
    \For {$i = 1 \to n$}
      \State Compute the ensemble OOB prediction for observation $i$, e.g.:
      $$\yih_{\text{OOB}} = \begin{cases}
      \frac{1}{S_{\text{OOB}}^{(i)}} \sum_{m = 1}^{M} 
      \I(i \in \text{OOB}^{[m]}) \cdot \hat{y}^{(i)[m]} & \text{in regression,} 
      \\ \phantom{x} \\
      \argmax_{k \in \gset} 
      \frac{\sum_{m = 1}^{M} \I(i \in \text{OOB}^{[m]}) \cdot 
      \I(\hat{h}^{(i)[m]} = k)} {S_{\text{OOB}}^{(i)}} &
      \text{in classification.}
      \end{cases}$$
    \EndFor
    \State Take the average of the resulting point-wise losses to estimate the generalization error of the forest: $$\widehat{\mathrm{GE}}_{\text{OOB}} = \meanin L(\yi, \yih_{\text{OOB}})$$
  
  \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{vbframe}{Using the Out-of-Bag Error Estimate}
This unbiased generalization error estimate can be accessed directly during training, making it possible to optimize hyperparameters like $\texttt{maxDepth, ntrees}$:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/forest-oob.pdf} 

}
\end{knitrout}

\end{vbframe}

\begin{vbframe}{Comparability Issues of OOB Error}

While the \textbf{OOB error} is an unbiased estimate of the generalization error for random forests, comparability issues arise when using OOB error to compare random forests with other models:

\begin{itemize}
\item \textbf{OOB error} is unique to random forests and uses bootstrap sampling.
\item Other models often use different validation methods (e.g., k-fold cross-validation), leading to different error estimates.
\end{itemize}

\textbf{OOB error} is best for:
\begin{itemize}
\item Comparing hyperparameters within random forests.
\item Rapidly evaluating different random forest configurations.
\end{itemize}
For comparing random forests with other models, use a consistent validation technique, like \textbf{cross-validation}, for all models!

\end{vbframe}

\endlecture
\end{document}