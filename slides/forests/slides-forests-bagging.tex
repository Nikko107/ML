\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/rf_majvot_averaging.png}
\newcommand{\learninggoals}{
\item Understand the basic idea of bagging
\item Be able to explain the connection of bagging and bootstrap
\item Understand how a prediction is computed for bagging
\item Understand why bagging improves the predictive power}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}


\lecturechapter{Random Forests: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Motivation}

CARTs offer several appealing features:

\begin{itemize}
  \item \textbf{Interpretability}: They are easy to understand and explain.
  \item \textbf{Invariance to linear transformations}: Their performance is unaffected by scaling or shifting of data.
  \item \textbf{Versatility}: They effectively hanlde both categorial and numerical data.
  \item \textbf{Robustness to missing values}: They can be designed to manage data with missing entries.
\end{itemize}

Despite these benefits, CARTs are not without their drawbacks:

\begin{quotation}
"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy."
\end{quotation}

\citebutton{Hastie et al., 1984, p. 352}{https://hastie.su.domains/Papers/ESLII.pdf}

We explore how to enhance the accuracy of decision trees while preserving their inherent advantages.

\end{vbframe}

\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., \textit{it combines many models into one 
        big \enquote{meta-model}}
  \item Such model ensembles often work much better than their members alone would.
  \item The constituent models of an ensemble are called \textbf{base learners} 
\end{itemize}

\framebreak 
In a \textbf{bagging} ensemble, all base learners are of the same type. The only difference between the models is the data they are trained on.\\
Specifically, we train base learners $\bl, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $M$ bootstrap samples to get models $\fh(x) = \blh, m = 1, \dots, M$
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1xodP6ayu1Gay6mMKgzVWYEFmSoeG5kNuqsaTkFFmd78/edit
\includegraphics[width=0.55\textwidth]{figure_man/bagging.pdf}
\end{center}

\framebreak

\textbf{Aggregate} the predictions of the $M$ fitted base learners to get the
\textbf{ensemble model} $\fMh$:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior class probabilities $\pikxh$ can be estimated by calculating predicted class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.6\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Bagging in practice}

Bagging can be used for any model:

\vspace{2em}

\begin{minipage}{0.4\textwidth}
\begin{center}
\textbf{Logistic Regression}
\end{center}

\includegraphics[width=120pt]{figure/bagging-bench_log.jpg}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{center}
\textbf{CART}
\end{center}

\includegraphics[width=120pt]{figure/bagging-bench_tree.jpg}
\end{minipage}

\end{vbframe}


\begin{vbframe}{Why/when does Bagging help?}

\begin{itemize}
  \item Bagging reduces the variability of predictions by averaging the outcomes from multiple base learner models.

  \item It is particularly effective when the errors of a base learner are mainly due to (random) variability rather than systematic issues.
\end{itemize}

For an in-depth explanation of the theory behind bagging, refer to the Deep Dive!



% In one sentence:\\
% \lz

% Because the variability of the average of the predictions of many base learner models is smaller than the variability of the predictions from one such base learner model.\\

% \vspace{2em}

% If the error of a base learner model is mostly due to (random) variability and not due to structural reasons, combining many such base learners by bagging helps reducing this variability.

\end{vbframe}

\begin{vbframe}{Improving Bagging}

Naturally, better-performing base learners lead to stronger aggregated outcomes. \\

\vspace{2em}

There are also bagging-specific knobs to tune:
\begin{itemize}
  \item \textbf{Benefits of larger ensembles}: Increasing the \textbf{number of base learners} improves performance (at least theoretically).
  \item \textbf{Advantages of diversity}: Diverse base learners contribute different perspectives, reducing the impact of any single model's bias or variance as long as they maintain overall accuracy.
  \item \textbf{Less correlation, more gain}: Bagging yiels better results when there is less correlation among the models. Independent errors help in cancelling out individual errors!
\end{itemize}
% \begin{small}
% \begin{align*}
% \Exy\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right]} - \Exy\left[\ambifM\right]\\
% \Exy\left[\ambifM\right] &\cong 
% \textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\bl\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\bl, \bl{m'}\right]\right)}
% \end{align*}
% \end{small}
% \begin{itemize}
% \item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
% \item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
% \item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
% \item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if base learners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
% \end{itemize}


\end{vbframe}

\begin{vbframe}{Bagging: Synopsis}

  \begin{itemize}
    \item Basic idea: fit the same model repeatedly on many \textbf{bootstrap} replications of the training data set and \textbf{aggregate} the results
    \item Gains performance by reducing the variance of predictions, but (slightly) increases the bias: it reuses training data many times, so small mistakes can get amplified. 
    \item Works best for unstable/high-variance base learners, where small changes in the training set can cause large changes in predictions:\\
    e.g., CART, neural networks, step-wise/forward/backward variable selection for regression\\
     \item Works best if base learners' predictions are only weakly correlated: they don't all make the same mistakes.
         \item Can degrade performance for stable methods like $k$-NN, LDA, Naive Bayes, linear regression
  \end{itemize}
\end{vbframe}

\endlecture
\end{document}
