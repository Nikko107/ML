\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/rf_majvot_averaging.png}
\newcommand{\learninggoals}{
\item Understand the basic idea of bagging
\item Be able to explain the connection of bagging and bootstrap
\item Understand how a prediction is computed for bagging
\item Understand when bagging improves the predictive power}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}


\lecturechapter{Random Forests: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., \textit{it combines many models into one big \enquote{meta-model}}
  \item Such model ensembles often work much better than their members alone would!
  \item The constituent models in an ensemble are called \textbf{base learners} and are all of the same type (e.g. CART)
  \item Each base learner is trained on a different \textbf{bootstrap} sample drawn with replacement from the original dataset $\D$
\end{itemize}

\framebreak 
Specifically, we train base learners $\bl, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $M$ bootstrap samples to get models $\fh(x) = \blh, m = 1, \dots, M$
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1xodP6ayu1Gay6mMKgzVWYEFmSoeG5kNuqsaTkFFmd78/edit
\includegraphics[width=0.55\textwidth]{figure_man/bagging.pdf}
\end{center}

\framebreak

\textbf{Aggregate} the predictions of the $M$ fitted base learners to get the
\textbf{ensemble model} $\fMh$:
  \begin{itemize}
    \item Regression: Aggregate via averaging
    \item Classification: Majority voting class frequencies or averaging the base learner's probabilities
    \item Posterior class probabilities $\pikxh$ can be estimated by calculating predicted class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.55\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Formalizing Bagging}

\begin{algorithm}[H]
  \footnotesize
  \caption*{Bagging algorithm: Training}
  \begin{algorithmic}[1]
    \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
    \For {$m = 1 \to M$}
      \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
      \State Train base learner on $\D^{[m]}$ to obtain model $\blh$
    \EndFor
    \State Save all base learners $\blh$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \footnotesize
  \caption*{Bagging algorithm: Prediction}
  \begin{algorithmic}[1]
    \State {\bf Input: } Observation $\xv$, trained base learners $\blh$
    \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
    \begin{center}
    $\fMh = \frac{1}{M} \sum_{m=1}^M \blh \textit{(averaging)}$
    $\quad \fMh = \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blh = k\right) \textit{(majority voting)}$
    \end{center}
  \end{algorithmic}
\end{algorithm}

\end{vbframe}


\begin{vbframe}{Bagging in practice}

Bagging can be used for any model:

\vspace{2em}

\includegraphics[width=220pt]{figure/bagging-bench.png}

\vspace{1em}

It seems that bagging is especially helpful for less stable learners.

\end{vbframe}


\begin{vbframe}{Why/when does Bagging help?}

\begin{itemize}
  \item Bagging reduces the variability of predictions by averaging the outcomes from multiple base learner models.

  \item It is particularly effective when the errors of a base learner are mainly due to (random) variability rather than systematic issues.
\end{itemize}

\begin{center}
\includegraphics[width=250pt]{figure/bagging-mean.png}
\end{center}

\footnotesize{For an in-depth explanation of the theory behind bagging, refer to the Deep Dive!}

\end{vbframe}

\begin{vbframe}{Improving Bagging}

Naturally, better-performing base learners lead to stronger aggregated outcomes. \\

\vspace{1em}

There are also bagging-specific knobs to tune:
\begin{itemize}
  \item \textbf{Benefits of larger ensembles}: Increasing the \textbf{number of base learners} improves performance (at least theoretically):
  \begin{center}
  \includegraphics[width=0.65\textwidth]{figure/bagging-ntree_MSE.png}
  \end{center}
  
  \vspace{2em}

  \item \textbf{Advantages of diversity}: Diverse base learners contribute different perspectives, reducing the impact of any single model's bias or variance.
  \begin{center}
  \includegraphics[width=0.55\textwidth]{figure/bagging-fit.png}
  \end{center}
\end{itemize}

$\Rightarrow$ How can we increase variability for tree-based ensembles? With Random Forests!

\end{vbframe}

\endlecture
\end{document}
