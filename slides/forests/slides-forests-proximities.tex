\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Proximities
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/forest-prox-vis_1.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand how a random forest can be used to define proximities of observations
  \item Know how proximities can be used for missing data, outliers, mislabeled data and a visualization of the forest
}

\begin{vbframe}{Proximities}
RFs have an in-built similarity measure between pairs of observations: 

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2e20bede50d_0_0
\includegraphics[width=1\textwidth]{figure_man/forest-proximities.png}
\end{center}

\begin{itemize}
  \item Once a random forest has been trained, all of the training data is put through each tree (both in- and out-of-bag).
  \item The proximity $\operatorname{prox}\left(\xi, \xi[j]\right)$
  between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the \textbf{same terminal node of the same tree}, divided by the number of trees: $\operatorname{prox}\left(\xi[1], \xi[2]\right) = 1/3$
  \item The proximities of all observations form a symmetric $n \times n$ matrix.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Visualizing the forest}

\vspace{-5ex}
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure_man/forest-prox-matrix.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-prox-vis_1.png}
\end{minipage}
\end{figure}
\vspace{-3ex}
% {\small
We can visualize the proximity matrix, e.g. for a 3-class penguin classification task, by projecting it onto a plane via multidimensional scaling (MDS):
\begin{itemize}
  \item Samples from the same class form \textbf{identifiable clusters}, which suggests that they share a common structure.
  \item \textbf{Highlights for which classes errors occur}, e.g. Adelie has high within-class variance and has overlaps with other classes.
\end{itemize}
% }

\end{vbframe}

\begin{vbframe}{Outliers}
\vspace{-6ex}
\begin{center}
\includegraphics[width=0.55\textwidth]{figure/forest-prox-vis_2.png}
\end{center}
\vspace{-4ex}

Proximities can be used to \textbf{locate outliers}, i.e. observations whose proximities to all other observations are small.

\begin{itemize}
\item Instances in the dataset are sometimes labeled ambiguously or incorrectly, especially in \enquote{manually} created data sets.
\item Outliers should be carefully inspected, as they may \textbf{be mislabeled}!
\end{itemize}
\end{vbframe}


\begin{frame}{Inputting missing data}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit
\only<1>{\includegraphics[width=0.74\textwidth]{figure_man/forest-missing-value_1.png}}
\only<2>{\includegraphics[width=0.74\textwidth]{figure_man/forest-missing-value_2.png}}
\only<3>{\includegraphics[width=\textwidth]{figure_man/forest-missing-value_3.png}}
\end{center}
\begin{enumerate}
\item Replace missing values for a given variable using the median of the non-missing values
\item Get proximities
\item Replace missing values in observation $\xi$ by a weighted average of non-missing values, with weights proportional to the proximity between observation $\xi$ and the observations with the non-missing values
\end{enumerate}
Steps 2 and 3 are then iterated a few times. % (5 to 6 times)
\end{frame}

\endlecture
\end{document}
