\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest 
  }{% Lecture title  
  Proximities
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/forest-proximity_plot.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand how a random forest can be used to define proximities of observations
  \item Know how proximities can be used for missing data, outliers, mislabeled data and a visualization of the forest
}

\begin{vbframe}{Proximities}
RFs have an in-built similarity measure between pairs of observations: 

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2e20bede50d_0_0
\includegraphics[width=1\textwidth]{figure_man/forest-proximities.png}
\end{center}

\begin{itemize}
  \item Once a random forest has been trained, all of the training data is put through each tree (both in- and out-of-bag).
  \item The proximity $\operatorname{prox}\left(\xi, \xi[j]\right)$
  between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the \textbf{same terminal node of the same tree}, divided by the number of trees: $\operatorname{prox}\left(\xi[1], \xi[2]\right) = 1/3$
  \item The proximities of all observations form a symmetric $n \times n$ matrix.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Visualizing the forest}

\vspace{-2ex}
\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/forest-proximity_plot.png}
\end{center}
\vspace{-1ex}
{\small
The figure \citebutton{Louppe, 2014}{arXiv:1407.7502} depicts the proximity matrix for a 10-class handwritten digit classification task, projected onto the plane via multidimensional scaling:
\begin{itemize}
  \item Samples from the same class form identifiable clusters, which suggests that they share a common structure
  \item Highlights for which classes errors occur, e.g. digits 1 and 8 have high within-class variance and have overlaps with other classes 
\end{itemize}
}

\end{vbframe}

\begin{vbframe}{Inputing missing data}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit
\includegraphics[width=0.74\textwidth]{figure_man/forest-missing_value.png}
\end{center}
\begin{enumerate}
\item Replace missing values for a given variable using the median of the non-missing values
\item Get proximities
\item Replace missing values in observation $\xi$ by a weighted average of non-missing values, with weights proportional to the proximity between observation $\xi$ and the observations with the non-missing values
\end{enumerate}
Steps 2 and 3 are then iterated a few times. %(5 to 6 times)
\end{vbframe}

\begin{vbframe}{Outliers}
\vspace{-2ex}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit
\includegraphics[width=0.74\textwidth]{figure_man/forest-mislabeled_data.png}
\end{center}

Proximities can be used to \textbf{locate outliers}, i.e. observations whose proximities to all other observations are small: 
\begin{itemize}
\item Some measure of outlyingness can be computed for each observation in the training sample
\item If the measure is unusually large, the observation should be carefully inspected
\end{itemize}

This measure can then be used for \textbf{identifying mislabeled data}:
\begin{itemize}
\item Instances in the training data are sometimes labeled ambiguously or incorrectly, especially in \enquote{manually} created data sets.
\item Proximities can help in finding them: they often show up as outliers in terms of their proximity values. 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Discussing proximities}
\begin{itemize}
  \item Proximities are one of the most useful tools in RFs, as they can be used for detecting outliers and mislabled data, visualization and inputting missing data.
  \item They measure similarity (meaning "closeness" or "nearness") of observations.
  \item With large datasets, it is not possible to fit a $n \times n$ matrix into fast memory
  \item A modification is needed - reduce the required memory size to $n \times M$, where $M$ is the number of trees in the forest!
\end{itemize}
\end{vbframe}

\endlecture
\end{document}
