\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest.png}
\newcommand{\learninggoals}{
\item Know how random forests are defined by extending the idea of bagging
\item Understand that the goal is to decorrelate the trees}
\usepackage{../../style/lmu-lecture}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Random Forest: Introduction}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Motivation}

CARTs offer several appealing features:

\begin{itemize}
  \item \textbf{Interpretability}: They are easy to understand and explain.
  \item \textbf{Invariance to linear transformations}: Their performance is unaffected by scaling or shifting of data.
  \item \textbf{Versatility}: CARTs work on categorial and numerical data.
  \item \textbf{Robustness to missing values}: They can be designed to manage data with missing entries.
\end{itemize}

\vspace{1em}
Despite these benefits, CARTs are not without their drawbacks:
\vspace{1em}

\begin{quotation}
"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy."
\end{quotation}
\citebutton{Hastie et al., 2001, p. 352}{https://hastie.su.domains/Papers/ESLII.pdf}

\vspace{1em}
The use of bagging and some clever tricks results in the better performing Random Forest,
while preserving CARTs advantages.

\end{vbframe}

\begin{vbframe}{Random Forests}

A bagged ensemble benefits from diverse base learners: \textit{Independent errors help cancel out individual errors.} Random Forests add an extra layer of randomness to increase diversity.

\begin{itemize}
\item Proposed by \citebutton{Breiman, 2001}{https://link.springer.com/article/10.1023/A:1010933404324}, Random Forests use CARTs as base learners applied to bootstrap samples of the data.
\item They decorrelate trees by randomizing the selection of features at each split. This reduces the correlation between individual trees, leading to improved model performance.
\end{itemize}

\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Random feature sampling}

RFs decorrelate their trees with a simple randomized approach:

\begin{itemize}
  \item At each node of each tree, randomly draw $\texttt{mtry} \le p$ candidate features.
  \item \textbf{Only} consider these features for finding the best split!
\end{itemize}

% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.p
\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/forest-feature-sampling.png}
\end{center}

$\texttt{mtry}$ is simply a value we set before training the RF.

\end{vbframe}

\begin{vbframe}{Finding the best $\texttt{mtry}$ value}

\begin{center}
\includegraphics[width=1\textwidth]{figure/forest-mtry.png}
\end{center}

$\Rightarrow$ Recommended values for $\texttt{mtry}$ are

\begin{itemize}
  \item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$,
  \item Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Important Settings}

In addition to random feature sampling ($\texttt{mtry}$), RFs have two other knobs to tune:

\begin{itemize}
  \item $\texttt{maxnodes}$: Denotes the depth of each base learner $\bl$ (akin to $\texttt{maxDepth}$ for CARTs). RF usually use fully expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variance of the ensemble} \citebutton{Louppe, 2015}{http://arxiv.org/abs/1407.7502}
\begin{center}
\includegraphics[width=1\textwidth]{figure/forest-maxdepth.png}
\end{center}
  \item $\texttt{ntrees}$: The number of trees in the forest. As always in bagging, RFs \textbf{benefit from large ensembles} \citebutton{Breiman, 2001}{http://link.springer.com/10.1023/A:1010933404324}
\end{itemize}

Sensible defaults are: $\texttt{ntrees} = 100$ and $\texttt{maxnodes} = \infty$.

\end{vbframe}

\begin{vbframe}{Effect of $\texttt{ntrees}$}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1} 

}

\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2} 

}
\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3} 

}
\end{knitrout}
\end{vbframe}

\begin{vbframe}{RF in practice}
\begin{center}
\includegraphics[width=1\textwidth]{figure/bagging-bench_RF.png}
\end{center}
\end{vbframe}

\endlecture
\end{document}
