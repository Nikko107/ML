\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest.png}
\newcommand{\learninggoals}{
\item Know how random forests are defined by extending the idea of bagging
\item Understand that the goal is to decorrelate the trees}
\usepackage{../../style/lmu-lecture}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Random Forest: Introduction}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Random Forests} % #TODO diese und n√§chste Folie mergen / besser aufteilen

Modification of bagging for trees proposed by \citebutton{Breiman, 2001}{https://link.springer.com/article/10.1023/A:1010933404324}:

\begin{itemize}
  \item Tree base learners on bootstrap samples of the data
  \item Uses \textbf{decorrelated} trees by randomizing splits (see below)
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lU1qPlY-NQkvWCDrrV6PdDYkloF3eFudM9v2NFYg1rY/edit?usp=sharing
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Random feature sampling}

As we've seen, a bagged ensemble benefits from variable base learners: \textit{Independent errors help in cancelling out individual errors.} RFs decorrelate their trees with a simple randomized approach:

\begin{itemize}
  \item At each node of each tree, randomly draw $\texttt{mtry} \le p$ candidate features.
  \item \textbf{Only} consider these features for finding the best split!
  % \item Recommended values: #TODO: als Folgerung von Experiment!
  % \begin{itemize}
  %   \item Classification: mtry $ = \lfloor \sqrt{p} \rfloor$
  %   \item Regression: mtry $ = \lfloor p/3 \rfloor$
  % \end{itemize}
\end{itemize}

% #TODO: Hier Grafik!!

$\texttt{mtry}$ is simply a hyperparameter we can tune to find the best values.

\end{vbframe}

\begin{vbframe}{Tuning $\texttt{mtry}$}

Test

\end{vbframe}

\begin{vbframe}{Other Key Hyperparamerters}

In addition to random feature sampling ($\texttt{mtry}$), RFs have two other important hyperparameters to consider:

\begin{itemize}
  \item $\texttt{maxnodes}$: Denotes the depth of each base learner $\bl$ (akin to $\texttt{maxDepth}$ for CARTs). RF usually use fully expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variance of the ensemble} \citebutton{Louppe, 2015}{http://arxiv.org/abs/1407.7502}
  % #TODO: kleine Grafik die das belegt
  \item $\texttt{ntrees}$: The number of trees in the forest. As always in bagging, RFs \textbf{benefit from large ensembles} \citebutton{Breiman, 2001}{http://link.springer.com/10.1023/A:1010933404324}
\end{itemize}

Sensible defaults are: $\texttt{ntrees} = 100$ and $\texttt{maxnodes} = \infty$.

\end{vbframe}

\begin{vbframe}{Effect of $\texttt{ntrees}$}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1} 

}

\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2} 

}
\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3} 

}
\end{knitrout}
\end{vbframe}

\begin{vbframe}{Random Forest Formalised}
  \begin{algorithm}[H]
  \caption*{Random Forest algorithm}
  \begin{algorithmic}[1]
  \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
  in the forest, number $\texttt{mtry}$ of variables to draw for each split
  \For {$m = 1 \to M$}
  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
  \State Grow tree $\bl$ using $\D^{[m]}$
  \State For each split only consider $\texttt{mtry}$ randomly selected features
  \State Grow tree \textit{without} early stopping or pruning
\EndFor
\State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
\end{algorithmic}
\end{algorithm}
\end{vbframe}

\endlecture
\end{document}
