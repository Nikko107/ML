\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest-gpt4o.jpg}
\newcommand{\learninggoals}{
\item Know how random forests are defined by extending the idea of bagging
\item Understand that the goal is to decorrelate the trees}
\usepackage{../../style/lmu-lecture}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Random Forest: Introduction}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Motivation}

CARTs offer several appealing features:

\begin{itemize}
  \item \textbf{Interpretability}: They are easy to understand and explain.
  \item \textbf{Invariance to rank-preserving transformations}: Their performance is unaffected by scaling or shifting of features.
  \item \textbf{Versatility}: CARTs work on categorial and numerical data.
  \item \textbf{Robustness to missing values}: They can be designed to manage data with missing entries.
\end{itemize}

\vspace{1em}
Despite these benefits, CARTs are not without their drawbacks:
\vspace{1em}

\begin{quotation}
"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy."
\end{quotation}
\citebutton{Hastie et al., 2001, p. 352}{https://hastie.su.domains/Papers/ESLII.pdf}

% \vspace{1em}
% Bagging and some clever tricks create the random forest (RF), which \textbf{sacrifices interpretability} for significant performance gains.
\end{vbframe}

\begin{vbframe}{random forests \citebutton{Breiman, 2001}{https://link.springer.com/article/10.1023/A:1010933404324}}

An extension of bagging creates the random forest (RF), which sacrifices interpretability for significant performance gains:

\begin{itemize}
\item RFs use \textbf{CARTs as base learners}. %$\bl$ applied to bootstrap samples of the data.
\item \textbf{Random feature sampling} decorrelates the base learners.
% \item Since bootstrap samples are largely similar, the trained base learners $\blh$ tend to be correlated.
\item \textbf{Fully expanded trees} increase variance of the ensemble.
\end{itemize}

\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Intuition behind random forests}
% TODO: gleich mit ausgetauschten Werten
This correlation affects the variance of an ensemble:

$$
\var\left(\bar{X}\right) = \var\left(\frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n^2} \left( \sum_{i=1}^{n} \var(X_i) + 2 \sum_{i < j} \cov(X_i, X_j) \right)
$$

For $\sigma = \var(X_i)$, $\rho = \cov(X_i, X_j)$ we get:

% TODO
$$
TODO
$$

Random forests reduce the correlation between individual trees (\(\rho_{ij}\)), leading to improved model performance.
\end{vbframe}

\begin{vbframe}{Random feature sampling}

RFs decorrelate trees with a simple randomized approach:

\begin{itemize}
  \item At each node of each tree, randomly draw $\texttt{mtry} \le p$ features.
  \item \textbf{Only} consider these features for finding the best split!
\end{itemize}

% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.p
\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/forest-feature-sampling.png}
\end{center}

$\texttt{mtry}$ is a hyperparameter we set before training the RF.

\end{vbframe}

\begin{vbframe}{Finding the best $\texttt{mtry}$ value}

\begin{center}
\includegraphics[width=1\textwidth]{figure/forest-mtry.png}
\end{center}

$\Rightarrow$ $\texttt{mtry}$ is typically larger for regression than for classification. While default values are generally effective, $\texttt{mtry}$ is the most relevant tuning parameter for random forests. Rule of thumb:

\begin{itemize}
\item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$,
\item Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Tree depth}

In addition to $\texttt{mtry}$, RFs have two other important hyperparameters:

\begin{itemize}
  \item The minimum number of observations in each decision tree node. Default (Ranger): $\texttt{min.node.size = 5}$ \citebutton{Breiman, 2001}{https://link.springer.com/article/10.1023/A:1010933404324}
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/forest-minnode.png}
\end{center}
  \item The depth of each base learner $\bl$. Default: $\texttt{maxDepth} = \infty$
\end{itemize}

RF usually use expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variance of the ensemble} \citebutton{Louppe, 2015}{http://arxiv.org/abs/1407.7502}
\end{vbframe}

\begin{vbframe}{$\texttt{ntrees}$}
\begin{itemize}
\item $\texttt{ntrees}$ is the number of trees in the forest. RFs benefit especially well from \textbf{large ensembles} \citebutton{Breiman, 2001}{http://link.springer.com/10.1023/A:1010933404324}
\item However, more trees also increase computational costs, and the performance gains diminish beyond a certain number.
\item A sensible default is $\texttt{ntrees} = 500$ as there exist greatly optimized implementations.
\end{itemize}

\begin{center}
\includegraphics[width=330pt]{figure/forest-ntree.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Effect of $\texttt{ntrees}$}
%TODO hier immer vistool!! https://slds-lmu.github.io/vistool/
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1} 

}

\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2} 

}
\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3} 

}
\end{knitrout}
\end{vbframe}

\begin{vbframe}{Can a RF overfit?}
Just like any other learner, RFs \textbf{can} overfit! However, RFs are generally \textbf{less} prone to overfitting than individual CARTs:
\begin{itemize}
  \item Overly complex trees can \textit{still} lead to overfitting! If most $\blh$ capture noise, so does the RF.
  \item But because of the added randomness, their averaged predictions tend to cancel out errors, meaning less noise is captured.
\end{itemize}
Since each tree is trained \textit{individually and without knowledge of previously trained trees}, increasing $\texttt{ntrees}$ generally reduces variance \textbf{without increasing the chance of overfitting!} \citebutton{P. Probst and A. -L. Boulesteix, 2018}{https://jmlr.org/papers/v18/17-269.html}
\end{vbframe}

\begin{vbframe}{RF in practice}

Benchmarking bagged ensembles with 100 base learners each on the $\texttt{spam}$ dataset versus a similar sized RF ($\texttt{ntrees} = 100$, $\texttt{mtry} = \sqrt{p}$, $\texttt{minnode} = 1$, $\texttt{maxnode} = \infty$), we see just how well a RF performs!

\begin{center}
\includegraphics[width=290pt]{figure/bagging-bench_RF.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Discussion}

Advantages:
\begin{itemize}
  \item Most advantages of trees also apply to RF: not much preprocessing required, missing value handling, etc.
  \item Easy to parallelize
  \item Often work well (enough)
  \item Works well on high-dimensional data 
  \item Works well on data with irrelevant \enquote{noise} variables
\end{itemize}

Disadvantages:
\begin{itemize}
  \item Often sub-optimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees (but many extra tools are nowadays
    available for interpreting RFs)
  \item Implementation can be memory-hungry
  \item Prediction can be computationally demanding for large ensembles
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
