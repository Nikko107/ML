\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest.png}
\newcommand{\learninggoals}{
\item Know how random forests are defined by extending the idea of bagging
\item Understand that the goal is to decorrelate the trees}
\usepackage{../../style/lmu-lecture}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Random Forest: Introduction}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Motivation}

CARTs offer several appealing features:

\begin{itemize}
  \item \textbf{Interpretability}: They are easy to understand and explain.
  \item \textbf{Invariance to linear transformations}: Their performance is unaffected by scaling or shifting of data.
  \item \textbf{Versatility}: CARTs work on categorial and numerical data.
  \item \textbf{Robustness to missing values}: They can be designed to manage data with missing entries.
\end{itemize}

\vspace{1em}
Despite these benefits, CARTs are not without their drawbacks:
\vspace{1em}

\begin{quotation}
"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy."
\end{quotation}
\citebutton{Hastie et al., 2001, p. 352}{https://hastie.su.domains/Papers/ESLII.pdf}

\vspace{1em}
Bagging and some clever tricks create the random forest (RF), which \textbf{sacrifices interpretability} for significant performance gains.
\end{vbframe}

\begin{vbframe}{random forests \citebutton{Breiman, 2001}{https://link.springer.com/article/10.1023/A:1010933404324}}

A bagged ensemble benefits from diverse base learners $\bl$
: \textit{Independent errors help cancel out individual errors.}

\begin{itemize}
\item RFs use CARTs as $\bl$ applied to bootstrap samples of the data.
\item Since bootstrap samples are largely similar, the trained base learners $\blh$ tend to be correlated. RFs reduce the correlation between individual trees, leading to improved model performance.

\end{itemize}

\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Random feature sampling}

RFs decorrelate their trees with a simple randomized approach:

\begin{itemize}
  \item At each node of each tree, randomly draw $\texttt{mtry} \le p$ candidate features.
  \item \textbf{Only} consider these features for finding the best split!
\end{itemize}

% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.p
\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/forest-feature-sampling.png}
\end{center}

$\texttt{mtry}$ is simply a hyperparameter we set before training the RF.

\end{vbframe}

\begin{vbframe}{Finding the best $\texttt{mtry}$ value}

\begin{center}
\includegraphics[width=1\textwidth]{figure/forest-mtry.png}
\end{center}

$\Rightarrow$ $\texttt{mtry}$ is typically larger for regression than for classification. While default values are generally effective, $\texttt{mtry}$ is the most relevant tuning parameter for random forests. Rule of thumb:

\begin{itemize}
\item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$,
\item Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Tree depth}

% \begin{footnotesize}
In addition to $\texttt{mtry}$, RFs have two other important hyperparameters:

\begin{itemize}
  \item The minimum number of observations in each decision tree node:
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/forest-minnode.png}
\end{center}
  \item The depth of each base learner $\bl$. Default: $\texttt{maxDepth} = \infty$
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/forest-maxdepth.png}
\end{center}
\end{itemize}

RF usually use expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variance of the ensemble} \citebutton{Louppe, 2015}{http://arxiv.org/abs/1407.7502}
% \end{footnotesize}
\end{vbframe}

\begin{vbframe}{$\texttt{ntrees}$}
\begin{itemize}
\item $\texttt{ntrees}$ is the number of trees in the forest. RFs benefit especially well from \textbf{large ensembles} \citebutton{Breiman, 2001}{http://link.springer.com/10.1023/A:1010933404324}
\item However, more trees also increase computational costs, and the performance gains diminish beyond a certain number.
\item A sensible default is $\texttt{ntrees} = 500$ as there exist greatly optimized implementations.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Effect of $\texttt{ntrees}$}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1} 

}

\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2} 

}
\end{knitrout}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3} 

}
\end{knitrout}
\end{vbframe}

\begin{vbframe}{Can a RF overfit?}
Just like any other learner, RFs \textbf{can} overfit! However, RFs are generally \textbf{less} prone to overfitting than individual CARTs:
\begin{itemize}
  \item Overly complex trees can \textit{still} lead to overfitting! If most $\blh$ capture noise, so does the RF.
  \item But because of the added randomness, their averaged predicitions tend to cancel out errors, meaning less noise is captured.
  \item Thus, increasing $\texttt{ntrees}$ generally reduces variance without increasing the chance of overfitting!
\end{itemize}
\end{vbframe}

\begin{vbframe}{RF in practice}

Benchmarking bagged ensembles with 100 base learners each on the $\texttt{spam}$ dataset versus a similar sized RF ($\texttt{ntrees} = 100$, $\texttt{mtry} = \sqrt{p}$, $\texttt{minnode} = 1$, $\texttt{maxnode} = \infty$), we see just how well a RF performs!

\begin{center}
\includegraphics[width=300pt]{figure/bagging-bench_RF.png}
\end{center}

\end{vbframe}

\endlecture
\end{document}
