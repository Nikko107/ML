\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/forest.png}
\newcommand{\learninggoals}{
\item Understand random forests as extension of bagging
\item Understand the algorithm of random forests
\item Understand the performance evaluation of random forests
\item Understand the (dis-)advantages of random forests}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}


\lecturechapter{Random Forests: Overview}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Initial Problem}
Some learners (e.g. CARTs) tend to have a very high variance:
\begin{itemize}
  \item[$\rightarrow$] Very sensitive to changes in the training data: 
  Small changes in the trainig data can lead to vastly different outcomes
\end{itemize}
\end{vbframe}

\begin{vbframe}{Initial Problem: Example}
We draw two bootstrap samples from the same data set: 
\begin{center}
\includegraphics[height=0.60\textheight, keepaspectratio]{figure/cart_forest_overview_bs_datasets.pdf}
\end{center}
The two data sets are pretty similar and only differ in 20\% of points
\end{vbframe}



\begin{vbframe}{Initial Problem: The combined classifiers}
What happens when we combine these two very different results?
\begin{figure}
\includegraphics[height=0.60\textheight, keepaspectratio]{figure/cart_forest_overview_2.pdf}
%\includegraphics{figure/cart_forest_overview_1.pdf}
\end{figure}
\end{vbframe}

\begin{vbframe}{Solution}
Idea: Take advanatge of the high variance of unstable learners and use it to create a more stable learner: 
\begin{itemize}
  \item Learners trained on different parts of the training data make different assumptions abt. the 
  underlying data structure 
  \item Learners trained on different parts of the training data different mistakes
\end{itemize}
$\rightarrow$ By combining  the results of the different learners trained on different parts of the data we can cover a lot of assumptions and mistakes in our prediction. 
%\begin{figure}
%\includegraphics[width=0.50\textwidth]{figure/cart_forest_overview_5.pdf}
%\end{figure}
\end{vbframe}


\begin{vbframe}{Solution}
\begin{figure}
\includegraphics[width=0.95\textwidth]{figure/cart_forest_overview_6.pdf}
\end{figure}
\end{vbframe}

\begin{vbframe}{Bagging}
\begin{itemize}
  \item Combination of bootstrapping data and aggregrating models (Bagging = \textbf{B}ootstrap \textbf{Agg}regation)
  \item Approach to stabelize learners and thereby reduce variance of predictions
\begin{enumerate}
  \item Draw m subsamples from dataset via bootstrapping
  \item Construct m base learners from these m subsamples (can be done in parallel)
  \item Aggregate the m single base learners to construct one ensemble learner
  \item Ensemble learner is used to make predictions
\end{enumerate}
\end{itemize}
\end{vbframe}


\begin{vbframe}{Random Forests and Bagging}
Several ways to further improve the performance bagging ensembles e.g.: 
\begin{itemize}
  \item Increase variance of the ensemble to improve performance
  \item Decorrelate single base learners to make base learners even more different
\end{itemize} 
\ \newline
Random forests implement these improvement measures and thereby extend bagging:
\begin{itemize}
  \item RF implement the bagging algorithm using CARTs as base learners
  \item Decorrelation by randomizing splits in each base learner
  \item Increased variance of the ensemble by fully expanding each learner (no pruning etc.)
\end{itemize}
\end{vbframe}


\begin{vbframe}{Random forest algorithm }
\begin{enumerate}
  \item For each of the m base learners do:
  \begin{itemize}
    \item Draw subset of data via bootstrapping and train CART 
    \item Per node/split randomly choose mtry features and only consider these mtry features in respective split
  \end{itemize}
  \item Aggregate the predictions of m the base learners to get the ensemble prediction
  %\begin{itemize}
  %  \item Regression: Aggregation via taking the mean
  %  \item Classification: Aggregation via majority vote
  %\end{itemize}
\end{enumerate}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lU1qPlY-NQkvWCDrrV6PdDYkloF3eFudM9v2NFYg1rY/edit?usp=sharing
\includegraphics[height=0.4\textheight, keepaspectratio]{figure_man/forest.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Performance Evaluation: The Out of bag error}
\begin{itemize}
  \item No need to define an explicit train and test set due to bootstrapping 
  \item Use the unused data points from each data set as test points   
\end{itemize}
\begin{center}
\includegraphics[height=0.60\textheight, keepaspectratio]{figure/cart_forest_overview_oob_datasets.pdf}
\end{center}
\end{vbframe}


\begin{vbframe}{Random Forest Performance Evaluation}
\begin{itemize}
  \item No need to define an explicit train and test set due to bootstrapping 
  \item OOB errors are used to estimate the performance of the random forest: 
  \begin{enumerate}
    \item Calculate the OOB prediction per observation for each tree in ensemble 
    \item Aggregate the OOB prediction per observation to get ensemble OOB prediction for observation
    \item Use ensemble OOB predictions to calculate the estimated OOB error for the ensemble (= unbiased estimate for GE) 
  \end{enumerate}
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Feature importance}
%Knowing the importance of each feature in a given learner/model helps with interpetability 
%and feature selection.\\
%Different approaches to determine feature importance:
%\begin{itemize}
%  \item Improvement split criterion:
%  \item Via Permutating features: 
%\end{itemize}
%\end{vbframe}

\begin{vbframe}{Advantages and Disadvantges}
\textbf{Advantages}
\begin{itemize}
  \item All advatages of trees
  \item Reduces variance/stablizes predictions w.r.t. a single CART
  \item Built in performance evaluation with an unbiased estimate for GE
  \item Built in analysis of feature importance and observation proximities
\end{itemize}
\ \newline
\textbf{Disadvantages}
\begin{itemize}
  \item Computationally expensive (especially for large ensembles) 
  \item No eas way to visually interpret a random forest (in contrast to a single CART) 
\end{itemize}
\end{vbframe}


\endlecture
\end{document}
