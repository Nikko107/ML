\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure_man/pexels-karolina-grabowska-4472108.jpg}
% stock free image from pexels.com


\newcommand{\learninggoals}{
\item Understand the main idea behind tuning,
\item why tuning matters,
\item and why tuning is difficult
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
% \institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-% lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

\lecturechapter{Tuning: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{What is Tuning?}
\begin{itemize}
\item \small Tuning is the process of selecting the best hyperparameters, denoted as $\lambda$, for a machine learning model
\item \small Hyperparameters are the parameters of the learner (versus model parameters $\theta$)
\item \small Consider a guitar analogy: Hyperparameters are akin to the tuning pegs. Learning the best parameters \bm{$\thetabh$} - playing the guitar - is a separate process that depends on tuning!
\end{itemize}

\begin{center}
\includegraphics[width = 0.75\textwidth]{figure_man/riskmin_bilevel3.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Why Tuning Matters}
\begin{itemize}
\item \small Just like a guitar won't perform well when out-of-tune, properly tuning a learner can drastically improve the resulting model performance
\item \small Tuning helps find a balance between underfitting and overfitting
\end{itemize}

\begin{center}
\vspace{3em}
\includegraphics[width = 0.9\textwidth]{figure/tuning_importance.png}
\end{center}

\end{vbframe}


\begin{vbframe}{How hard could it be?}
\begin{itemize}
\item \small Very difficult: There are lots of different configurations to choose from, known as the hyperparameter space, denoted by $\Lam$ (analogous to $\Theta$)
\item \small Black box: If one opts for a configuration $\lamv \in \Lam$, how can its performance be measured (and compared)?
\item \small Well-thought-out approaches - black box optimization techniques - are needed!

\begin{center}
\vspace{2em}
\includegraphics[width=200pt]{figure/cart_tuning_balgos_1.pdf}
\end{center}

\end{itemize}
\end{vbframe}


\begin{vbframe}{Naïve Approaches}
Let's start with two naïve approaches -

\textbf{Grid Search} and \textbf{Random Search}:

\vspace{4em}
\begin{minipage}{0.51\textwidth}
\begin{center}
\textbf{Grid Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_1.pdf}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{center}
\textbf{Random Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_2.pdf}
\end{minipage}

\end{vbframe}

% TODO: Sollten die Algorithmen überhaupt erklärt werden? Ich finde nicht, da Video ja auch nur Einführung. Daher nur Vorteil: Effizienter (weniger Evals)

\vspace{10pt}

Main advantage: tend to be more efficient by requiring less evaluations

\end{vbframe}

% #TODO: Formulierungen etc. - passt Aufbau so?
\begin{vbframe}{Pipelines}
Choosing suitable models, evaluating their many configurations $\lamv \in \Lam$, training and deploying the best one is a lot of work and allows for human errors - how can we automate it (called \textbf{AutoML})?

\vspace{10pt}

By using Pipelines!

% main idea: sequential processing
\vspace{10pt}% Bild einer Pipeline (DAG)

\textbf{END TO END PERFORMANCE EVALUATION AND TRAINING}
\begin{itemize}
\item consistency
\item reproducibility
\item error reduction
\end{itemize}
\end{vbframe}

\endlecture
\end{document}