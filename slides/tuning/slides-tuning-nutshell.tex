\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure_man/pexels-karolina-grabowska-4472108.jpg}
% stock free image from pexels.com

\newcommand{\learninggoals}{
\item Understand the main idea behind tuning,
\item why tuning matters,
\item and why tuning is difficult
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
% \institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-% lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

\lecturechapter{Tuning: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{What is Tuning?}
\begin{itemize}
\item \small Tuning is the process of selecting the best hyperparameters, denoted as $\lambda$, for a machine learning model
\item \small Hyperparameters are the parameters of the learner (versus model parameters $\theta$)
\item \small Consider a guitar analogy: Hyperparameters are akin to the tuning pegs. Learning the best parameters \bm{$\thetabh$} - playing the guitar - is a separate process that depends on tuning!
\end{itemize}

\begin{center}
\includegraphics[width = 0.75\textwidth]{figure_man/riskmin_bilevel3.png}
\end{center}

\end{vbframe}


\begin{vbframe}{Why Tuning Matters}
\begin{itemize}
\item \small Just like a guitar won't perform well when out-of-tune, properly tuning a learner can drastically improve the resulting model performance
\item \small Tuning helps find a balance between underfitting and overfitting
\end{itemize}

\begin{center}
\vspace{3em}
\includegraphics[width = 0.9\textwidth]{figure/tuning_importance.png}
\end{center}

\end{vbframe}


\begin{vbframe}{How hard could it be?}
\begin{itemize}
\item \small Very difficult: There are lots of different configurations to choose from, known as the hyperparameter space, denoted by $\Lam$ (analogous to $\Theta$)
\item \small Black box: If one opts for a configuration $\lamv \in \Lam$, how can its performance be measured (and compared)?
\item \small Well-thought-out approaches - black box optimization techniques - are needed!

\begin{center}
\vspace{2em}
\includegraphics[width=200pt]{figure/cart_tuning_balgos_1.pdf}
\end{center}

\end{itemize}
\end{vbframe}


\begin{vbframe}{Naïve Approaches}
Let's start with two naïve approaches -

\textbf{Grid Search} and \textbf{Random Search}:

\vspace{2em}
\begin{minipage}{0.51\textwidth}
\begin{center}
\textbf{Grid Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_1.pdf}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{center}
\textbf{Random Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_2.pdf}
\end{minipage}

\vspace{2em}
\small Beyond these basic methods, there are more sophisticated techniques which operate on certain assumptions about the objective function. These assumptions enable them to search for optimal solutions more efficiently.

\end{vbframe}

\begin{vbframe}{Nested Resampling}

\begin{itemize}
\item \small Nested resampling is a method used for reliable performance estimation of tuned machine learning models.
\item \small It involves two layers of resampling: the outer layer for performance estimation and the inner layer for hyperparameter tuning.
\item \small The key idea is to separate model selection and model evaluation to avoid biased performance estimates.
\end{itemize}

\textbf{Why Nested Resampling Matters:}
\begin{itemize}
\item \small It provides an unbiased estimate of the Generalization Error (GE).
\item \small Prevents overfitting to the validation set during hyperparameter tuning.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Pipelines in Machine Learning}
Pipelines are like the assembly lines in machine learning. They automate the sequence of data processing and model building tasks, ensuring efficiency and consistency.

\textbf{Why Pipelines Matter:}
\begin{itemize}
\item \small \textbf{Streamlined Workflow:} Automates the flow from data preprocessing to model training.
\item \small \textbf{Reproducibility:} Ensures that results can be reproduced consistently.
\item \small \textbf{Error Reduction:} Minimizes the chance of human errors in the model building process.
\end{itemize}

\textbf{Simple Pipeline Example:}
\begin{itemize}
\item A basic pipeline might include data normalization, feature selection, and a learning algorithm.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Pipelines with Nested Resampling and AutoML}

\begin{itemize}
\item \small Incorporating nested resampling into pipelines allows for a more accurate estimate of the Generalization Error (GE).
\item \small The pipeline includes data preprocessing, nested resampling for hyperparameter tuning, and final model training.
\item \small AutoML can be seen as an extension of this process, where not only hyperparameters are optimized, but also model selection and preprocessing steps are automated.
\end{itemize}

\textbf{Example Pipeline with Nested Resampling and AutoML:}
\begin{enumerate}
\item Data Preprocessing (e.g., normalization, feature selection)
\item Nested Resampling for Hyperparameter Tuning
    \begin{itemize}
    \item \small Inner Loop: Hyperparameter tuning on training subset
    \item \small Outer Loop: Performance estimation on validation subset
    \end{itemize}
\item AutoML for Model and Preprocessing Optimization
\item Training the Final Model with Optimal Settings
\item Estimating the GE using the outer loop of nested resampling
\end{enumerate}

\end{vbframe}

\endlecture
\end{document}