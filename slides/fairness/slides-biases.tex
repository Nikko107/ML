\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
%\newcommand{\titlefigure}{figure/fig-title-knn-3d-contour.png}
\newcommand{\learninggoals}{
\item XXXX
}

\newcommand{\subf}[2]{%
  {\small\begin{tabular}[t]{@{}c@{}}
   \mbox{}\\[-\ht\strutbox]
   #1\\#2
   \end{tabular}}%
}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Biases}
\lecture{Introduction to Machine Learning}

%-------------------------------------------------------------------------------
\begin{vbframe}{Algorithmic Decisions Affect People! 
(sometimes negatively)}

\begin{figure}
\centering
\begin{tabular}{cc}

\subf{\includegraphics[width=0.3\textwidth]{figure_man/biases-policing.png}}
     {Policing\\
     \begin{tiny}source: \href{https://www.theverge.com/2020/6/24/21301465/ai-machine-learning-racist-crime-prediction-coalition-critical-technology-springer-study}{theverge.com}\end{tiny}}
&
\subf{\includegraphics[width=0.4\textwidth]{figure_man/biases-hiring.png}}
     {Hiring\\
     \begin{tiny}source: \href{http://web.br.de/interaktiv/ki-bewerbung/en/
}{br.de}\end{tiny}}
\\

\subf{\includegraphics[width=0.4\textwidth]{figure_man/biases-healthcare.png}}
     {Healthcare\\
     \begin{tiny}source: \href{www.pnas.org/content/117/23/12592
}{pnas.org}\end{tiny}}
&
\subf{\includegraphics[width=0.4\textwidth]{figure_man/biases-work1.png}\\
\includegraphics[width=0.4\textwidth]{figure_man/biases-work2.png}}
     {Work (non AI)\\
     \begin{tiny}source: \href{www.bbc.com}{bbc.bom}\end{tiny}}
\\

\end{tabular}
\end{figure}
\end{vbframe}

%-------------------------------------------------------------------------------
\begin{vbframe}{Bias and Fairness}

\begin{itemize}
   \item In the context of automated decision-making, we are facing a lot of so-called biases.
   \item \textbf{Bias} means \enquote{a systematic error or an unexpected tendency to favor one outcome over another} Mehrabi et al., 2019)\footnote{A Survey on Bias and Fairness in Machine Learning, Mehrabi et al., 2019}
   
   \item Depending on the ethical values we care about in the society or as individuals, we want to mitigate biases which are "unfair". 
   
   \item \textbf{Fairness} is the \enquote{absence of any prejudice or favoritism towards an individual or a group based on their intrinsic or acquired traits in the context of decision-making} 
(Mehrabi et al., 2019)

\item As fairness and unfairness is defined context-dependent, there exist a lot of definitions. 

\item In the following, we will present common biases in the context of machine learning. 

\end{itemize}
\end{vbframe}
%-------------------------------------------------------------------------------


%-------------------------------------------------------------------------------
\begin{vbframe}{Historical Bias}


Very often data contains historical biases we do not want to perpetuate:

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
   \item Often, texts reflect historical inequalities (see:figure). 
   \item Another example is the fact that dark-skinned people get longer prison sentences in the US.
   \item As a result of the higher police presence in poorer areas, more arrests or reoffenders were found.
   \item Historically, women were paid less.
\end{itemize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
    \begin{center}
     \includegraphics[width=\textwidth]{figure_man/biases-googletranslate.png}
     \end{center}
\end{column}
\end{columns}

\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{Sampling Bias}

Data is often not representative of the whole population we want to predict. 
\begin{itemize}
   \item Is the training dataset a representative sample for the whole population?
   \item Collecting data from underrepresented groups are often neglected as it is expensive and those e.g. have little buying power.
   \item Data often does not exist.
   \item Women could not take out loans in many countries until the 1970s.
   \item Representation is a big problem in medicine: Gender Medicine.
\end{itemize}

\begin{center}
     \includegraphics[width=0.5\textwidth]{figure_man/biases-sampling.png}
     \end{center}
\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{Model Bias}
 Models are lazy: 
\begin{itemize}
   \item Models often focus on performance in the \textbf{majority group} while neglecting minorities.
   \item Models can pick up \textbf{spurious correlations}, i.e., storks have a influence on the number of babies.
   \item Models need to be complex enough to model interactions in the data.
\end{itemize}

\begin{footnotesize}
 Explainability techniques can hint at model bias, but they can also be manipulated, see Anders et al., Fairwashing Explanations with Off-Manifold Detergent.
\end{footnotesize}

\begin{center}
     \includegraphics[width=0.5\textwidth]{figure_man/biases-explain.png}
\end{center}

\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{Feedback Loops}
When models have a transformative effect on the world, biases can be amplified.

\begin{itemize}
   \item \textbf{Bank}: If I only 'accept' people for a loan when my model says the person will pay it back, I will never know if the 'declined' would have payed back.
   \item \textbf{Search}: If I only show the majority, people and systems will pick this up as the state of the world!
\end{itemize}




%FIGURE SOURCE: Florian Pfisterer https://docs.google.com/presentation/d/14Vse6gkQ6PaVPsCr_rQXXBlN21Yu8tVDmU7v-YLoqQQ/edit#slide=id.gd484c705bc_0_432

\begin{columns}
\begin{column}{0.5\textwidth}
    \begin{center}
     \includegraphics[width=0.7\textwidth]{figure_man/biases-loop.png}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
    \begin{center}
     \includegraphics[width=0.9\textwidth]{figure_man/biases-google-doctor.png}
     \end{center}
\end{column}
\end{columns}




\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{When a metric becomes a target it ceases to be a good metric}


\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{Clever Hans and Goal Exploitation for AI Agents}


\end{vbframe}
%-------------------------------------------------------------------------------
\endlecture
\end{document}
