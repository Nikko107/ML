\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
%\newcommand{\titlefigure}{figure/fig-title-knn-3d-contour.png}
\newcommand{\learninggoals}{
\item XXXX
}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Measuring Fairness and Debiasing}
\lecture{Introduction to Machine Learning}

%-------------------------------------------------------------------------------
\begin{vbframe}{A causal perspective}
\begin{columns}
\begin{column}{0.55\textwidth}
\end{column}
\begin{column}{0.45\textwidth}
\begin{center}
    \includegraphics[width=\textwidth]{figure_man/fair-dag.png}\\
%FIGURE SOURCE: https://docs.google.com/presentation/d/1ZOcphgTxOzFQLF7NgJgIFRAobvXXOho21ptBLLvT0Mc/edit?usp=sharing
    \includegraphics[width=\textwidth]{figure_man/fair-dag2.png}\\
%FIGURE SOURCE: https://docs.google.com/presentation/d/1ZOcphgTxOzFQLF7NgJgIFRAobvXXOho21ptBLLvT0Mc/edit?usp=sharing

\end{center}

\end{column}
\end{columns}





\end{vbframe}
%-------------------------------------------------------------------------------

\begin{vbframe}{Fairness Metrics}
\textbf{What is fair?}
\newline
This is partially a philosophical question:
\begin{itemize}
\item Equality of Treatment
\item Equality of Opportunity
\item Equality of Outcomes 
\end{itemize}

\textbf{Different Approaches}:

\begin{itemize}
\item Individual Fairness: 
"Similar predictions for similar individuals" 


\item Statistical (Sub-group) Fairness:
Treat different groups equally


\item Causal Fairness:
Consider the causal graph when assessing fairness
\end{itemize}

Often, we use statistical fairness, as it is simple to measure and does not require too many assumptions.

%-------------------------------------------------------------------------------
\framebreak
\begin{footnotesize}
Depending on the context, there a many fairness definitions which can be applied. Amongst others, there are: 
\vfill
A binary predictor $\hat{Y}$ satisfies the following fairness criteria with respect to a sensitive attribute (e.g., gender) and the true label $Y$, if...

\begin{itemize}

\item \textbf{Equalized Odds} 
... $\hat{Y}$ and A are independent conditional on $Y$:
$$P(\hat{Y}=1|A=0,Y =y) = P(\hat{Y}=1|A=1,Y =y),\ y \in \{0,1\}.$$
%Moritz Hardt, Eric Price, Nati Srebro, et al. 2016. Equality of opportunity in supervised learning. In Advances in neural information processing systems. 3315–3323
\item \textbf{Equal Opportunity}
... the protected and unprotected groups should have equal true positive rates:
$$P(\hat{Y}=1|A=0,Y=1) = P(\hat{Y}=1|A=1,Y=1).$$
\item \textbf{Demographic Parity}
... the likelihood of a positive outcome
is the same regardless in which group a person is: 
$$P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1).$$

\item \textbf{Treatment Equality}
... the ratio of false negatives and false positives is the same for both protected group categories.

\end{itemize}
\end{footnotesize}
\framebreak

Having so many different fairness metrics, how do we decide which one we use? 

One possibility is deciding based on the Aequitas Fairness Tree: 

    \begin{center}
    \includegraphics[width=0.65\textwidth]{figure_man/fair-aequitas-tree.png}\\
    %FIGURE SOURCE: http://www.datasciencepublicpolicy.org/projects/aequitas/
      \begin{tiny}
    \href{http://www.datasciencepublicpolicy.org/projects/aequitas/}{datasciencepublicpolicy.org}
    \end{tiny}
    \end{center}

%-------------------------------------------------------------------------------
\framebreak
%What do those fairness metrics actually mean? \\
\null
\vfill
\begin{columns}
\begin{column}{0.5\textwidth}
\null
\vfill
\textbf{Example}: \\

If we decide about a punitive intervention, we might want that one group is not punished more often than another. 
\newline

False Positive Rate Parity:
\begin{footnotesize}
$$
\Delta FPR = FPR(Group A) - FPR(Group B)
$$
\end{footnotesize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here

    \begin{center}
    \includegraphics[width=\textwidth]{figure_man/fair-aequitas-tree3.png}
    %FIGURE SOURCE: http://www.datasciencepublicpolicy.org/projects/aequitas/
        \begin{tiny}
    \href{http://www.datasciencepublicpolicy.org/projects/aequitas/}{datasciencepublicpolicy.org}
    \end{tiny}
    \end{center}

 
\end{column}
\end{columns}
   \vfill

\framebreak 


\framebreak
\null
\vfill
\begin{large}
\textit{\enquote{One of the things that I worry about most in this field is that there’s a conversation that gets had about fairness as a kind of metric that can be solved, rather than an invitation to an inquiry about justice and human flourishing and well-being ...}}\\
\end{large}
Alex Hana
\vfill

%SOURCE: https://venturebeat.com/2020/10/29/trump-faces-executive-order-lawsuit-as-critical-race-theory-fuels-ai-research/


\end{vbframe}
%-------------------------------------------------------------------------------
\begin{vbframe}{Clever Hans and Goal Exploitation for AI Agents}
\textcolor{blue}{Equalized Odds}\\
\textbf{Goodhart's law}:\\

\textit{\enquote{When a metric becomes a target it ceases to be a good metric.}}


\textcolor{blue}{Wie Quellen bei Bildern zitieren...}
\begin{center}
\begin{figure}
\includegraphics[width=0.5\textwidth]{figure_man/fair-clever-hans.png}

%FIGURE SOURCE: https://culturacientifica.com/2016/06/27/el-secreto-de-hans/cleverhans-2/

\end{figure}
\end{center}

%-------------------------------------------------------------------------------
\framebreak
Optimal Transport

\end{vbframe}
%-------------------------------------------------------------------------------


\endlecture
\end{document}
