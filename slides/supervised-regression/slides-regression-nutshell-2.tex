\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{}


\newcommand{\learninggoals}{
\item 
\item 
\item 
\item 
}

\title{Introduction to Machine Learning}
\date{}
\begin{document}

%\lecturechapter{In a Nutshell: ML-Basics}
\lecturechapter{Supervised Regression: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Linear Regression Tasks}
\begin{itemize}
\item \small Learn linear combination of features for predicting the target variable
\item \small Find parameters with minimal Loss
\end{itemize}
%Include Plots with training and prediction

\begin{columns}  
\begin{column}{0.1\textwidth} 
\begin{center}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
  \includegraphics[width = 0.9\textwidth]{slides/supervised-regression/figure_man/nutshell-regression-learning-task.png}
\end{center}
\end{column}
\end{columns}
%\end{center}
%\begin{center}
\begin{columns}
\begin{column}{0.1\textwidth} 
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
  \includegraphics[width = 0.8\textwidth]{slides/supervised-regression/figure_man/nutshell-regression-prediction-task.png} 
\end{center}
\end{column}
\end{columns}

\end{vbframe}


\begin{vbframe}{Linear Models with L2 and L1 Loss}
\begin{itemize}
\item \small The exact values of the parameters depend on the loss function 
%\item \small One of the most often used Losses is the L2 Loss
%$$\Lxy = (y - \yh)^2 = (y-\fx)^2 = (y-\thx)^2$$
%\item \small Which is a quadratical penalization of residuals \textcolor{blue}{$r = y - \thx$}
\end{itemize}
\begin{columns}  
\begin{column}{0.4\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L1.pdf}
\end{center}
\end{column}
\begin{column}{0.4\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L2.pdf}
\end{center}
\end{column}
\end{columns}

\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L1} \textcolor{blue}{penalizes} the absolute value of residuals \textbf{$r = y - \fx$}
        \item \small Robust to outliers

    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L2} is a quadratical \textcolor{blue}{penalization} of residuals \textbf{$r = y - \fx$}
        \item \small Analytically solvable
        

    \end{itemize}
\end{minipage}


% I
% Scatter plot with residuals -> L2 Loss
% SSE Plots for different models -> select parameters with minimal SSE; part of data variance not explained by model

% II
% Special case L2 Loss linear Rergession: analytical solution
% For the L2 Loss a derivative exists; That means the best parameters can be computed by a formula and no iterative algorithm must be applied.
% Plot of Loss -> smooth function differentiable everywhere

%III
% Interpretation of Model Summary



\end{vbframe}

\begin{vbframe}{Linear Models with L2 and L1 Loss}

\end{vbframe}


%\begin{vbframe}{Linear Classifiers}
%\end{vbframe}

\begin{vbframe}{Polynomial Regression}




\end{vbframe}



\endlecture
\end{document}