## Optimal Partitioning Clustering

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73",
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2,
                     function(x)
                       rgb(x[1], x[2], x[3], alpha=alpha))
}
```

**Hierarchical clustering (HC):**

- Stepwise merging (agglomerative HC) or dividing (divisive HC) of clusters based on distances and linkages.

- The number of clusters are selected by splitting the dendrogram at a specific **height** after visual inspection.

**Partitioning clustering:**

- Partitions observations into a predefined number of $K$ clusters by optimizing a numerical criterion.

- The most common method is $K$-means, which uses **centroids**, i.e., artificial data points located at the mean value of each dimension (but only works for numerical data).

<!-- (uses centroids, i.e., an artificial data point calculated by taking the average in each dimension) -->
<!-- (similar to centroids, but uses the median value instead of the average in each dimension) -->
<!-- (uses medoids, i.e., the data point with minimal average distance to all other points of a cluster) -->

## $K$-means

$K$-means partitions the $N$ observations into $K$ predefined clusters $C_1, C_2, \hdots, C_K$ by minimizing the **compactness**, i.e. the **within-cluster variation** of all clusters using
$$\textstyle\sum_{k=1}^K \sum_{i \in C_k} \|X_i-\bar{X}_k\|_2^2 \rightarrow \min,$$
where $\bar{X}_k = \frac{1}{N_k} \sum_{i \in C_k} X_i$ is the centroid of cluster $k$ and $N_k$ is the number of observations in cluster $k$.

<!--Hence, equivalently we seek a clustering $C$ that minimizes the
within-cluster variation.-->

```{r, echo = FALSE, fig.height=3.5, fig.width=6.5, out.width="0.6\\textwidth", message=FALSE}
Colors = colorspace::rainbow_hcl(3)
library(MASS)
library(cluster)
library(pdist)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-1.5), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,0.25), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar = c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])


centers = as.data.frame(rbind(
  c("x" = e2$loc[1], "y" = e2$loc[2]),
  c("x" = e1$loc[1], "y" = e1$loc[2])))
#lines(centers, col = "black", lwd = 2)
arrows(x0 = centers$x[1], x1 = centers$x[2],
  y0 = centers$y[1], y1 = centers$y[2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(centers)
text(xy[1], xy[2], label = "between cluster variation", col = "black")

text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1], pos = 1)
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2], pos = 1)
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3], pos = 3)

d = predict(e1)
#summary(d)
xy = dat[c(14,16),]
#lines(xy, col = "black", lwd = 2)
arrows(x0 = xy[1,1], x1 = xy[2,1],
  y0 = xy[1,2], y1 = xy[2,2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(xy)
text(xy[1], xy[2], label = "within-cluster variation", col = "black")

```


## $K$-means

**Idea**: Consider all possible partitions of $N$ observations into $K$ clusters and select the one with the lowest **within-cluster variation**.

**Problem**: Requires trying all possible assignments of $N$ observations into $K$ clusters, which in practice is nearly impossible:

|$N$     |$K$     | Number of possible partitions|
|-------:|:------:|:-----------------------------|
|15      | 3      | 2.375.101                    |
|20      | 4      | 45.232.115.901               |
|100     | 5      | $10^{68}$                    |

\tiny
Hothorn, T., Everitt, B. S. (2009). A handbook of statistical analyses using R. Chapman and Hall/CRC. p. 322.

## $K$-means
<!--
The $K$-means clustering algorithm approximately minimizes
the enlarged criterion by alternately minimizing over $C$
and $c_1,\hdots , c_K$

We start with an initial guess for $c_1,\hdots , c_K$ (e.g., pick $K$
points at random over the range of $X_1,\hdots , X_n$), then repeat:

1.  Minimize over $C$: for each $i=1,\hdots , n$, find the
    cluster center $c_k$ closest to $X_i$, and let $C(i)=k$

2.  Minimize over $c_1,\hdots , c_K$: for each
    $k=1,\hdots , K$, let $c_k = \bar{X}_k$, the average of points in
    group $k$

Stop when within-cluster variation doesnâ€™t change

In words:

1.  Cluster (label) each point based the closest center

2.  Replace each center by the average of points in its cluster

## $K$-means example

Here $X_i \in \R^2$, $n=300$, and $K=3$ -->

<!-- ![image](km0.pdf) -->

<!-- ![image](km1.pdf) -->

<!-- ![image](km2.pdf)\ -->
<!-- ![image](km3.pdf) -->

<!-- ![image](km9.pdf) -->

Use an approximation:

1. **Initialization:** Choose $K$ arbitrary observations to be the initial cluster
   centers.

2. **Assignment:** Assign every observation to the cluster with the closest center.

3. **Update:** Compute the new center of each cluster as the mean of its members.

4. Repeat (2) and (3) until the centers do not move.

<!--
1. Find some initial partition of the observations into the required number of clusters.
2. Calculate the change in the clustering criterion produced by "moving" each
individual from its own cluster to another cluster.
3. Make the change that leads to the greatest improvement in the value of the clustering criterion.
4. Repeat steps (2) and (3) until no move of an observation causes the clustering criterion to improve.
-->
<!-- Cool Example: http://www.edureka.co/blog/implementing-kmeans-clustering-on-the-crime-dataset/ -->


## Choice of $K$

- Many methods exist for choosing the number of clusters $K$ (there is no perfect solution).

- The easiest method is to apply $K$-means for different $K$ and plot the **within-cluster variation** for each number of $K$.

- The **within-cluster variation** always decreases with increasing number of clusters.

- An **"elbow"** in the plot might indicate a useful solution.

```{r, echo=FALSE, fig.height=4.5, fig.align="center", out.width="0.7\\textheight"}
set.seed(1)
K = 8
wss = numeric(K)
for (k in 1:K) {
  km = kmeans(iris[,3:4], centers = k, nstart = 10, iter.max = 100, algorithm = "Lloyd")
  wss[k] = km$tot.withinss
}
par(mar = c(4,4,1,1))
plot(1:K, wss, type = "b", xlab = "K (number of clusters)", ylab = "within-cluster variation")
```

## Summary

- Minimizing the **within-cluster variation** exactly is not feasible and can be approximated by the $K$-means algorithm.

- $K$-means always converges, however, the cluster assignments strongly depend on the initial
  centers. \
  $\rightarrow$ repeat it several times with different initial centers.

- The **elbow method** is a simple approach to choose the optimal number of clusters $K$.

- Other partitioning methods similar to $K$-means exist:

  - $K$-medians: uses the **median value** instead of the mean value of each dimension (allows ordinal but not nominal data).
  - $K$-medoids: uses **medoids**, i.e., non-artificial data points with minimal average distance to all other points (also allows nominal data, but is difficult to find).

<!-- - Without the knowledge of the real groups, one can use *indices*, which combine *compactness* and *separation* measures, to validate the clustering assignments. -->

<!-- - A simple solution for choosing the number of clusters $K$ is to plot the **within-cluster variation** for several $K$ and look for an **"elbow"** which is a good guess for $K$. -->
