## Motivation for Clustering

Consider multivariate data with $N$ observations (e.g. customers) and $P$
features (e.g. characteristics of customers).

Task: divide data into groups (clusters), such that

- the observations in each cluster are as "similar" as possible (homogeneity
  within each cluster), and

- the clusters are as "far away" as possible from other clusters (heterogeneity
  between different clusters).

## Clustering vs. Classification

- In classification, the groups are known and we try to learn what
  differentiates these groups (i.e., learn a classification function) to
  properly classify future data.

- In clustering, we look at data, where groups are unknown and try to find
  similar groups.

Why do we need clustering?

- Discovery: looking for new insights in the data (e.g. finding groups of customers that buy a similar product).

- Derive a reduced representation of the full data set.


## Hierarchical Clustering
```{r include=FALSE, cache=FALSE}
library(knitr)
root = rprojroot::find_root(rprojroot::is_git_root)
ap = adjust_path(paste0(getwd(), "/figure"))
```

Hierarchical clustering is a recursive process that builds a hierarchy of clusters.
We distinguish between:

1. Agglomerative (or bottom-up) clustering:
    - Start: Each observations is an *individual cluster*.
    - Repeat: Merge the two closest clusters.
    - Stop when there is only one cluster left.

2. Divisive (or top-down) clustering:
    - Start: All observations are within *one* cluster.
    - Repeat: Divide the cluster that results in two clusters with biggest distance.
    - Stop when each observation is an individual cluster.

<!-- We focus on agglomerative clustering methods as they are simpler. -->

## Hierarchical Clustering

Let $X_1,\hdots , X_N$ be observations with $P$ features (dimensions), where
$X_i = (x_{i1}, \ldots, x_{iP})^\top$. A data set is a (N $\times$ P)-matrix of
the form:

|        | feature $1$ | $\hdots$   | $\hdots$    | feature $P$|
|:------:|:-----------:|:----------:|:-----------:|:----------:|
|$X_1$   |     $x_{11}$|    $\hdots$|     $\hdots$|    $x_{1P}$|
|$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$|
|$X_N$   |     $x_{N1}$|    $\hdots$|     $\hdots$|    $x_{NP}$|

<!-- |$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$| -->
<!-- |$X_{150}$|         5.9|         3.0|          5.1|         1.8| -->

## Hierarchical Clustering

For hierarchical clustering, we need a definition for

- distances $d(X_i, X_j)$ between two observations $X_i$ and $X_j$:

    - manhattan distance:
      $$d(X_i,X_j)= ||X_i - X_j||_1 = \sum_{k=1}^P|x_{ik}-x_{jk}|$$
    - euclidean distance:
      $$d(X_i,X_j)= ||X_i - X_j||_2 = \sqrt{\sum_{k=1}^P(x_{ik}-x_{jk})^2}$$

- distances between two clusters (called linkage).

<!--
 -  Let $K$ be the number of clusters.
 - A clustering of observations $X_1,\hdots , X_N$ is a function $C$ that
   assigns each observation $X_i$ to a cluster $k \in \{1,\hdots , K\}$.
-->

## Distances between Observations

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73",
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
```

```{r,echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth"}
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19,
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(X[i]), expression(X[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(2,1))

text(x = 5, y = 1, expression(d(X[i],X[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2)

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(X[i],X[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
```


- \small manhattan: sum up the absolute distances in each dimension.

    In R: `dist(data, method = "manhattan")`

- \small euclidean: remember Pythagoras theorem from school?

    In R: `dist(data, method = "euclidean")`

- \small gower: can be used for mixed variables (categorical and numeric).

    In R: `daisy(data, metric = "gower)` from the `cluster` package

- \small see `?dist` for other distances.

## Gower Distance I

- The Gower's metric calculates the distance between two observations for each feature separately.

- For a categorical feature $X_k$, the distance between the $i$-th and the $j$-th observation ($X_{ik}$ and $X_{jk}$) is defined by
$$s_{ijk}=\begin{cases} 0\ \text{if}\ X_{ik}=X_{jk} \\ 1\ \text{if}\ X_{ik}\neq X_{jk}.\end{cases}$$

- For a numerical feature $X_k$, the Manhattan distance (scaled to the range $[0,1]$) is used:
$$s_{ijk}=\frac{|X_{ik}-X_{jk}|}{\max(X_k)-\min(X_k)}, \;\;\; \text{so that } 0 \leq s_{ijk} \leq 1.$$

## Gower Distance II

The Gower's metric between two observations $i$ and $j$ combines all individual distances $s_{ijk}$ of all $P$ features by
$$S_{ij}=\dfrac{\sum_{k=1}^P w_{k} s_{ijk}}{\sum_{k=1}^P w_{k}}.$$

  - $w_k$: weight for feature $k$ (typically $w_k = 1$).
  - $s_{ijk}$: distance between $i$-th and $j$-th observation of feature $k$.

\begin{center}
\includegraphics[width=0.8\textwidth]{`r ap("gower_distance_2.png")`}
\end{center}

## Distances between Observations

**Remember**: Normalize the data before computing distances, especially when the scale of features is different, e.g.:

```{r, echo=FALSE, fig.align="center", fig.height=3.5, fig.width=7.5}
par(mar = c(3.5,3.5,1,1), mfrow = c(1,3), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))


dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",
  pch = 19, xlim = range(dat$shoe.size)*c(1.5, 1.5), ylim = range(dat$height)*c(1.5, 1.5))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(X[1],X[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))
```

Normalization of $X_{\texttt{shoe.size}}$ means $\tfrac{X_{\texttt{shoe.size}}-\texttt{mean}(X_{\texttt{shoe.size}})}{\texttt{sd}(X_{\texttt{shoe.size}})}.$

<!-- ## Distances between Observations -->

<!-- \small -->
<!-- The normalized feature $\tilde{X}_{\texttt{height}}$ is computed using  -->
<!-- $X_{\texttt{height}}$ by -->
<!-- \[\tilde{X}_{\texttt{height}} = \tfrac{X_{\texttt{height}}-\texttt{mean}(X_{\texttt{height}})}{\texttt{sd}(X_{\texttt{height}})}.\] -->

<!-- Distances based on normalized data are better comparable and **robust** in terms of linear transformations (e.g., conversion of physical units). -->

<!-- ```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.55\\textwidth"} -->
<!-- par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0)) -->

<!-- #dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72)) -->
<!-- dat = as.data.frame(scale(dat)) -->
<!-- plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",  -->
<!--   pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1)) -->
<!-- lines(x = dat$shoe.size[-3], y = dat$height[-3]) -->
<!-- asp = getCurrentAspect() -->
<!-- text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),  -->
<!--   bquote(paste(d(X[1],X[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),  -->
<!--   adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp)) -->
<!-- text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5)) -->
<!-- ``` -->

## Distances between Clusters (Linkage)

- Assume that all observations $X_1,\hdots , X_N$ belong to $K<N$ different clusters.

- The linkage of two clusters $C_r$ and $C_s$ is a "score" describing their distance.

The most popular and simplest linkages are

- *Single Linkage* <!-- the shortest distance between two observations in each cluster.  -->

- *Complete Linkage* <!-- the longest distance between two observations in each cluster. -->

- *Average Linkage* <!-- the average distance between each observation in one cluster to every observation in the other cluster. -->

- *Centroid Linkage*

```{r, echo=FALSE, message=FALSE}
library(MASS)
library(cluster)
library(pdist)
set.seed(12345)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,2), Sigma = diag(2))
cl2 = mvrnorm(n = n, mu = c(1,-3), Sigma = diag(2)*2)
e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
dat = rbind(cl1, cl2)
d = as.matrix(pdist(cl1, cl2))
```

## Single Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
single = which(d==min(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[single[1],], cl2[single[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Single linkage defines <!-- we compute all pairwise distances for observations in
opposite clusters.--> the distance of the *closest point pairs* from different clusters as the distance between two clusters:

\[d_{\text{single}}(C_r,C_s) = \min_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Complete Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
complete = which(d==max(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[complete[1],], cl2[complete[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Complete linkage defines the distance of the *furthest point pairs* of different clusters as
the distance between two clusters:

\[d_{\text{complete}}(C_r,C_s) = \max_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Average Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

for(i in 1:nrow(cl2)) lines(rbind(cl1[single[1],], cl2[i,]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

\begin{center}
{\scriptsize (Note: Plot only shows distances between all green points and
\textit{one} red point)}
\end{center}

In average linkage, the distance between two clusters is defined as the average
distance across *all* pairs of two different clusters.

## Centroid Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.65\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)#, adj = c(0, 0.5))
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)#, adj = c(1, 0.5))


#points(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), pch = 1, cex = 4)
lines(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), type = "b", cex = 3.5)
```

\small
Centroid linkage defines the distance between two clusters as the distance between the two cluster centroids.
The centroid of a cluster $C_s$ with $N_s$ points is the mean value of each dimension:

\[\bar{X}_s = \frac{1}{N_s} \sum_{i \in C_s} X_i\]

## Example: Hierarchical Clustering

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$ \\
% Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$\\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$ \\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{red}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: ${\color{blue}\{1,2,3\}},{\color{red}\{4,5\}}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1:3, 1),], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: $\{1,2,3\},\{4,5\}$\\
Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1,2,3,5,4,1),], col = "blue")
```

\end{column}
\end{columns}

## Dendrogram

A Dendrogram is a tree showing which clusters / observations are merged after each step.
The `height' is proportional to the distance between the two merged clusters:

```{r, echo = FALSE, results='hide'}
# compute distance matrix
euclid = dist(simple.data, method = "euclidean")
# do clustering with complete linkage
agglo = hclust(euclid, method = "complete")
agglo
```

```{r, echo=FALSE, out.width="\\textwidth", fig.width=8, fig.height=4}
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))

par(mar = c(2,3,1,4))
plot(agglo, hang = -1, ylab = "", axes = F)
axis(2, line = 0, padj = 0.5)
title(ylab = "Height", line = 2)
abline(h = c(0, agglo$height), lty = 2, col = "gray70")
text(x = 5, y = c(0, agglo$height), labels = paste("Step ", 1:5), xpd = TRUE, col = "gray70", pos = 4)
```

<!-- # Dendrogram -->

<!-- A Dendrogram is a tree that visualizes the merging of clusters: -->

<!-- - Each leaf node is a singleton (i.e., a cluster containing a single  -->
<!--   observation). -->

<!-- - Each internal node has two daughter nodes (children), representing -->
<!--   the clusters that were merged to form it.  -->

<!-- - Each node represents a cluster. -->

<!-- - The root node is the cluster containing all observations. -->

<!-- - The $y$ axis shows the distance between two clusters when they were merged. -->

<!-- Remember: the choice of linkage determines how the distance between clusters is  -->
<!-- measured.  -->

<!-- If we fix the leaf nodes at height zero, then each internal node is
drawn at a height proportional to the dissmilarity between
its two daughter nodes -->

## Summary

- Hierarchical agglomerative cluster methods **iteratively** merges observations/clusters until all observations are in one single cluster.

- A **dendogram** visualizes the hierarchy of all possible cluster assignments.
Each node of the dendrogram represents a cluster and its `height' is proportional to the distance of its child nodes.

- The most common linkage functions are **single, complete, average** and **centroid** linkage.
There is no perfect linkage and each linkage has its own advantages and disadvantages.
