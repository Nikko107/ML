\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/automl2.png}
\newcommand{\learninggoals}{
  \item ...
  \item ...
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Functional Features}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{What is Functional Data}
    \begin{center}
        \includegraphics[height = 5cm]{figure_man/definition.png}
    \end{center}
    
    Functional or sequence data has a (temporal) order between (some) features.
\end{vbframe}

\begin{vbframe}{Functional Data - Example: Energy Usage}
    
    \begin{center}
        \includegraphics[width=0.9\textwidth]{figure_man/energy-usage.png}
    \end{center}
    
    
\end{vbframe}

\begin{vbframe}{Handling functional features - 3 Ways}
    
    1. Ignore the structure and let the model figure it out.
    \vspace*{-0.6cm}
    \begin{center}
        \includegraphics[width = 0.7\textwidth]{figure_man/tree.png}
    \end{center}
    \vspace*{-1cm}
    
    \begin{itemize}
        \item Can be quite difficult for the model to implicitly learn and utilize the structure.
        \item Requires complex models that can model high-level interactions between features, e.g., (boosted) trees.
        \item No \textbf{translation invariance}, i.e., structure has to be learned for every position in the timeseries seperately.
    \end{itemize}
    
    \framebreak
    
    2. Use specialized machine learning algorithms that can handle functional features.
    
    \vspace*{-0.6cm}
    
    \begin{center}
        \includegraphics[width = 0.7\textwidth]{figure_man/custom.png}
    \end{center}
    
    \vspace*{-1cm}
    
    \begin{itemize}
        \item Extensions of existing algorithms, e.g., functional k-nearest neighbor using \textbf{dynamic time warping} as distance metric.
        \item \textbf{Theoretical} correct/optimal way to handle functional features.
        \item \textbf{But:} Less flexibility in model choice and (oftentimes) less efficient implementations.
    \end{itemize}
    
    \begin{footnotesize}
        \textbf{Note:} Deep neural networks (both CNNs and RNNs) can be used for functional data. But depending on the overall data structure not always the best choice.
    \end{footnotesize}
    
    \framebreak
    
    3. Use feature engineering to extract information from functional structure.
    
    \vspace*{-0.2cm}
    
    \begin{center}
        \includegraphics[width = 0.7\textwidth]{figure_man/extract.png}
    \end{center}
    
    \vspace*{-0.6cm}
    
    \begin{itemize}
        \item Large number of possible \textbf{extractors}, both simple and complex.
        \item Allows application of any standard machine learning algorithms.
        \item Thus, very flexible approach.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Feature Extraction for Functional Data}
    Simple descriptive statistics:
    \begin{itemize}
        \item min, max, mean, variance, ...
    \end{itemize}
    \vspace{+.4cm}
    
    More complex transformations:
    \begin{itemize}
        \item Fourier Transformation
        \item Functional Principal Components
        \item Wavelets
        \item Spline Coefficients
        \item ...
    \end{itemize}
    
    \textbf{Problem:} Often unclear which of these techniques to use.
    
    \vspace{+.2cm}
    
    \textbf{Solution:} Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition and use tuning strategies (can get quite expensive!)
\end{vbframe}

\begin{vbframe}{Example - Ames Housing}
    
    
    \footnotesize
    \begin{center}
        \begin{tabular}{c|c|c|c|c}
            \hline
            house.id & mean.energy & var.energy & max.energy & ... \\
            \hline
            2279     & 0.081       & 0.009      & 0.943      & ... \\
            \hline
            1590     & 0.038       & 0.009      & 1.446      & ... \\
            \hline
            2921     & 0.057       & 0.008      & 0.512      & ... \\
            \hline
            1648     & 0.050       & 0.008      & 0.616      & ... \\
            \hline
            484      & 0.056       & 0.008      & 0.490      & ... \\
            \hline
        \end{tabular}
    \end{center}
    
    
    \normalsize
    \vspace{+.4cm}
    \begin{itemize}
        \item Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
        \item More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Comparison}
    
    \begin{center}
        \includegraphics[width=0.9\textwidth]{figure_man/comparison.png}
    \end{center}
    
\end{vbframe}


\endlecture
\end{document}
