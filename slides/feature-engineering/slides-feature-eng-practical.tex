\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/automl2.png}
\newcommand{\learninggoals}{
  \item ...
  \item ...
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Practical Feature Engineering}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Feature Engineering in Practice}
    There are generally two ways of doing feature engineering in practice:
    \medskip
    
    1. Manual Feature Engineering
    \medskip
    
    Trial and error with educated guesses what methods might be important/useful to apply to the data
    
    \vspace*{1cm}
    
    2. (Semi-)Automatic Feature Engineering
    \medskip
    
    Define a set of feature engineering operations and let an optimizer search for a well working pipeline
    
    \vspace*{1cm}
    
    \textbf{Important:} It is crucial for both approaches that this process is embedded in a nested cross-validation loop.
\end{vbframe}

\begin{vbframe}{(Semi-)Automatic Feature Engineering}
    \begin{center}
        \includegraphics[width= 5.5cm, height=5cm]{figure_man/dag.png}
    \end{center}
    
    \begin{itemize}
        \item Define a space of possible operations from the previous chapters and let an optimizer search an optimal pipeline
        \item If model choice and hyperparameters are included in the search, this process is called \textbf{Auto}matic \textbf{M}achine \textbf{L}earning (AutoML)
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Stacking and AutoML}
    \begin{itemize}
        \item AutoML approaches create a large number of models while searching for an optimal pipeline
        \item To further boost the pipeline performance, multiple pipelines can be \textbf{stacked} in a post-processing step
    \end{itemize}
    \medskip
    
    \begin{center}
        \includegraphics[height=4.5cm]{figure_man/stacking.png}
    \end{center}
\end{vbframe}


% \begin{vbframe}{Deployment of Pipelines}
% The complexity of a machine learning pipeline can be restricted due to the field of application after \textbf{deployment}.
% \medskip

% Questions to answer before any feature engineering and pipeline construction can take place:
% \medskip

% \begin{itemize}
% \item In which environment is the pipelines deployed?
% \item How is the data streamed into the pipeline?
% \item How often and how fast is the pipeline triggered?
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Production Environments}
% The environment in which the pipeline is deployed can severely restrict the complexity.
% \medskip

% \begin{itemize}
% \item Interpreters (R/Python/...) can be outdated, packages can not be available...
% \item No interpreters are available at all
% \item Specialized data format
% \end{itemize}
% \medskip

% \textbf{Solutions:}
% \medskip

% \begin{itemize}
% \item Containerization: Model is integrated into a (minimal) virtual machine with correct versions of interpreters and packages.
% \item Services: Create a (rest-)API that can be triggered by sending data and receiving predictions.
% \end{itemize}
% \medskip

% This can also solve the problem of scalability since containerized services can easily be scaled
% \end{vbframe}

% \begin{vbframe}{Simple Example}
% \begin{center}
% \includegraphics[width=11cm]{figure_man/scaling.pdf}
% \end{center}
% \end{vbframe}

\begin{vbframe}{Feature Engineering and Domain Knowledge}
    \vspace{+.4cm}
    
    \begin{itemize}
        \item Some forms of feature engineering are very hard to automate.
        \item Information that is not present in the data can not be found automatically.
        \item This is why we still need humans with domain knowledge to find optimal models.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Domain Knowledge: Example}
    \vspace{+.4cm}
    
    A simple example can be spatial information hidden in categorical features:
    \vspace{+.4cm}
    
    \footnotesize
    \begin{tabular}{c|c|c|c|c|c|c}
        \hline
        Neighborhood & BrDale & IDOTRR & Blueste & Greens & SawyerW & NWAmes \\
        \hline
        n            & 30     & 93     & 10      & 8      & 125     & 131    \\
    \end{tabular}
    
    \vspace{0.3cm}
    
    \normalsize
    The \textbf{Neighborhood} feature does not directly include which houses are close to each other across different neighborhoods.
    \medskip
    
    Some manual preprocessing / feature engineering is still required to enrich this data with actual spatial information.
\end{vbframe}

\begin{vbframe}{Human-in-the-loop Approaches}
    \vspace{+.4cm}
    
    With better software it becomes easier for humans to integrate such knowledge.
    \medskip
    
    Examples:
    \medskip
    
    \begin{itemize}
        \item Features could be tagged as \textbf{spatial} and trigger an automatic GoogleMaps API query, adding longitude and latitude to the data.
        \item Feature groups can be tagged, e.g., revenues of different departments, where adding or averaging multiple features can be beneficial.
    \end{itemize}
    \medskip
    
    Random or exhaustive combination of features quickly becomes infeasible due to exponential growth in possible combinations with more features.
\end{vbframe}

% \begin{vbframe}{Relational Datasets}
% \vspace{+.4cm}

% \begin{itemize}
% \item Up until now we assumed that a single dataset exists (usually with i.i.d. observations)
% \item In many application additional data exists that is linked by \textbf{keys}, e.g., \text{house\_id} or \text{customer\_id}.
% \item Often \textbf{n-to-1} relationship between main dataset and datasets containing additional information
% \item Machine learning algorithms usually cannot automatically run on such linked datasets.
% \end{itemize}
% \medskip

% \textbf{Aggregate information from linked dataset and add to main dataset}
% \end{vbframe}

% \begin{vbframe}{Example - Ames Housing}
% \begin{itemize}
% \item We can have multiple events of refurbishments of the houses. These should help estimate the selling price:
% \end{itemize}
% \medskip

% <<echo=FALSE, message=FALSE, warning = FALSE>>=
% data.frame(
%   "House_id" = c(997, 997, 997, 17, 855),
%   "Refurbishment" = c("bath", "windows", "floor", "windows", "windows"),
%   "Cost" = c(7500, 12000, 800, 700, 1100),
%   "Year" = c(2006, 2006, 2008, 2007, 2007),
%   "..." = rep("...", times = 5)) %>%
%   knitr::kable(format = 'latex') %>%
%   kableExtra::kable_styling(latex_options = 'HOLD_position', font_size = 7)
% @
% \medskip

% \begin{itemize}
% \item Different extractions can be performed, e.g., number of refurbishments, overall cost:
% \end{itemize}
% \medskip

% <<echo=FALSE, message=FALSE, warning = FALSE>>=
% data.frame(
%   "House_id" = c(997, 17, 855),
%   "n_refurbishment" = c(3, 1, 1),
%   "sum_cost" = c(20300, 700, 1100),
%   "avg_cost" = c(6767, 700, 1100),
%   "last_ref" = c(2008, 2007, 2007),
%   "..." = rep("...", times = 3)) %>%
%   knitr::kable(format = 'latex') %>%
%   kableExtra::kable_styling(latex_options = 'HOLD_position', font_size = 7)
% @
% \end{vbframe}

% \begin{vbframe}{Automatic Feature Engineering for Relational Data}

% \begin{center}
% \includegraphics[height=3.7cm, width = 6cm]{figure_man/n-to-1.pdf}
% \end{center}

% \begin{itemize}
% \item Oftentimes more than 2 datasets exist, linked by multiple keys.
% \item Domain knowledge is a requirement to do meaningful feature engineering if the relations get more complex.
% \item First approaches for automatic feature engineering for relational datasets exist, e.g., \textbf{featuretools} by using \textbf{Deep Feature Synthesis}.
% \end{itemize}
% \end{vbframe}


\endlecture
\end{document}

