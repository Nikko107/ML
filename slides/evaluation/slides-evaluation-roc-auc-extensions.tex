\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}

\newcommand{\titlefigure}{figure/eval_mclass_roc_sp_13}
\newcommand{\learninggoals}{
\item Understand partial AUC and its motivation 
\item Understand multi-class AUC variants
}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: AUC Extensions}
\lecture{Introduction to Machine Learning}
\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Partial AUC (pAUC)}

\begin{itemize}
  \item Can be useful to limit region under ROC curve
  % curve $\Rightarrow$ partial AUC (pAUC).
  % \item This leads to partial AUC (pAUC), originally proposed by 
  % \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}
  % {McClish (1989)}.
  \item E.g. FPR > 0.2 or TPR < 0.8 might not be acceptable for task, then 
      we don't want to integrate over that region
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.9\textwidth]{figure/eval_mclass_roc_sp_13}}
\end{knitrout}

\end{vbframe}

\begin{vbframe}{Corrected Partial AUC}

\begin{itemize}
  \item Range of pAUC depends on cut-off value for region 
      % used to 
  % determine the region of interest $\Rightarrow \text{pAUC} \in [0, c_2 - c_1]$.
  % \item For standard AUC: $c_1 = 0$ and $c_2 = 1$.
  \item We can normalize to $[0, 1]$:
  $$\text{pAUC}_\text{corrected} = \cfrac{1}{2} \left( 1 + \cfrac{\text{pAUC} - 
  \text{AUC}_{\text{min}}}{\text{AUC}_{\text{max}} - \text{AUC}_{\text{min}}} 
  \right),$$
  where
  \begin{itemize}
    \item $\text{AUC}_{\text{min}}$ is the value of the non-discriminant AUC, 
    and
    \item $\text{AUC}_{\text{max}}$ is the maximum possible AUC in the region.
  \end{itemize}
  % \item As usual, $\text{pAUC} = 0.5$ corresponds to the non-discriminant and 
  % $\text{pAUC} = 1$ to the perfect classifier.

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{ROC Measure under Contraint}

BB CHECKEN hier

\begin{itemize}
  \item Applications where TPR and FPR are treated 
  asymmetrically often occur in biomedical contexts.
  \item For example, 
  \href{https://clincancerres.aacrjournals.org/content/16/24/6111}
  {Wild et al. (2010)} fix FPR on a certain value in their study of biomarkers for the detection 
  of colorectal cancer.
  \item TPR, i.e., being able to correctly detect present diseases, is 
  crucial in this setting.
  \item At the same time, high TPR is only useful if the classifier also 
  achieves low FPR. \\
  $\rightarrow$ Otherwise, healthy patients might receive costly and unnecessary treatment.
  \item It is therefore reasonable to define an upper bound for FPR and 
  evaluate learners on the resulting pAUC.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{vbframe}{Multi-class AUC}

% \small

\begin{itemize}
  \small
  \item In its original form, AUC (as the other ROC metrics) is defined for the 
  binary-class case.
  \item We can extend AUC to \textbf{multi-class} classification, where 
  estimating the area under the ROC curve evolves into estimating the 
  hypervolume under the ROC surface.
  \item This can be achieved by considering a set of two-dimensional curves, 
  resulting from binary comparisons, and subsequent aggregation. \\
  $\rightarrow$ In principle, we have the choice between one-vs-one and 
  one-vs-rest comparisons.
\end{itemize}  

\vfill
 
\begin{minipage}[c]{0.75\textwidth}
  \centering
  \includegraphics[trim = 0 40 -20 40, clip, width=\textwidth]
  {figure/eval_auc_extensions}
\end{minipage}%
\begin{minipage}[c]{0.25\textwidth}
  \scriptsize
  \raggedright
  One-vs-one comparisons between classes for classification of \texttt{iris} 
  species with LDA according to sepal width.
\end{minipage}
 
% \begin{center}
%   \includegraphics[trim = 0 40 0 40, clip, width=0.5\textwidth]
%   {figure/eval_auc_extensions}
% \end{center}
 
\framebreak
  

  \small
For one-vs-one comparison,
  \href{https://link.springer.com/article/10.1023/A:1010920819831}
  {Hand and Till (2001)} proposed to average the AUC of respective pairwise 
  comparisons between two classes:
  $$\text{AUC}_{MC} = \frac{2}{g(g - 1)} \sum_{k < \ell} \text{AUC}(k, \ell) 
    \in [0, 1].$$
  \begin{itemize}
    \small
    % \small
  % , where 
  % the classifier predicts the probability $\pik$ of belonging to class $k$ for 
  % each class $k \in \setg$.
  % \item \href{https://link.springer.com/article/10.1023/A:1010920819831}
  % {Hand and Till (2001)} proposed to average the AUC of pairwise comparisons 
  % (one-vs-one) of a multi-class classifier.
    \item $\text{AUC}(k ~|~ \ell)$ is the 
    probability that a member of class $k$ 
    has a lower probability of belonging to class $\ell$ than a member of class $\ell$, for all pairs of classes $k, \ell \in \gset$.
    \item For $g = 2$, $\text{AUC}(k ~|~ \ell) = 
    \text{AUC}(\ell ~|~ k)$, but not necessarily for $g > 2$.
    \item However, since class identifiability is immune to any bijective 
    transformation of the labels, %we cannot distinguish $\text{AUC}(k ~|~ \ell)$ 
    %from $\text{AUC}(\ell ~|~ k)$, so 
    we set $\text{AUC}(k, \ell) = \frac{1}{2} \cdot [\text{AUC}(k ~|~ \ell) + 
    \text{AUC}(\ell ~|~ k)]$.
  
  % \begin{itemize}
  %   \item Compute $\text{AUC}(i,j)$ for each pair of classes $i, j \in \setg$.
  %   \item $\text{AUC}(i,j)$ is the probability that a randomly drawn member of 
  %   class $i$ has a lower probability of belonging to class $j$
  %   than a randomly drawn member of class $j$.
  %   \item For $g$ classes, we have ${{g}\choose{2}} = \tfrac{g (g-1)}{2}$ values 
  %   of $\text{AUC}(i,j)$, which are then averaged to compute the multi-class 
  %   AUC.
  % \end{itemize}
  
  %  \item Averaging over all pairs of classes yields the overall 
   % $\text{AUC}_{MC}$ as a multi-class performance metric:
    %$$\text{AUC}_{MC} = \frac{2}{g(g - 1)} \sum_{k < \ell} \text{AUC}(k, \ell) 
    %\in [0, 1].$$
    \item This reduces to the standard AUC for the binary case.
    % \item Again, a non-discriminant classifier will have $\text{AUC}_{MC} = 0.5$
    % as all pairwise probability estimates equal 0.5.
  \end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
