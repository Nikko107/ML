\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}

\newcommand{\titlefigure}{figure/eval_mclass_roc_sp_13}
\newcommand{\learninggoals}{
\item Understand why pAUC is a reasonable metric in some contexts
\item Know how pAUC is computed and normalized
\item Understand multi-class AUC}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: AUC Extensions}
\lecture{Introduction to Machine Learning}
\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Partial AUC}

\begin{itemize}
  \item Sometimes it can be useful to look at a specific region under the ROC
  curve $\Rightarrow$ partial AUC (pAUC).
  % \item This leads to partial AUC (pAUC), originally proposed by
  % \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}
  % {McClish (1989)}.
  \item For example, we might focus on a region with low FPR or a region with
  high TPR:
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.9\textwidth]{figure/eval_mclass_roc_sp_13}}
\end{knitrout}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Partial AUC -- example}

\begin{itemize}
  \item Applications where sensitivity and specificity are treated
  asymmetrically often occur in biomedical contexts.
  \item For example,
  \href{https://clincancerres.aacrjournals.org/content/16/24/6111}
  {Wild et al. (2010)} used pAUC in their study of biomarkers for the detection
  of colorectal cancer.
  \item Sensitivity, i.e., being able to correctly detect present diseases, is
  crucial in this setting.
  \item At the same time, high sensitivity is only useful if the classifier also
  achieves high specificity. \\
  $\rightarrow$ Otherwise, healthy patients might receive costly and entirely
  unnecessary treatment.
  \item It is therefore reasonable to demand a certain level of specificity and
  evaluate/optimize learners on the resulting pAUC.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Corrected Partial AUC}

\begin{itemize}
  \item The scale of the partial AUC depends on the FPR cut-off values used to
  determine the region of interest $\Rightarrow \text{pAUC} \in [0, c_2 - c_1]$.
  \item For standard AUC, we have $c_1 = 0$ and $c_2 = 1$.
  \item We can scale pAUC to take on values in $[0, 1]$ again:
  $$\text{pAUC}_\text{corrected} = \cfrac{1}{2} \left( 1 + \cfrac{\text{pAUC} -
  \text{AUC}_{\text{min}}}{\text{AUC}_{\text{max}} - \text{AUC}_{\text{min}}}
  \right),$$
  where
  \begin{itemize}
    \item $\text{AUC}_{\text{min}}$ is the value of the non-discriminant AUC,
    and
    \item $\text{AUC}_{\text{max}}$ is the maximum possible AUC in the region.
  \end{itemize}
  % \item As usual, $\text{pAUC} = 0.5$ corresponds to the non-discriminant and
  % $\text{pAUC} = 1$ to the perfect classifier.
  \lz
  \item NB: using pAUC means casting aside parts of the information
  deliberately.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Multi-class AUC}

% \small

\begin{itemize}
  \small
  \item AUC (as other ROC metrics) is usually defined for binary classification.
  \item Different ways to estimate a \textbf{multi-class AUC} have been proposed.
  % We can extend AUC to \textbf{multi-class} classification, where
  % estimating the area under the ROC curve evolves into estimating the
  % hypervolume under the ROC surface.
  \item Many of these definitions are based on aggregating AUC values
  %a set of two-dimensional ROC curves
  that result from binary comparisons (e.g., one-vs-one or one-vs-rest).
  %$\rightarrow$ In principle, we have the choice between one-vs-one and one-vs-rest comparisons.
\end{itemize}

%\vfill

\textbf{Example:} One-vs-one comparisons between classes for classification of \texttt{iris} species with LDA according to sepal width.

\centering
\includegraphics[trim = 0 40 -20 40, clip, width=\textwidth]{figure/eval_auc_extensions}

\textbf{Note:} AUC of setosa vs. versicolor $\neq$ AUC of versicolor vs. setosa.

% \begin{minipage}[c]{0.75\textwidth}
%   \centering
%   \includegraphics[trim = 0 40 -20 40, clip, width=\textwidth]
%   {figure/eval_auc_extensions}
% \end{minipage}%
% \begin{minipage}[c]{0.25\textwidth}
%   \scriptsize
%   \raggedright
% \end{minipage}

% \begin{center}
%   \includegraphics[trim = 0 40 0 40, clip, width=0.5\textwidth]
%   {figure/eval_auc_extensions}
% \end{center}
%
% \framebreak
%
% \begin{itemize}
%   \small
%   \item \href{https://link.springer.com/article/10.1023/A:1010920819831}
%   {Hand and Till (2001)} proposed to average the AUC of pairwise comparisons between two classes.
%   \item Remember:
%   \begin{itemize}
%     \small
%     % \small
%   % , where
%   % the classifier predicts the probability $\pik$ of belonging to class $k$ for
%   % each class $k \in \setg$.
%   % \item \href{https://link.springer.com/article/10.1023/A:1010920819831}
%   % {Hand and Till (2001)} proposed to average the AUC of pairwise comparisons
%   % (one-vs-one) of a multi-class classifier.
%     \item First, compute for all pairs of classes $k, \ell \in \gset$ the
%     probability $\text{AUC}(k ~|~ \ell)$ of a randomly drawn member of class $k$
%     having a lower probability of belonging to class $\ell$ than a randomly drawn
%     member of class $\ell$.
%     \item For $g = 2$, we have $\text{AUC}(k ~|~ \ell) =
%     \text{AUC}(\ell ~|~ k)$, but not necessarily so for $g > 2$.
%     \item However, since class identifiability is immune to any bijective
%     transformation of the labels, we cannot distinguish $\text{AUC}(k ~|~ \ell)$
%     from $\text{AUC}(\ell ~|~ k)$, so we set
%     $\text{AUC}(k, \ell) = \frac{1}{2} \cdot [\text{AUC}(k ~|~ \ell) +
%     \text{AUC}(\ell ~|~ k)]$.
%
%   % \begin{itemize}
%   %   \item Compute $\text{AUC}(i,j)$ for each pair of classes $i, j \in \setg$.
%   %   \item $\text{AUC}(i,j)$ is the probability that a randomly drawn member of
%   %   class $i$ has a lower probability of belonging to class $j$
%   %   than a randomly drawn member of class $j$.
%   %   \item For $g$ classes, we have ${{g}\choose{2}} = \tfrac{g (g-1)}{2}$ values
%   %   of $\text{AUC}(i,j)$, which are then averaged to compute the multi-class
%   %   AUC.
%   % \end{itemize}
%
%     \item Averaging over all pairs of classes yields the overall
%     $\text{AUC}_{MC}$ as a multi-class performance metric:
%     $$\text{AUC}_{MC} = \frac{2}{g(g + 1)} \sum_{k < \ell} \text{AUC}(k, \ell)
%     \in [0, 1].$$
%     \item This reduces to the standard AUC for the binary case.
%     % \item Again, a non-discriminant classifier will have $\text{AUC}_{MC} = 0.5$
%     % as all pairwise probability estimates equal 0.5.
%   \end{itemize}
% \end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Multi-class AUC}

\begin{itemize}
%\small
%\item A multiclass classifier predicts $\hat \pi_k$ for all classes $k \in \gset$, i.e., the probability of an observation belonging to class $k$.
%\item Recall: $\hat \pi_k$ is the probability of an observation belonging to class $k$.

\item Let $\text{AUC}(k ~|~ \ell)$ be the AUC of a classifier for class $k$ and $l$, where $k$ (first index) refers to the positive and $\ell$ to the negative class.

\item $\text{AUC}(k ~|~ \ell)$ is computed by subsetting the predicted probability matrix according to class $k$ and $l$ using $\hat \pi_k$ as scores.

\item It can be interpreted as the probability that a randomly drawn member of class $\ell$ has a lower probability of belonging to class $k$ than a randomly drawn member of class $k$.
%$\Rightarrow$ Known interpretation of the AUC as probability.

%\item For $g = 2$, $\text{AUC}(k ~|~ \ell) = \text{AUC}(\ell ~|~ k)$, but not necessarily for $g > 2$.
%\item The first index in $\text{AUC}(k ~|~ \ell)$ refers to the positive class (here: $k$) and the second index to the negative class (here: $\ell$).
%For $g > 2$, $\text{AUC}(k ~|~ \ell) \neq \text{AUC}(\ell ~ | k)$.
%(while for $g = 2$ $\text{AUC}(k ~|~ \ell) = \text{AUC}(\ell ~|~ k)$).
%\item Let $\text{AUC}(k ~|~ \ell)$ be the probability of our classifier ranking a random drawn member of class $\ell$ higher than a random drawn member of class $\ell$.
\end{itemize}

%\framebreak

\textbf{Example:} Compute AUC(3|1) when $g=3$ classes are available
%\begin{center}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
\centerline{\includegraphics[trim = 250 15 00 10, clip, width=\textwidth]{figure_man/multiclass-auc.pdf}}
\end{column}
\begin{column}{0.7\textwidth}
\begin{enumerate}
\small
\item Subset the probability matrix according to class $k=3$ and $\ell = 1$.
\item Define $k=3$ as the positive class and $\ell = 1$ as the negative class.
\item Compute the standard AUC using $\hat \pi_3$ as scores.\\ %and class $k=3$ as the positive class vs. class $\ell = 1$ as the negative class.\\
$\Rightarrow$ AUC(3|1) = 1 as all positives have a lower probability for $\hat \pi_3$ than all negatives.
\end{enumerate}
\end{column}
\end{columns}
% Source: https://docs.google.com/presentation/d/1Yap557n9SDyq9-fpA8u7wP46dkpzApJRx5bWpvTgIpM/edit?usp=sharing
%\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Multi-class AUC}

\begin{itemize}
\item For binary classes, we always have $\text{AUC}(1 | 0) = \text{AUC}(0 | 1)$.

\item For the multi-class case with $g > 2$, we usually have
$\text{AUC}(k ~|~ \ell) \neq \text{AUC}(\ell ~|~ k)$, $\forall k, \ell \in \gset, k \neq \ell$.

\item
%\textbf{Example:} For $g=3$, $\text{AUC}(3|1) \neq \text{AUC}(1|3)$ as $\hat \pi_3 = 0.25$ separates all negative from positive classes (right table) but $\hat \pi_1$ does not (left table).
\textbf{Example} with $g=3$ where $\text{AUC}(1|3) \neq \text{AUC}(3|1) $:
\begin{itemize}
\item $\text{AUC}(1|3) \neq 1$ (left table) as the score $\hat \pi_1$ does not perfectly separate the positive class $k=1$ from negative class $\ell=3$.
\item $\text{AUC}(3|1) = 1$ (right table) as the score $\hat \pi_3$ perfectly separates all negative from positive classes.
\end{itemize}

%\begin{center}
\centerline{\includegraphics[trim = 0 15 00 10, clip, width=0.75\textwidth]{figure_man/multiclass-auc.pdf}}
% Source: https://docs.google.com/presentation/d/1Yap557n9SDyq9-fpA8u7wP46dkpzApJRx5bWpvTgIpM/edit?usp=sharing
%\end{center}

\end{itemize}


\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Multi-class AUC}
\href{https://link.springer.com/article/10.1023/A:1010920819831}{Hand and Till (2001)} proposed to average the AUC of pairwise comparisons (\textbf{one-vs-one}) of a multiclass classifier:
  %\begin{itemize}

\begin{itemize}
    \item For all class pairs $k, \ell \in \gset, k \neq \ell$, compute $\text{AUC}(k ~|~ \ell)$.
    %\item Estimate $AUC(i,j)$ for each pair of class $i$ and $j$.
    %\item $AUC(i,j)$ is the probability of a randomly drawn member from class $i$ having a lower probability of belonging to class $j$ than a randomly drawn member of class $j$.
    \item For $g$ classes, we have $g (g-1)$ different $\text{AUC}(k ~|~ \ell)$ values that are averaged to compute a multi-class AUC measure:
    %$$\text{AUC}_{MC} = \tfrac{2}{g(g - 1)} \sum_{k < \ell} \text{AUC}(k, \ell) = \tfrac{1}{g(g - 1)} \sum_{k \neq \ell} \text{AUC}(k, \ell) \in [0, 1].$$
    % ${{g}\choose{2}} = \tfrac{g (g-1)}{2}$
    $$\text{AUC}_{MC} = \tfrac{1}{g(g - 1)} \sum_{k \neq \ell} \text{AUC}(k | \ell) \in [0, 1].$$
  %\end{itemize}
\end{itemize}

\textbf{Final comments}:

\begin{itemize}
\item Other definitions use \textbf{one-vs-rest} comparisons and need to average only $g$ AUC values. $\Rightarrow$ More efficient than one-vs-one.
\item The one-vs-rest approach introduces imbalanced classes even if the original classes were balanced.
\item Imbalanced classes can be considered by weighting individual AUC values with prior class distributions, see \href{https://doi.org/10.1016/j.patrec.2008.08.010}{Ferri et al. (2003)}.
\end{itemize}

\end{vbframe}

%
% \begin{vbframe}{Multi-class AUC}
% \begin{itemize}
%   \item Provost and Domingos (2003)\footnote{Provost, F., Domingos, P. (2003). Tree induction for probability-based ranking.} proposed to average the AUC of \\
%   \textbf{1-vs-rest} comparisons of a multiclass classifier.
%   \item The idea is to iteratively take each class as the reference class, i.e., set the current class to 0 and all other classes to 1.
%   \item Then average these single $AUC(i,j)$ values to compute the multiclass AUC according to \textbf{1-vs-rest}.
%   \item For $g$ classes, we have $g$ values of $AUC(i,j)$ that are then averaged to compute the multiclass AUC.
% \end{itemize}
%
% \textbf{Note}: Both versions of the multiclass AUC can be weighted with their respective prior class distributions to also consider imbalanced classes.
%
% % Source: https://link.springer.com/content/pdf/10.1023/A:1024099825458.pdf
% \end{vbframe}
%
%
% \begin{vbframe}{Multi-class AUC}
%
% \begin{itemize}
%   \item Note that the 1-vs-1 procedure is computationally heavier than 1-vs-rest.
%   \item The 1-vs-rest approach leads to a skewed comparison as we create an artificial class imbalance even if the original classes were balanced in the first
%   place.
%   \item Provost and Domingos (2003) tried to solve the imbalance problem by
%   incorporating the respective prior class distributions as weights.
% \end{itemize}
% \end{vbframe}

\endlecture
\end{document}
