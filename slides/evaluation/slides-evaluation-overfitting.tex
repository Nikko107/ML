\input{../../style/preamble}

\newcommand{\titlefigure}{figure/eval_ofit_1}
\newcommand{\learninggoals}{
\item Understand what overfitting is and why it is a problem
\item Understand how to avoid overfitting}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}
<<<<<<< HEAD
% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

%! includes: evaluation-intro
=======
>>>>>>> e9f57038e01c2150eb0d08cb1ed54f000058f8d4

\lecturechapter{Evaluation: Overfitting}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Overfitting}

\begin{minipage}[t]{0.5\textwidth}
  \raggedright
  Overfitting learner\\
  \lz
  \includegraphics[width=\textwidth]{figure/eval_ofit_1}
  Better training set performance (seen examples)
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}
  \raggedright
  Non-overfitting learner\\
  \lz
  \includegraphics[width=\textwidth]{figure/eval_ofit_2}
  Better test set performance (unseen examples)
\end{minipage}

\framebreak

\begin{itemize}
  \item \textbf{Overfitting} is a well-known problem with powerful 
  (non-linear) learning algorithms.
  \item It occurs when the learner starts modeling patterns in the data 
  that are not actually there, i.e., noise or artefacts in the training data. \\
  $\rightarrow$ Too many hypotheses and not enough data to tell them apart.
  \item Symptoms of overfitting are small training errors that come at the 
  expense of high test errors.
  \item Recall that our primary focus is on the learner's ability to generalize 
  well to new observations, not fit the training data perfectly.
  \item In larger data sets overfitting is less of an issue as more data allow 
  for more poor hypotheses to be eliminated, but if the hypothesis space is not 
  constrained, there may never be enough data.
  \item Many learners come with parameters that enable to constrain 
  (\textbf{regularize}) such complexity.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Overfitting and Noise}
% 
% \begin{itemize}
%   \item Overfitting is seriously exacerbated by \textit{noise} (errors in the 
%   training data)
%   \item An unconstrained learner will start to model that noise
%   \item It can also arise when relevant features are missing in the data
%   \item In general it's better to make some mistakes on training data ("ignore 
%   some observations") than trying to get all correct
% \end{itemize}
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Avoiding Overfitting}

Remedies against overfitting include:
\begin{itemize}
  \item Use less complex models. 
  \item Get more, or better, data.
  \item Apply regularization.
  \item Some learners are also eligible for "early stopping", where the 
  learning process is terminated before perfectly fitting (i.e., overfitting) 
  the training data.
\end{itemize}

\lz

In the end, we will always need to balance a trade-off between model fit 
and generalization ability.

\end{vbframe}

% \begin{vbframe}{Triple Trade-Off}

% In all learning algorithms that are trained from data, there is a trade-off between three factors:
% \begin{itemize}
  % \item The complexity of the hypothesis we fit to the training data
  % \item The amount of training data (in terms of both instances and informative features)
  % \item The generalization error on new examples
% \end{itemize}
% If the capacity of the learning algorithm is large enough to a) approximate the data generating process and b) exploit the information contained in the data, the generalization error will decrease as the amount of training data increases.

% For a fixed size of training data, the generalization error decreases first and then starts to increase (overfitting) as the complexity of the hypothesis space $H$ increases.
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Trade-Off Between Complexity and Generalization Error}

\textbf{Apparent error} (at training time) and \textbf{actual error} 
(at prediction time) evolve in opposite directions with increasing 
complexity:

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width = 0.9\textwidth]{figure/eval_ofit_3} 
}
\end{knitrout}

\vfill

$\rightarrow$ We would like to find the optimal level of complexity: \\ 
Hit the sweet spot  for the given amount of data where generalization error 
becomes minimal.

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
