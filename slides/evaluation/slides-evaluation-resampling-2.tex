\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/crossvalidation.png}
\newcommand{\learninggoals}{
\item Understand the advantages of subsampling over single hold-out split
\item Understand the challenges when comparing learners based on CV results
\item Understand what pessimistic bias means
\item Be able to compare different resamping strategies}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-hpo.tex}
\input{../../latex-math/ml-eval.tex}
%! includes: evaluation-test

\lecturechapter{Evaluation: Resampling 2}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------


\begin{vbframe}{Bias-Variance Analysis for Subsampling}

  \begin{itemize}
    \item Reconsider our hold-out experiment on the \texttt{spirals} data from 
    the train-test unit (maybe re-read it again)
    \item Again, we use split rates $s \in \{0.05, 0.1, ..., 0.95\}$ for 
    training with $|\Dtrain| = s \cdot 500$.
    \item But: now we compare 50 subsampling experiments with $50 \cdot 50$ 
    hold-out experiments per split.
    \item Every subsampling experiment is the result of averaging 50 hold-out 
    experiments, so each performance estimate is much more reliable (but also 
    more expensive) than one computed by a single hold-out experiment.
  \end{itemize}

\framebreak

\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.9\textwidth]{figure/eval-resampling-example-1}
\end{center}



\begin{itemize}
  \item Both experiments are compared to the "real" MMCE (black line).
  \item Subsampling has the same pessimistic bias for small split rates but 
  much less variance overall.
  \item This allows to use much smaller test sets with good results.
\end{itemize}

\framebreak

\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.7\textwidth]{figure/eval-resampling-example-2}
\end{center}

\begin{itemize}
  \item The MSE is strictly better for subsampling compared to hold-out.
  \item The optimal split rate now is a higher $s \approx 0.8$.
  \item We see the variance picking up at the end because training sets 
  increasingly overlap.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Comparing learners based on CV results}
\begin{footnotesize}
\begin{itemize}
\item Since the $k$ training sets and their respective independent test sets stem
from the same distribution we get $k$ unbiased estimators of $\GEfull$.
\item However, in order to compare two different learners we also need to assess the 
uncertainty of our overall  estimator.
\item This becomes challenging sice the $k$ splits are not independent of each other: 
\item $\mathbb{V}[\GEh]$ of CV is a linear combination of 
\begin{itemize}
\begin{footnotesize}
\item the average variance we get from estimating from finite training sets,
\item the covariance arising from the dependence of test errors the learners made
since the were trained on overlapping training sets,
\item the covariance due to the dependence of training sets and that test sets appear
also in training sets. [Bengio, 2004]
\end{footnotesize}
\end{itemize}
\item[$\Rightarrow$] Taking the empirical variance of the $k$ $\GEh$s yields a biased 
estimator of $\mathbb{V}[\GEh]$.
\item[$\Rightarrow$] These dependences should be taken into account 
when testing if a learner is significantly better than an other learner 

\end{itemize}
\end{footnotesize}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Resampling discussion}

\begin{itemize}

  \item In ML we fit, at the end, a model on all our given data.\\

  \item \textbf{Problem:} We need to know how well this model will perform in 
  the future, but no data is left to reliably quantify this.\\
  $\Rightarrow$ Approximate using hold-out / CV / bootstrap / subsampling \\
  estimate\\ 

  \item \textbf{But:} pessimistic bias because we don't use all data points.\\

  \item The final model is (usually) computed on all data points.

  \item Strictly speaking, resampling only produces one number, the performance 
  estimator.
  It does NOT produce models, parameters, etc. These are intermediate results 
  and discarded.
  
  \item The model and parameters are only obtained when we finally fit the 
  learner on the complete data.

\end{itemize}

\framebreak

\begin{itemize}
  \item 5-CV or 10-CV have become standard.
  \item Do not use hold-out, CV with few iterations, or subsampling with a low 
  subsampling rate for small samples, since this can cause the estimator to be 
  extremely biased, with large variance.
  \item For small-data situations with less than 500 or 200 observations, use 
  LOO or, probably better, repeated CV.
  \item For some models, computationally fast calculations or approximations 
  for LOO exist.
  \item A data set $\D$ with $|\D| = 100.000$ can have small-sample properties 
  if one class has few observations 
  \item Research indicates that subsampling has better properties than
    bootstrapping. The repeated observations can cause problems in training,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}

\framebreak

\fboxsep=0pt
\noindent%
\begin{minipage}[t]{0.42\linewidth}
\vspace{0pt}
\includegraphics{figure_man/resampling_dec_tree}
\end{minipage}%
\hfill%
%
\begin{minipage}[t]{0.58\linewidth}
\vspace{0pt}
\scriptsize
\begin{itemize}
  \item 5-CV or 10-CV have become standard.
  \item Do not use hold-out, CV with few iterations, or subsampling with a low 
  subsampling rate for small samples, since this can cause the estimator to be 
  extremely biased, with large variance.
  \item For small-data situations with less than 500 or 200 observations, use 
  LOO or, probably better, repeated CV.
  \item For some models, computationally fast calculations or approximations 
  for LOO exist.
  \item A data set $\D$ with $|\D| = 100.000$ can have small-sample properties 
  if one class has few observations 
  \item Research indicates that subsampling has better properties than
    bootstrapping. The repeated observations can cause problems in training,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}
\end{minipage}



\end{vbframe}

\endlecture
\end{document}
