\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}

\newcommand{\titlefigure}{figure_man/crossvalidation}
\newcommand{\learninggoals}{
\item Understand why resampling is 
    produces a better estimator than hold-out
\item In-depth bias-var analysis of resampling
\item Understand that CV does not produce independent samples
\item Short guideline for practical use 
}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: Resampling 2}
\lecture{Introduction to Machine Learning}
\sloppy


% ------------------------------------------------------------------------------
% GE(I, n, L) ::: GE_hat(I, ntrain, L)


% GE_hat = 1/m sum_{i=1 bis m} L(y_test_i, I(Dtrain)(x_test_i))

% E[ GE_hat] = 1/m sum_{i=1 bis m} E [L(y_test_i, I(Dtrain)(x_test_i)) ] = GE(I, ntrain, L)

% Bias: - GE(I, ntrain, L)


\begin{frame}{Bias-Variance Analysis for Subsampling}
\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.9\textwidth]{figure/eval-resampling-example-1}
\end{center}

\vfill

\only<1>{
  \begin{itemize}
    \item Reconsider bias-var experiment for holdout (maybe re-read)
    \item Split rates $s \in \{0.05, 0.1, ..., 0.95\}$ with $|\Dtrain| = s \cdot 500$.
    \item Holdout vs. subsampling with 50 iters
    \item 50 replications
    % \item Every subsampling experiment is the result of averaging 50 hold-outs,
        % so
    % experiments, so each performance estimate is much more reliable (but also 
    % more expensive) than one computed by a single hold-out experiment.
  \end{itemize}
}

\only<2>{
\begin{itemize}
  \item Both estimators are compared "real" MCE (black line)
  \item SS same pessimistic bias as holdout, but much less var
  \item Allows to use smaller testsets
\end{itemize}
}

% \framebreak

\end{frame}


\begin{vbframe}{Bias-Variance Analysis for Subsampling}

\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.7\textwidth]{figure/eval-resampling-example-2}
\end{center}

\begin{itemize}
  \item MSE of $\GEh$ strictly better for SS
  \item The optimal split rate now is a higher $s \approx 0.8$.
  \item Beyond that: MSE goes up as var doesn't go down as much as we want 
      due to increasing overlap in trainsets
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{No independence of CV results}
% \begin{footnotesize}
\begin{itemize}
    \item Similar analysis as before holds for CV
    \item Might be tempted to report distribution or SD 
        of individual CV split perf values, e.g.
        to test if perf of 2 learners is significantly different
    % \item Since the $k$ training sets and their respective independent test sets stem
% from the same distribution we get $k$ unbiased estimators of $\GEfull$.
% \item However, in order to compare two different learners we also need to assess the 
% uncertainty of our overall  estimator.
\item But $k$ CV splits are not independent
\framebreak
\item $\mathbb{V}[\GEh]$ of CV is a difficult combination of 
\begin{itemize}
\begin{footnotesize}
\item average variance as we estim on finite trainsets
\item covar from test errors,
    as learner trained on overlapping trainsets
\item covar due to the dep of trainsets and that testsets appear in trainsets
\end{footnotesize}
\end{itemize}
\item Empirical var of $k$ individual $\GEh$s yields biased 
estimator of $\mathbb{V}[\GEh]$ 
\item Worse: there is no unbiased estimator of $\mathbb{V}[\GEh]$ [Bengio, 2004]
\item Must be taken into account when comparing learners by significance tests
\item Somewhat difficult topic, we leave it with the warning here

\end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Resampling discussion}

% \begin{itemize}

%   \item In ML we fit, at the end, a model on all our given data.\\

%   \item \textbf{Problem:} We need to know how well this model will perform in 
%   the future, but no data is left to reliably quantify this.\\
%   $\Rightarrow$ Approximate using hold-out / CV / bootstrap / subsampling \\
%   estimate\\ 

%   \item \textbf{But:} pessimistic bias because we don't use all data points.\\

%   \item The final model is (usually) computed on all data points.

%   \item Strictly speaking, resampling only produces one number, the performance 
%   estimator.
%   It does NOT produce models, parameters, etc. These are intermediate results 
%   and discarded.
  
%   \item The model and parameters are only obtained when we finally fit the 
%   learner on the complete data.

% \end{itemize}

% \framebreak

% \begin{itemize}
%   \item 5-CV or 10-CV have become standard.
%   \item Do not use hold-out, CV with few iterations, or subsampling with a low 
%   subsampling rate for small samples, since this can cause the estimator to be 
%   extremely biased, with large variance.
%   \item For small-data situations with less than 500 or 200 observations, use 
%   LOO or, probably better, repeated CV.
%   \item For some models, computationally fast calculations or approximations 
%   for LOO exist.
%   \item A data set $\D$ with $|\D| = 100.000$ can have small-sample properties 
%   if one class has few observations 
%   \item Research indicates that subsampling has better properties than
%     bootstrapping. The repeated observations can cause problems in training,
%     especially in nested setups where the \enquote{training} set is split up again.
% \end{itemize}
% 
% \framebreak
\begin{vbframe}{Short Guideline}

\fboxsep=0pt
\noindent%
\begin{minipage}[t]{0.42\linewidth}
\vspace{0pt}
\includegraphics{figure_man/resampling_dec_tree}
\end{minipage}%
\hfill%
%
\begin{minipage}[t]{0.58\linewidth}
\vspace{0pt}
\scriptsize
\begin{itemize}
  \item 5-CV or 10-CV have become standard.
  \item Do not use hold-out, CV with few iterations, or subsampling with a low 
  subsampling rate for small samples, since this can cause the estimator to be 
  extremely biased, with large variance.
  \item For small-data situations with less than 500 or 200 observations, use 
  LOO or, probably better, repeated CV.
  \item For some models, computationally fast calculations or approximations 
  for LOO exist.
  \item A data set $\D$ with $|\D| = 100.000$ can have small-sample properties 
  if one class has few observations 
  \item Research indicates that subsampling has better properties than
    bootstrapping. The repeated observations can cause problems in training,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}
\end{minipage}



\end{vbframe}

\endlecture
\end{document}
