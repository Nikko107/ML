\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}

\newcommand{\titlefigure}{figure_man/eval_fig_title_intro}
\newcommand{\learninggoals}{
\item Understand the goal of performance estimation
\item Understand the difference between outer and inner loss
\item Know the definition of generalization error}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: Introduction and Remarks}
\lecture{Introduction to Machine Learning}
\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Performance estimation}

\begin{itemize}
  \item After training our model, we are naturally interested in its
  \textbf{performance}.
  \item Recall that supervised learning is about finding the optimal model 
  for our data at hand, given a set of hyperparameters: 
  $$\inducer: \preimageInducerShort \rightarrow \Hspace, \quad (\D, \lamv)
  \mapsto \fhDlam.$$
  \item We obtain $\fhDlam$ by means of empirical risk minimization, 
  based on what we will now call \textbf{inner loss}.
  \item However, the inner loss does not necessarily tell us something about the performance 
  of our learner -- after all, we chose our model precisely so it would be 
  loss-minimal on the data we trained it on.
\end{itemize}

\lz
$\rightarrow$ We cannot hope for $\fh_{\D, \lamv}$ to perform equally well 
on unseen data. \\
$\rightarrow$ Evaluation based on the inner loss would be 
\textbf{optimistically biased}.

\framebreak

\begin{itemize}
  \item We wish to compute the true expected loss of our learner, referred to as
  \textbf{generalization error} or \textbf{outer loss}.
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/drawings/d/13AH298rMnDL5p0SrBd6VCukC9vg1qyRXGqgMcvuPRc0/edit?usp=sharing
\includegraphics[trim = 0 0 0 30, clip, width=0.7\textwidth]
{figure_man/evaluation-intro-ge.pdf}
\end{center}

\begin{itemize}
  \item In order to estimate performance on previously unseen observations, we 
  need independent \textbf{test data}. 
  \item As such a test set is not always available, we split the data at hand into non-overlapping sets $\Dtrain$ and $\Dtest$, 
  with respective sizes $\ntrain + \ntest = n$.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Set-based performance metrics}

\begin{itemize}
  \item For the outer loss, we introduce a \textbf{set-based metric} $\rho$.
  \item This allows us to use outer losses that are defined on the entire test 
  set rather than point-wise (e.g., the AUC), and potentially differ from the 
  inner loss.
  \item For arbitrary data sets of size $m$ and a prediction matrix 
  $$\F = \begin{bmatrix} \fh(\xi[1])^\top & \dots & \fh(\xi[m])^\top \end{bmatrix}^\top \in \R^{m \times g}$$ returned by our model $\fh$, $\rho$ is defined as:
  $$\rho: \preimageRho  \rightarrow \R, \quad (\yv, \F) \mapsto \rho(\yv, \F).$$
  \item We can easily translate this to our usual notion of point-wise loss $L$:
  $$\rho_L(\yv, \F) = \frac{1}{m}\sumim L(\yi, \Fi)  \quad \left(= \frac{1}{m}\sumim L(\yi, \fh(\xi)\right).$$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Generalization error}

\begin{itemize}
  % \item To ease notation, we will represent our train and test sets by index 
  % vectors $J_\mathrm{train} \in \nset^{m_\mathrm{train}}$ and $J_\mathrm{test} 
  % \in \nset^{m_\mathrm{test}}$.
  % \item With this, we can denote the corresponding vector of labels as
  % $\yv_J = \left(y^{(J^{(1)})},\dots, y^{(J^{(m)})} \right) \in \Yspace^m$ and 
  % the matrix of prediction scores as
  % $\bm{F}_{J,f} = \left( f(\xv^{(J^{(1)})}), \dots, f(\xv^{(J^{(m)})}) \right)
  % \in \R^{m\times g}$.
  \item This allows us to define the \textbf{generalization error} as:
  $$\GEfull := 
  \lim_{\ntest \rightarrow \infty} \E \left[ \rho \left(
  \yv, \F_{\Dtest, \inducer(\D_{\mathrm{train}}, \lamv)} 
  \right)\right]$$
  \item The expectation is taken w.r.t. the unknown distribution $\Pxy$ from 
  which both $\Dtrain$ and $\Dtest$ are sampled independently.
  \item We must therefore estimate the generalization error, based on our 
  train-test split:
  $$\GEh_{\Dtrain, \Dtest}(\inducer,
  \lamv, \ntrain, \rho) =
  \rho \left( \yv_{\Dtest}, \F_{\Dtest, 
  \inducer(\D_{\mathrm{train}}, \lamv)} \right).$$
  \item In practice, we will not rely on a single split, but use 
  \textbf{resampling} to repeatedly carve out test observations from our data.
\end{itemize}

\framebreak

\begin{itemize}
  \small
  \item In order to get a better feeling for the quantities we are trying to 
  estimate, let's look at the (pointwise) losses we can expect when applying 
  a model on unseen test data.
  \item We fit a linear model to the \texttt{boston housing} regression task 
  with $\Dtrain$ of fixed size $\ntrain = 354$ (70\% of total observations).
  \item From the remaining unseen data, we draw one observation at a time, 
  feed it to the trained model, and compute the pointwise $L2$ loss.
  \item Then, we can plot the density of the stacked loss values:
\end{itemize}

\vfill

\begin{minipage}[c]{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/fig_loss_distribution}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \begin{itemize}
    \small
    \item The result is a unimodal 
    distribution with long tails.
    \item Mean and one standard deviation to either side are highlighted in 
    grey.
  \end{itemize}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Inner vs outer loss}

\begin{itemize}
  \item Supervised learning thus implies the following dichotomy:
  \begin{itemize}
    \item \textbf{Learning}: minimize inner loss
    \item \textbf{Evaluation}: estimate outer loss
  \end{itemize}
  \item Beyond evaluating a single learner, the outer loss lends itself to
  comparing different types of learners, or learners with varying hyperparameter
  configurations $\lamv$.
  \item Ideally, we have \textbf{inner loss = outer loss}. In this case, it holds by the law of the large numbers for the empirical risk $\riske$ that
  $$\E_{\D\sim(\Pxy)^{\ntrain}}\left[\frac{1}{n}\sumin L\left(\yi, \fhDlam(\xi)\right)  \right] \xrightarrow{n \to \infty} \GE(\inducer, \lamv, \ntrain, \rhoL) .$$
  \item This is not always possible -- some special (set-based) metrics for 
  evaluation, such as the AUC, are not applicable as inner loss.
  \item On the other hand, we sometimes wish to use losses that are 
  hard to optimize or do not even specify one directly.
  % , as in:
  % \begin{itemize}
  %   \item Logistic regression: minimization of binomial loss
  %   \item k-NN: no explicit loss minimization
  % \end{itemize}
\end{itemize}

\framebreak

\textbf{Example:} Logistic regression

\begin{itemize}
  \item An intuitive choice would be the share of incorrect predictions.
  \item This leads to the \textbf{mislassification error rate (MCE)}, computing
  the mean over pointwise \textbf{0-1 loss}.
\end{itemize}

\lz

\begin{minipage}{0.6\textwidth}
  \begin{itemize}
    \item 0-1 loss simply assigns a loss of 1 for incorrect predictions and 0
    otherwise: $$\Lhxy = \I_{\{y \neq \hx\}}$$
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{figure/zero-one-loss}
\end{minipage}

\lz

\begin{itemize}
  \item Problem: 0-1 loss is not differentiable (not continuous even). \\
  $\rightarrow$ This is why we use \textbf{binomial} loss as inner loss 
  instead.
  \item For evaluation, differentiability is not required, so evaluation on 
  \textbf{MCE} is feasible. 
\end{itemize}

\normalsize

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
