\documentclass{beamer}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{I2ML :\,: BASICS} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ \invisible{x} % Package description in header
	% The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}
	

	
\begin{document}
\begin{frame}[fragile]{}
\vspace{-8ex}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First Column begin
%-------------------------------------------------------------------------------
% Data
%-------------------------------------------------------------------------------
					\begin{myblock}{Data}
					   $\Xspace \subset \R^p$ : $p$-dimensional \textbf{feature / input space}\\ 
            \hspace*{1ex}Usually we assume $\Xspace \equiv \R^p$, but sometimes, dimensions may be bounded (e.g., for categorical or non-negative features.)    \\
            
            $\Yspace \subset \R^g$ : \textbf{target space} \\ 
            \hspace*{1ex}e.g.: $\Yspace = \R$, $\Yspace = \lbrace 0, 1 \rbrace$, $\Yspace = \lbrace -1, 1 \rbrace$, $\Yspace = \gset$ with $g$ classes\\
            
            $\xv = \xvec \in \Xspace$ : \textbf{feature vector} \\ 
             
            $y \in \Yspace$ : \textbf{target / label / output} \\
             
            $\allDatasets \textcolor{red}{=} \defAllDatasets$ : \textbf{set of all finite data sets} \\
             
            $\allDatasetsn \textcolor{red}{=} \defAllDatasetsn \textcolor{red}{\subset \allDatasets}$ : \textbf{set of all finite data sets of size $n$} \\
             
            $\D = \Dset \in \textcolor{red}{\allDatasetsn} $ : \textbf{data set} with $n$ observations \\
             
            $\Dtrain$, $\Dtest \subset \D$ : \textbf{data for training and testing} (often: $\D = \Dtrain \dot{\cup} ~ \Dtest$)\\
             
            $\xyi \in \Xspace\times \Yspace$ : $i$ -th \textbf{observation} or \textbf{instance} \\
             
            $\P_{xy}$ : \textbf{joint probability distribution on} $\Xspace \times \Yspace$ \\
              
            \textcolor{red}{$\pdfxy: \Xspace \times \Yspace \rightarrow [0, 1]$ (or $\pdfxyt$) : \textbf{joint probability density function (pdf)}, often parametrized by $\thetab \in \Theta$} \\
					\end{myblock}
%-------------------------------------------------------------------------------
% Model and Learner
%-------------------------------------------------------------------------------
					\begin{myblock}{Model and Learner}
            \textbf{Model / \textbf{hypothesis}: }$f : \Xspace \rightarrow \R^g, \quad \xv \mapsto \fx$ (also: $f_{\thetab} : \Xspace \rightarrow \R^g, \xv ~|~ \thetab \mapsto \fxt$) is a function that maps feature vectors to predictions, often parametrized by $\thetab \in \Theta$. \\
            
            % $\fx$ or $\fxt \in \R$ or $\R^g$ : prediction function (\textbf{model}) \\ %learned from data
            % \hspace*{1ex}We might suppress $\thetab$ in notation. \\
            
            % $\hx$ or $\hxt \in \Yspace$ : discrete prediction for classification. \\
            
            $\Theta \subset \R^d$ : \textbf{parameter space} \\
              
            $\thetab = (\theta_1, \theta_2, ..., \theta_d) \in \Theta$: model \textbf{parameters}\\
              \hspace*{1ex}Some models may traditionally use different symbols. \\
      
            $\Hspace = \{f : \Xspace \rightarrow \R^g ~|~ f \text{ belongs to a certain functional family}\}$ : \textbf{hypothesis space} \\
              \hspace*{1ex}Set of functions defining a specific model class to which we restrict our learning task. 					
					\end{myblock}\vfill
% End First Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Second Column
					\begin{myblock}{} \vspace{-4ex}
						 \textbf{Learner }$\inducer: \preimageInducerShort \rightarrow \Hspace$  takes a \textbf{training set}  $\Dtrain \in \allDatasets$  and produces a \textbf{model} $f : \Xspace \rightarrow \R^g$, its \textbf{hyperparameters} are set to $\bm{\lambda} \in \bm{\Lambda}$.\\
            \hspace*{1ex}For a parametrized model the definition can be adapted $\inducer: \preimageInducerShort \rightarrow \Theta$ \\
            
              $\bm{\Lambda} \textcolor{red}{\subset \R^{foo}}$ : \textbf{hyperparameter space} \\
            
              $\bm{\lambda} = (\lambda_1, \lambda_2, ..., \textcolor{red}{\lambda_{foo}}) \in \bm{\Lambda}$ : \textbf{hyperparameter vector} \\
              
              $\pi_k = \P(y = k)$:\textbf{ prior probability} for class $k$ \\
                \hspace*{1ex}In case of binary labels we might abbreviate $\pi = \P(y = 1)$. \\
             
              $\pikx = \postk$: \textbf{posterior probability} for class $k$, given $\xv$ \\
                \hspace*{1ex}In case of binary labels we might abbreviate $\pix = \post$. \\
              
              $\LLt$ and $\llt = \log(\LLt)$ : likelihood and log-likelihood for parameter $\thetab$ \\
                \hspace*{1ex}These are based on a statistical model.\\
               %????????????????????????????????
               
              \textcolor{red}{$\eps = y - \fx$ or} $\epsi = \yi - \fxi$ : $i$-th \textbf{residual} in regression.\\

              \textcolor{red}{$\yf$ or} $\yfi$ : \textbf{margin} for $i$-th observation in binary classification \\
                \hspace*{1ex} (with $\Yspace = \{-1, 1\}$). \\
              %????????????????????????????????
              
              $\yh$, $\fh$, \textcolor{red}{$\hh$,} $\pikxh$, $\pixh$ and $\thetah$ \\
                \hspace*{1ex}The hat symbol denotes \textbf{learned} functions and parameters.
					\end{myblock}
%-------------------------------------------------------------------------------
% Loss and Risk 
%-------------------------------------------------------------------------------
          \begin{myblock}{Loss and Risk}
				    $L: \Yspace \times \R^g \to \R.$ : \textbf{loss function : } $\Lxy$ quantifies the "quality" of the prediction $\fx$ of a \textcolor{red}{single observation} $\xv$.  \\
            
            $\riske:  \Hspace \to \R $ :  The ability of a model $f$ to reproduce the association between $x$ and $y$ that is present in the data $\D$ can be measured by the summed loss, the \textbf{empirical risk }:
            
            $$\riskef = \sumin \Lxyi$$ 
          Since $f$ is usually defined by \textbf{parameters} $\thetab$, this becomes:
            $$\risk_{emp} : \textcolor{red}{\R^d} \to \R$$
            $$\risket  =  \sumin \Lxyit $$
            \hspace*{1ex}Learning then amounts to \textbf{empirical risk minimization} -- figuring out \hspace*{1ex}which model $f$ has the smallest summed loss:$$\fh = \argmin_{\thetab \in \Theta} \risket).$$
				  \end{myblock}
% End Second Column					
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Third Column#
%-------------------------------------------------------------------------------
% HRO - Components of Learning 
%-------------------------------------------------------------------------------          
          \begin{myblock}{Components of Learning}
  
            \textbf{Learning = Hypothesis space + Risk  + Optimization}. \\
        
            \textbf{Hypothesis space : } defines (and restricts!) what kind of model $f$
        can be learned from the data.
            
            \hspace*{1ex} Examples: linear functions, decision trees etc.
          
          \vspace*{0.5ex}
          
          \textbf{Risk: } quantifies how well a model performs on a given
        data set. This allows us to rank candidate models in order to choose the best one.
        
          \hspace*{1ex} Examples: squared error, likelihood etc.
          
          \vspace*{0.5ex}
          
          \textbf{Optimization: } defines how to search for the best model, i.e., the model with the smallest {risk}, in the hypothesis space.
          
          \hspace*{1ex} Examples: gradient descent, quadratic programming etc.
        
            
          \end{myblock}
%-------------------------------------------------------------------------------
% Regression Losses 
%------------------------------------------------------------------------------- 
          \begin{myblock}{Regression Losses}
			        \textbf{Basic idea (L2 loss / squared error):} 
						\begin{itemize}    
						  \setlength{\itemindent}{+.3in}
              \item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
              \item Convex and differentiable
              \item Tries to reduce large residuals (loss scaling quadratically)
              \item Optimal constant model: $\fxh = \text{mean of } y | \xv$
            \end{itemize}

            \vspace*{1ex}
            %        \includegraphics[width=1\columnwidth]{img/reg_loss.PNG}

            
              \textbf{Basic idea (L1 loss / absolute error):} 
            \begin{itemize}     \setlength{\itemindent}{+.3in}
              \item $\Lxy = |y-\fx|$
              \item Convex and more robust
              \item Non-differentiable for $y = \fx$, optimization becomes harder
              \item Optimal constant model: $\fxh = \text{median of } y | \xv$      
            \end{itemize}
              %\includegraphics[width=1.03\columnwidth]{img/reg_loss_2.PNG} 
          \end{myblock}
%-------------------------------------------------------------------------------
% Classification 
%------------------------------------------------------------------------------- 

      		\begin{myblock}{Classification}
% 				    We want to assign new observations to known categories according to criteria learned from a training set.  
%             \vspace*{1ex}
%             
            Assume we are given a \textbf{classification problem:}
            \begin{eqnarray*} & \xv \in \Xspace \quad & \text{feature vector}\\ & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\ &\D = \Dset & \text{observations of $\xv$ and $y$} \end{eqnarray*}
            \vspace*{1ex}
            Classification usually means to construct $g$ discriminant functions:
              
              \hspace*{1ex}$f_1(\xv), \ldots, \fgx$, so that we choose our class as $h(\xv) = \argmax_{k \in \gset} \fkx$ 
              
              \vspace*{1ex}
            
            
            \textbf{Linear Classifier:}
            
            \hspace*{1ex}If the functions $\fkx$ can be specified as linear functions, we will call \hspace*{1ex}the classifier a \emph{linear classifier}.\\
            
            % \hspace*{1ex}\textbf{Note: }All linear classifiers can represent non-linear decision boundaries \hspace*{1ex}in our original input space if we include derived features. For example: \hspace*{1ex}higher order interactions, polynomials or other transformations of x in \hspace*{1ex}the model.
            
            \textbf{Binary classification: }If only 2 classes exist, we can use a single discriminant function $\fx = f_{1}(\xv) - f_{2}(\xv)$.  
    
				  \end{myblock}
% End Third Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			  }
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
\end{columns}

\end{frame}
\end{document}
