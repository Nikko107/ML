\begin{enumerate}
    \item[a)] See \texttt{regularization\_l\_1.R}
    \item[b)]
    \begin{itemize}
        \item With no noise features the linear model is best due to the real linear relationship.
        \item The restricted hypothesis space of the Lasso and Ridge Regression yields a slightly worse estimator since we are not as close to the real model due to the regularization.
        \item After adding noise features, the linear model has difficulties capturing the true relationship which explains the bias in the generalization error.
    \end{itemize}
    \item[c)]
    We want to find the model with the smallest mse:

    $$
    \text{MSE}(\hat{f}, \mathcal{D}) = \frac{1}{n}\sum\limits_{i = 1}^n (y^{(i)} - \hat{f}(x^{(i)}))
    $$

    The mse estimates to the variance of the data generating process.
    If we look at the error term $\varepsilon \sim N(0,1)$ of $Y$ we recognize that the variance used for the simulation is exactly 1.
    This is the Bayes error of the simulation. As seen in the visualization we are not able to get much better than $\text{MSE}(\hat{f}, \mathcal{D}) \approx 1$.
\end{enumerate}
