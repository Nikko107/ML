{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e21a04",
   "metadata": {},
   "source": [
    "## Solution 1: Splitting criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1e78d",
   "metadata": {},
   "source": [
    "### a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdef5d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#| label: 1-a-1\n",
    "x <- c(1, 2, 7, 10, 20)\n",
    "y <- c(1, 1, 0.5, 10, 11)\n",
    "\n",
    "compute_mse <- function (y) mean((y - mean(y))**2)\n",
    "\n",
    "compute_total_mse <- function (yleft, yright) {\n",
    "  num_left <- length(yleft)\n",
    "  num_right <- length(yright)\n",
    "  w_mse_left <- num_left / (num_left + num_right) * compute_mse(yleft)\n",
    "  w_mse_right <- num_right / (num_left + num_right) * compute_mse(yright)\n",
    "  w_mse_left + w_mse_right\n",
    "}\n",
    "\n",
    "split <- function(x, y) {\n",
    "  # try out all unique points as potential split points and ...\n",
    "  unique_sorted_x <- sort(unique(x))\n",
    "  split_points <- head(unique_sorted_x, length(unique_sorted_x) - 1) + \n",
    "    0.5 * diff(unique_sorted_x)\n",
    "\n",
    "  node_mses <- lapply(\n",
    "    split_points, \n",
    "    function(i) { \n",
    "      y_left <- y[x <= i]\n",
    "      y_right <- y[x > i]\n",
    "      # ... compute SS in both groups\n",
    "      mse_split <- compute_total_mse(y_left, y_right)\n",
    "      print(sprintf(\"split at %.6f: empirical risk = %.2f\", i, mse_split))\n",
    "      mse_split\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # select the split point yielding the maximum impurity reduction\n",
    "  split_points[which.min(node_mses)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094a00c3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"split at 1.500000: empirical risk = 19.14\"\n",
      "[1] \"split at 4.500000: empirical risk = 13.43\"\n",
      "[1] \"split at 8.500000: empirical risk = 0.13\"\n",
      "[1] \"split at 15.000000: empirical risk = 12.64\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "8.5"
      ],
      "text/latex": [
       "8.5"
      ],
      "text/markdown": [
       "8.5"
      ],
      "text/plain": [
       "[1] 8.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: 1-a-2\n",
    "split(x, y) # 3rd obs is best split point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660834ff",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"split at 0.346574: empirical risk = 19.14\"\n",
      "[1] \"split at 1.319529: empirical risk = 13.43\"\n",
      "[1] \"split at 2.124248: empirical risk = 0.13\"\n",
      "[1] \"split at 2.649159: empirical risk = 12.64\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "2.12424762102468"
      ],
      "text/latex": [
       "2.12424762102468"
      ],
      "text/markdown": [
       "2.12424762102468"
      ],
      "text/plain": [
       "[1] 2.124248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: 1-a-3\n",
    "split(log(x), y) # again, 3rd obs wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250af688",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb72b59",
   "metadata": {},
   "source": [
    "- For regression trees, we usually identify *impurity* with *variance*. Here is why:\n",
    "  - It is reasonable to define impurity via the deviation between actual target values and the predicted constant -- either using absolute or square distances to enforce symmetry of positive and negative residuals.\n",
    "  - Recall the constant \\(L2\\) risk minimizer for a node \\(\\mathcal{N}\\):\n",
    "    $$\n",
    "    \\bar y = \\arg\\min_c \\frac{1}{|\\mathcal{N}|}  \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2,\n",
    "    $$\n",
    "    because\n",
    "\n",
    "    \\begin{align*}\n",
    "    \\min_c \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2 &\\rightarrow \\frac{\\partial}{\\partial c} \\left( \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} (y_i^2 - 2y_i c + c^2) \\right) = 0 \\\\\n",
    "    &\\rightarrow \\frac{1}{|\\mathcal{N}|} \\left( \\sum_{i = 1}^{|\\mathcal{N}|} (-2y_i + 2c) \\right) = 0 \\\\\n",
    "    &\\rightarrow \\sum_{i = 1}^{|\\mathcal{N}|} (-2y_i + 2c) = 0 \\\\\n",
    "    &\\rightarrow -2 \\sum_{i = 1}^{|\\mathcal{N}|} y_i + 2|\\mathcal{N}|c = 0 \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    This implies $\\hat{c} = \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} y_i = \\bar{y}$.\n",
    "\n",
    "  - Consequently, we have \n",
    "    $$\n",
    "    \\bar y = \\arg\\min_c \\frac{1}{|\\mathcal{N}|}  \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2,\n",
    "    $$\n",
    "    where the right hand side is the (biased) sample variance for sample mean \\(c\\).\n",
    "  - Therefore, predicting the sample mean both minimizes risk under \\(L2\\) loss and variance impurity.\n",
    "  - Since constant mean prediction is equivalent to an intercept LM (minimizing the sum of squared residuals!), regression trees with \\(L2\\) loss perform piecewise constant linear regression.\n",
    "  - The same correspondence holds between impurity via absolute distances and \\(L1\\) regression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-i2ml",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
