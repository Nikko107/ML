\begin{enumerate}
\item[1)] The conclusion is incorrect. Since the tree structure is built recursively, the algorithm does not necessarily identify the optimal tree with lowest empirical risk on the training data.
This lies in the nature of greedy optimization procedures.
Empirical risk minimization (ERM) is only performed to identify \textit{the next} splitting rule, and not entire sets of subsequent splitting rules.

\item[2)] CART automatically selects features for splitting nodes if they lead to an expected reduction in empirical risk.
Irrelevant features are therefore more likely to be picked less often for split rules in model construction.
Of course, the subject of assessing feature importance is left for the chapter on random forests. However, one could gain a rough understanding of a feature's relevance by looking at how often it was picked for splitting a node. However, this kind of "split rule selection frequency" does not necessarily relate to a feature variable's contribution to ERM. 

\item[3)] CART performs automatic feature selection by remembering surrogate splits in model construction. For each split rule, a surrogate split rule that leads to sorting observations into child nodes in a similar way is retained. These surrogate splits can then be used to "guide" observations through the tree even if they have some missing feature values.
\end{enumerate}


  
