{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a81582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e21a04",
   "metadata": {},
   "source": [
    "## Solution 1: Splitting criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1e78d",
   "metadata": {},
   "source": [
    "### a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdef5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 1-a-1\n",
    "# self-defined function for computing mse of an array\n",
    "def mse(y):\n",
    "    l1 = lambda var: var - y.mean()\n",
    "    residuals = l1(y)\n",
    "    return np.mean(residuals**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e23cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 1-a-2\n",
    "# self-defined function for computing all possible splits of a regression tree and return the best splits and empirical risk\n",
    "def find_best_split(x_train,y_train):\n",
    "    best_threshold = None\n",
    "    min_risk = np.inf\n",
    "    unique_sorted_x = np.unique(x_train)\n",
    "    unique_sorted_x.sort()\n",
    "    # compute the threshold with biggest margin (middle of all unique values)\n",
    "    thesholds = unique_sorted_x[1:] - (unique_sorted_x[1:] - unique_sorted_x[:len(unique_sorted_x)-1])/2\n",
    "    \n",
    "    for t in thesholds:\n",
    "        y_left_ix = x_train < t # retuns an index set for all true values\n",
    "        y_left, y_right = y_train[y_left_ix], y_train[~y_left_ix] # ~ considers all other indices\n",
    "        weight_left = len(y_train[y_left_ix])/len(y_train) # compute weight of left node\n",
    "        t_mse = weight_left * mse(y_left) + (1-weight_left) * mse(y_right) # compute empirical risk of split t\n",
    "        print(\"split at %.2f: empirical risk = %.2f\" % (t,t_mse)) # tracking the emp. risk of each split\n",
    "        \n",
    "        if t_mse < min_risk: # save best split\n",
    "            min_risk = t_mse\n",
    "            best_threshold = t\n",
    "            \n",
    "    print(\"best split at \", best_threshold)\n",
    "    return {'threshold': best_threshold, 'empirical_risk': min_risk}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a25de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 1-a-3\n",
    "# actually compute regression tree for your data\n",
    "x = np.array([1, 2, 7, 10, 20])\n",
    "y = np.array([1, 1, 0.5, 10, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370f3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split at 1.50: empirical risk = 19.14\n",
      "split at 4.50: empirical risk = 13.43\n",
      "split at 8.50: empirical risk = 0.13\n",
      "split at 15.00: empirical risk = 12.64\n",
      "best split at  8.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 8.5, 'empirical_risk': 0.13333333333333333}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: 1-a-4\n",
    "# run function\n",
    "find_best_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e8cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split at 0.35: empirical risk = 19.14\n",
      "split at 1.32: empirical risk = 13.43\n",
      "split at 2.12: empirical risk = 0.13\n",
      "split at 2.65: empirical risk = 12.64\n",
      "best split at  2.1242476210246797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 2.1242476210246797, 'empirical_risk': 0.13333333333333333}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: 1-a-5\n",
    "# test with log transformed feature\n",
    "find_best_split(np.log(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8bde7",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a352f3",
   "metadata": {},
   "source": [
    "- For regression trees, we usually identify *impurity* with *variance*. Here is why:\n",
    "  - It is reasonable to define impurity via the deviation between actual target values and the predicted constant -- either using absolute or square distances to enforce symmetry of positive and negative residuals.\n",
    "  - Recall the constant \\(L2\\) risk minimizer for a node \\(\\mathcal{N}\\):\n",
    "    $$\n",
    "    \\bar y = \\arg\\min_c \\frac{1}{|\\mathcal{N}|}  \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2,\n",
    "    $$\n",
    "    because\n",
    "\n",
    "    \\begin{align*}\n",
    "    \\min_c \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2 &\\rightarrow \\frac{\\partial}{\\partial c} \\left( \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} (y_i^2 - 2y_i c + c^2) \\right) = 0 \\\\\n",
    "    &\\rightarrow \\frac{1}{|\\mathcal{N}|} \\left( \\sum_{i = 1}^{|\\mathcal{N}|} (-2y_i + 2c) \\right) = 0 \\\\\n",
    "    &\\rightarrow \\sum_{i = 1}^{|\\mathcal{N}|} (-2y_i + 2c) = 0 \\\\\n",
    "    &\\rightarrow -2 \\sum_{i = 1}^{|\\mathcal{N}|} y_i + 2|\\mathcal{N}|c = 0 \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    This implies $\\hat{c} = \\frac{1}{|\\mathcal{N}|} \\sum_{i = 1}^{|\\mathcal{N}|} y_i = \\bar{y}$.\n",
    "\n",
    "  - Consequently, we have \n",
    "    $$\n",
    "    \\bar y = \\arg\\min_c \\frac{1}{|\\mathcal{N}|}  \\sum_{i = 1}^{|\\mathcal{N}|} (y_i - c)^2,\n",
    "    $$\n",
    "    where the right hand side is the (biased) sample variance for sample mean \\(c\\).\n",
    "  - Therefore, predicting the sample mean both minimizes risk under \\(L2\\) loss and variance impurity.\n",
    "  - Since constant mean prediction is equivalent to an intercept LM (minimizing the sum of squared residuals!), regression trees with \\(L2\\) loss perform piecewise constant linear regression.\n",
    "  - The same correspondence holds between impurity via absolute distances and \\(L1\\) regression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-i2ml",
   "language": "python",
   "name": "python-i2ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
