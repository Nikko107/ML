In this exercise, we will examine loss functions for regression tasks 
somewhat more in depth.

```{r, echo=FALSE, fig.height=3, fig.width=5}
set.seed(1L)
x <- runif(20L, min = 0L, max = 10L)
y <- 0.2 + 3 * x
y <- y + rnorm(length(x), sd = 0.8)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw() + 
  ggplot2::annotate("point", x = 10L, y = 1L, color = "orange", size = 2)
```

Consider the above linear regression task. How will the model 
  parameters be affected by adding the new outlier point (orange) if you use
  $L1$ loss and $L2$ loss, respectively, 
  in the empirical risk? (You do not need to actually compute the 
  parameter values.)
  
::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
$L2$ loss penalizes vertical distances to the regression line *quadratically*, while $L1$ only considers the *absolute*
distance. As the outlier point lies pretty far from the remaining training data, it will have a large loss with $L2$,
and the regression line will pivot to the bottom right to minimize the resulting empirical risk. A model trained
with $L1$ loss is less susceptible to the outlier and will adjust only slightly to the new data.

```{r, echo=FALSE, fig.height=3, fig.width=5, message=FALSE}
set.seed(1L)
x <- runif(20L, min = 0L, max = 10L)
y <- 0.2 + 3 * x
y <- y + rnorm(length(x), sd = 0.8)
x <- c(x, 10L)
y <- c(y, 1L)
dt <- data.frame(x = x, y = y)

ggplot2::ggplot(dt, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw() + 
  ggplot2::geom_smooth(
    formula = y ~ x, 
    method = "lm", 
    se = FALSE, 
    ggplot2::aes(col = "L2 loss"), 
    linewidth = 0.7
  ) +
  ggplot2::geom_quantile(quantiles = 0.5, ggplot2::aes(col = "L1 loss")) +
  ggplot2::scale_color_manual(
    "", values = c("L2 loss" = "blue", "L1 loss" = "red")
  ) +
  ggplot2::annotate("point", x = 10L, y = 1L, color = "orange", size = 2)
```

</details> 
:::

***

```{r, echo=FALSE, fig.height=3, fig.width=5}
huber_loss <- function(res, delta = 0.5) {
  if (abs(res) <= delta) {
    0.5 * (res^2)
  } else {
    delta * abs(res) - 0.5 * (delta^2)
  }
}

x <- seq(-10L, 10L, length.out = 1000L)
y <- sapply(x, huber_loss, delta = 5L)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_line() + 
  ggplot2::theme_bw()
```

The second plot visualizes another loss function popular in 
  regression tasks, the so-called *Huber loss* (depending on 
  $\epsilon > 0$; here: $\epsilon = 5$). 
  Describe how the Huber loss deals with residuals as compared to $L1$ and $L2$ 
  loss.
  Can you guess its definition? 
  
::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
The Huber loss combines the respective advantages of $L1$ and $L2$ loss: 
  it is smooth and (once) differentiable like $L2$ but does not punish larger 
  residuals as severely, leading to more robustness. 
  It is simply a (weighted) piecewise combination of both losses, 
  where $\epsilon$ marks where $L2$ transits to $L1$ loss. The exact definition 
  is:
    $$
  \Lxy = \begin{cases}
    \frac{1}{2}(y - \fx)^2  & \text{ if } \lone \le \epsilon \\
    \epsilon \lone-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
    \end{cases}, \quad \epsilon > 0
  $$
  In the plot we can see how the parabolic shape of the loss around 0 evolves 
  into an absolute-value function at $\lone > \epsilon = 5$.
</details> 
:::
