Assume the following (noisy) data-generating process from which we have 
observed 50 realizations: $$y = -3 + 5 \cdot 
\sin(0.4 \pi x) + \epsilon$$ with $\epsilon \, \sim \mathcal{N}(0, 1)$.

***
We decide to model the data with a cubic polynomial (including intercept term). State the corresponding hypothesis space.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
Cubic means degree 3, so our hypothesis space will look as 
follows:
$$\Hspace = \{ \fxt = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 
~|~ (\theta_0, \theta_1, \theta_2, \theta_3)^\top \in \R^4 \}$$
</details>
:::

***  
State the empirical risk w.r.t. $\thetab$ for a member of the hypothesis space. Use $L2$ loss and be as explicit as possible.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
The empirical risk is:
$$\risket = \sum_{i = 1}^{50} \left(\yi - \left[ \theta_0 + \theta_1 x^{(i)} + 
\theta_2 \left( x^{(i)} \right)^2 + \theta_3 \left( x^{(i)} \right)^3 \right] 
\right)^2$$
</details>
:::

::: {.content-hidden when-profile="b"}
***
::: {.callout-tip icon=false title="Only for lecture group A"}
:::
We can minimize this risk using gradient descent. Derive the gradient of the empirical risk w.r.t $\thetab$.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We can 
find the gradient just as we did for an intermediate result when we derived 
the least-squares estimator:

\begin{align*}
  \nabla_{\thetab} \risket &=
  \pd{}{\thetab} \left \| \yv - \Xmat \thetab \right \|_2^2 \\
  &= \pd{}{\thetab} \left( \left(\yv - \Xmat \thetab\right)^\top 
  \left(\yv - \Xmat \thetab \right) \right) \\ 
  &= - 2 \Xmat^\top \yv + 2 \Xmat^\top \Xmat \thetab\\
  &= 2 \cdot \left( - \Xmat^\top \yv + \Xmat^\top 
  \Xmat \thetab \right)
\end{align*}
</details>
:::
:::

::: {.content-hidden when-profile="b"}

***
::: {.callout-tip icon=false title="Only for lecture group A"}
::: 
Using the result for the gradient, state the calculation to update the current 
  parameter $\thetat$.
  
::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
Recall that the idea of gradient descent (\textit{descent}!) is to 
  traverse the risk surface in the direction of the \textit{negative} gradient 
  as we are in search for the minimum.
  Therefore, we will update our current parameter set $\thetat$ with the 
  negative gradient of the current empirical risk w.r.t. $\thetab$, scaled 
  by learning rate (or step size) $\alpha$:
  
  $$\thetatn = \thetat - \alpha \cdot \nabla_{\thetab} \riske(\thetat).$$
  
  Note that the $L2$-induced multiplicative constant of 2 in the gradient 
  can simply be absorbed by $\tilde \alpha := \tfrac{1}{2} \alpha$:
  
  \begin{align*}
    \underbrace{\thetatn}_{p \times 1} &= \underbrace{\thetat}_{p \times 1} - 
    \tilde \alpha \cdot \left( - \underbrace{\Xmat^\top \phantom{y}}_{
    p \times n} \underbrace{\yv}_{n \times 1} + \underbrace{
    \Xmat^\top \Xmat \phantom{y}}_{p \times p} 
    \underbrace{\thetat \phantom{y}}_{p \times 1} 
    \right) \\
    \mat{\theta_1 \\ \theta_2 \\ \vdots \\ \theta_p}^{[t + 1]} &= 
    \mat{\theta_1 \\ \theta_2 \\ \vdots \\ \theta_p}^{[t]} - \tilde \alpha 
    \cdot \left( - \Xmat^\top \yv + \Xmat^\top \Xmat 
    \mat{\theta_1 \\ \theta_2 \\ \vdots \\ \theta_p}^{[t]} \right)
  \end{align*}
  
  What actually happens here: we update each component of our current 
  parameter vector $\thetat$ in the \textit{direction} of the negative 
  gradient, i.e., following the steepest downward slope, and also by an 
  \textit{amount} that depends on the value of the gradient.
  
  In order to see what that means it is helpful to recall that the gradient 
  $\nabla_{\thetab} \risket$ tells us about the effect (infinitesimally small) 
  changes in $\thetab$ have on $\risket$.
  Therefore, gradient updates focus on  influential components, and we 
  proceed more quickly along the important dimensions.
</details>
:::  
:::

***
You will not be able to fit the data perfectly with a cubic polynomial.
  Describe the advantages and disadvantages that a more 
  flexible model class would have. 
  Would you opt for a more flexible learner?
  
::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We see that, for example, the first model in exercise b) fits the data 
fairly well but not perfectly.
Choosing a more flexible function (a polynomial of higher degree or a function 
from an entirely different, more complex, model class) might be advantageous:

* We would be able to trace the observations more closely if our 
function were less smooth, and thus reduce empirical risk.
On the other hand, flexibility also has drawbacks:

* Flexible model classes often have more parameters, making training 
harder.

* We might run into a phenomenon called *overfitting*. 
Recall that our ultimate goal is to make predictions on *new* 
observations. 
However, fitting every quirk of the training observations -- possibly caused 
by imprecise measurement or other factors of randomness/error -- will not 
generalize so well to new data.

In the end, we need to balance model fit and generalization. 
We will discuss the choice of hypotheses quite a lot since it is one 
of the most crucial design decisions in machine learning. 

</details>

:::