---
title: "Exercise 1: ML basics"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
format:
  html:
    code-fold: false
    code-background: true
execute: 
  warning: false
  enabled: true
jupyter: python3
knitr:
  opts_chunk: 
    collapse: true
    comment: "#>" 
---

{{< include ex_preamble.qmd >}}

::: {.callout-tip}
## Learning goals

This and that.
:::

# Exercise 1: Car Price Prediction

Imagine you work at a second-hand car dealer and are tasked with finding for-sale vehicles your company can
acquire at a reasonable price. You decide to address this challenge in a data-driven manner and develop a model
that predicts adequate market prices (in EUR) from vehicles’ properties.

a) Characterize the task at hand: supervised or unsupervised? Regression or classification? Learning to explain
or learning to predict? Justify your answers.

a) How would you set up your data? Name potential features along with their respective data type and state the
target variable.

a) Assume now that you have data on vehicles’ age (days), mileage (km), and price (EUR). Explicitly define the
feature space $\Xspace$ and target space $\Yspace$.

a) You choose to use a linear model (LM) for this task. For this, you assume the targets to be conditionally
independent given the features, i.e., y(i)|x(i) ∈ y(j)|x(j) for all i, j ∈ {1, 2, . . . , n}, i 6 = j, with sample size n. The
LM models the target as a linear function of the features with Gaussian error term.
State the hypothesis space for the corresponding model class. For this, assume the parameter vector θ to include
the intercept coefficient.

a) Which parameters need to be learned? Define the corresponding parameter space Θ.

a) State the loss function for the $i$-th observation using $L2$ loss.

a) Now you need to optimize this risk to find the best parameters, and hence the best model, via empirical risk
minimization. State the optimization problem formally and list the necessary steps to solve it.

::: {.content-visible when-profile="a"}

a) In classical statistics, you would estimate the parameters via maximum likelihood estimation (MLE). The
likelihood for the LM is given by:

:::

::: {.content-visible when-profile="b"}

::: {.panel-tabset}

## R

```{r}
library(ggplot2)
```

## Python

```{python}
# from foo import bar
```

:::

:::
