---
title: "Exercise 1: ML basics"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
toc: true
format:
  html:
    code-fold: false
    code-background: true
  ipynb: default
execute: 
  warning: false
  enabled: true
jupyter: python3
knitr:
  opts_chunk: 
    collapse: true
    comment: "#>" 
---

::: {.content-hidden when-format="pdf"}

{{< include ../ex_preamble.qmd >}}

:::

::: {.callout-tip}
## Learning goals

This and that.
:::

# Exercise 1: Car Price Prediction

Imagine you work at a second-hand car dealer and are tasked with finding for-sale vehicles your company can
acquire at a reasonable price. You decide to address this challenge in a data-driven manner and develop a model
that predicts adequate market prices (in EUR) from vehicles’ properties.

(@) Characterize the task at hand: supervised or unsupervised? Regression or classification? Learning to explain
or learning to predict? Justify your answers.

::: {.content-visible when-profile="solution"}

<details> 

<summary>Solution</summary>

We face a **supervised regression** task: we definitely need 
labeled training data to infer a relationship between cars' attributes and 
their prices, and price in EUR is a continuous target (or quasi-continuous, 
to be exact -- as with all other quantities, we can only measure it with 
finite precision, but the scale is sufficiently fine-grained to assume 
continuity). **Prediction** is definitely the goal here, however, it 
might also be interesting to examine the explanatory contribution of each 
feature.

</details> 

:::

(@) How would you set up your data? Name potential features along with their respective data type and state the
target variable.

::: {.content-visible when-profile="solution"}

<details> 

<summary>Solution</summary>

Target variable and potential features:

| Variable | Role     | Data type| 
|:---------|:---------|----------|
| Price in EUR  |  Target  |    Numeric   |
| Age in days  |  Feature |   Numeric   |
| Accident-free y/n  | Feature | Binary |
| ... | ...| ... |

</details> 

:::

(@) Assume now that you have data on vehicles’ age (days), mileage (km), and price (EUR). Explicitly define the
feature space $\Xspace$ and target space $\Yspace$.

::: {.content-visible when-profile="solution"}

<details> 

<summary>Solution</summary>

Let $x_1$ and $x_2$ measure age and mileage, respectively. 
  Both features and target are numeric and (quasi-) continuous. It is also 
  reasonable to assume non-negativity for the features, such that we 
  obtain $\Xspace = (\R_{0}^{+})^2$, with $\xi = (x_1, x_2)^{(i)} \in \Xspace$ 
  for $i = 1, 2, \dots, n$ observations. 
  As the standard LM does not impose any 
  restrictions on the target, we have $\Yspace = \R$, though we would probably 
  discard negative predictions in practice.
  
 </details> 

:::

(@) You choose to use a linear model (LM) for this task. For this, you assume the targets to be conditionally
independent given the features, i.e., y(i)|x(i) ∈ y(j)|x(j) for all i, j ∈ {1, 2, . . . , n}, i 6 = j, with sample size n. The
LM models the target as a linear function of the features with Gaussian error term.
State the hypothesis space for the corresponding model class. For this, assume the parameter vector θ to include
the intercept coefficient.

(@) Which parameters need to be learned? Define the corresponding parameter space Θ.

(@) State the loss function for the $i$-th observation using $L2$ loss.

(@) Now you need to optimize this risk to find the best parameters, and hence the best model, via empirical risk
minimization. State the optimization problem formally and list the necessary steps to solve it.

::: {.content-visible when-profile="a"}

(@) In classical statistics, you would estimate the parameters via maximum likelihood estimation (MLE). The
likelihood for the LM is given by:

:::

::: {.content-visible when-profile="foo"}

BAAAAAAAR

:::

::: {.content-visible when-profile="b"}

::: {.panel-tabset}

## R

```{r}
library(ggplot2)
```

## Python

```{python}
# from foo import bar
```

:::

:::
