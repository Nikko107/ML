<<setup-child, include = FALSE>>=
knitr::set_parent("../../style/preamble_ueb.Rnw")
@

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{2}

\loesung{
\begin{enumerate}[a)]
    \item We know that, in the optimum, (log-)likelihood is maximal. We can
  directly translate this into risk minimization by using the \emph{negative}
  log-likelihood as our empirical risk. We will just use the pointwise negative
  log-likelihood as our loss function: $$\Lpixyit = - \left(\yi \log \left(
  \pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit \right)
  \right) \right)$$ (the so-called \emph{Bernoulli loss}). The
  empirical risk is then the sum of point-wise losses: $$\risket = -\sumin \yi
  \log \left(\pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit
  \right) \right)$$
  \item We can now solve this optimization problem via empirical risk
  minimization, which, in this case, is perfectly equivalent to ML estimation.
  Therefore, we set the first derivative of $\risket$ wrt $\thetab$ to 0 and
  solve for $\thetab$. However -- unlike linear regression -- this has no
  closed-form solution, so a numerical optimization procedure such as gradient
  descent is required.
\end{enumerate}

}

\dlz

\loesung{
    <<child="exercises/lin_reg_l_2.Rnw">>=
    @
}
