{
  "hash": "57a378a5ead58d01471f235f479f61de",
  "result": {
    "markdown": "---\ntitle: \"Exercise 1: ML basics\"\nsubtitle: \"[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)\"\nformat:\n  html:\n    code-fold: false\n    code-background: true\n  ipynb: default\nexecute: \n  warning: false\n  enabled: true\njupyter: python3\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \n---\n\n\n\n\n::: {.content-hidden when-format=\"pdf\"}\n\n::: {.hidden}\n$$\n \\def \\Xspace{\\mathcal{X}}\n \\def \\Yspace{\\mathcal{Y}}\n \n % math spaces\n\\ifdefined\\N                                                                \n\\renewcommand{\\N}{\\mathds{N}} % N, naturals\n\\else \\newcommand{\\N}{\\mathds{N}} \\fi \n\\newcommand{\\Z}{\\mathds{Z}} % Z, integers\n\\newcommand{\\Q}{\\mathds{Q}} % Q, rationals\n\\newcommand{\\R}{\\mathds{R}} % R, reals\n\\ifdefined\\C \n  \\renewcommand{\\C}{\\mathds{C}} % C, complex\n\\else \\newcommand{\\C}{\\mathds{C}} \\fi\n\\newcommand{\\continuous}{\\mathcal{C}} % C, space of continuous functions\n\\newcommand{\\M}{\\mathcal{M}} % machine numbers\n\\newcommand{\\epsm}{\\epsilon_m} % maximum error\n\n% counting / finite sets\n\\newcommand{\\setzo}{\\{0, 1\\}} % set 0, 1\n\\newcommand{\\setmp}{\\{-1, +1\\}} % set -1, 1\n\\newcommand{\\unitint}{[0, 1]} % unit interval\n\n% basic math stuff\n\\newcommand{\\xt}{\\tilde x} % x tilde\n\\newcommand{\\argmax}{\\operatorname{arg\\,max}} % argmax\n\\newcommand{\\argmin}{\\operatorname{arg\\,min}} % argmin\n\\newcommand{\\argminlim}{\\mathop{\\mathrm{arg\\,min}}\\limits} % argmax with limits\n\\newcommand{\\argmaxlim}{\\mathop{\\mathrm{arg\\,max}}\\limits} % argmin with limits  \n\\newcommand{\\sign}{\\operatorname{sign}} % sign, signum\n\\newcommand{\\I}{\\mathbb{I}} % I, indicator\n\\newcommand{\\order}{\\mathcal{O}} % O, order\n\\newcommand{\\pd}[2]{\\frac{\\partial{#1}}{\\partial #2}} % partial derivative\n\\newcommand{\\floorlr}[1]{\\left\\lfloor #1 \\right\\rfloor} % floor\n\\newcommand{\\ceillr}[1]{\\left\\lceil #1 \\right\\rceil} % ceiling\n\n% sums and products\n\\newcommand{\\sumin}{\\sum\\limits_{i=1}^n} % summation from i=1 to n\n\\newcommand{\\sumim}{\\sum\\limits_{i=1}^m} % summation from i=1 to m\n\\newcommand{\\sumjn}{\\sum\\limits_{j=1}^n} % summation from j=1 to p\n\\newcommand{\\sumjp}{\\sum\\limits_{j=1}^p} % summation from j=1 to p\n\\newcommand{\\sumik}{\\sum\\limits_{i=1}^k} % summation from i=1 to k\n\\newcommand{\\sumkg}{\\sum\\limits_{k=1}^g} % summation from k=1 to g\n\\newcommand{\\sumjg}{\\sum\\limits_{j=1}^g} % summation from j=1 to g\n\\newcommand{\\meanin}{\\frac{1}{n} \\sum\\limits_{i=1}^n} % mean from i=1 to n\n\\newcommand{\\meanim}{\\frac{1}{m} \\sum\\limits_{i=1}^m} % mean from i=1 to n\n\\newcommand{\\meankg}{\\frac{1}{g} \\sum\\limits_{k=1}^g} % mean from k=1 to g\n\\newcommand{\\prodin}{\\prod\\limits_{i=1}^n} % product from i=1 to n\n\\newcommand{\\prodkg}{\\prod\\limits_{k=1}^g} % product from k=1 to g\n\\newcommand{\\prodjp}{\\prod\\limits_{j=1}^p} % product from j=1 to p\n\n% linear algebra\n\\newcommand{\\one}{\\boldsymbol{1}} % 1, unitvector\n\\newcommand{\\zero}{\\mathbf{0}} % 0-vector\n\\newcommand{\\id}{\\boldsymbol{I}} % I, identity\n\\newcommand{\\diag}{\\operatorname{diag}} % diag, diagonal\n\\newcommand{\\trace}{\\operatorname{tr}} % tr, trace\n\\newcommand{\\spn}{\\operatorname{span}} % span\n\\newcommand{\\scp}[2]{\\left\\langle #1, #2 \\right\\rangle} % <.,.>, scalarproduct\n\\newcommand{\\mat}[1]{\\begin{pmatrix} #1 \\end{pmatrix}} % short pmatrix command\n\\newcommand{\\Amat}{\\mathbf{A}} % matrix A\n\\newcommand{\\Deltab}{\\mathbf{\\Delta}} % error term for vectors\n\n% basic probability + stats\n\\renewcommand{\\P}{\\mathds{P}} % P, probability\n\\newcommand{\\E}{\\mathds{E}} % E, expectation\n\\newcommand{\\var}{\\mathsf{Var}} % Var, variance\n\\newcommand{\\cov}{\\mathsf{Cov}} % Cov, covariance\n\\newcommand{\\corr}{\\mathsf{Corr}} % Corr, correlation\n\\newcommand{\\normal}{\\mathcal{N}} % N of the normal distribution\n\\newcommand{\\iid}{\\overset{i.i.d}{\\sim}} % dist with i.i.d superscript\n\\newcommand{\\distas}[1]{\\overset{#1}{\\sim}} % ... is distributed as ...\n$$\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n## Learning goals\n\nThis and that.\n:::\n\n# Exercise 1: Car Price Prediction\n\nImagine you work at a second-hand car dealer and are tasked with finding for-sale vehicles your company can\nacquire at a reasonable price. You decide to address this challenge in a data-driven manner and develop a model\nthat predicts adequate market prices (in EUR) from vehicles’ properties.\n\na) Characterize the task at hand: supervised or unsupervised? Regression or classification? Learning to explain\nor learning to predict? Justify your answers.\n\n::: {.content-visible when-profile=\"solution\"}\n\n<details> \n\n<summary>Solution</summary>\n\ntrivial\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n</details> \n\n:::\n\na) How would you set up your data? Name potential features along with their respective data type and state the\ntarget variable.\n\na) Assume now that you have data on vehicles’ age (days), mileage (km), and price (EUR). Explicitly define the\nfeature space $\\Xspace$ and target space $\\Yspace$.\n\na) You choose to use a linear model (LM) for this task. For this, you assume the targets to be conditionally\nindependent given the features, i.e., y(i)|x(i) ∈ y(j)|x(j) for all i, j ∈ {1, 2, . . . , n}, i 6 = j, with sample size n. The\nLM models the target as a linear function of the features with Gaussian error term.\nState the hypothesis space for the corresponding model class. For this, assume the parameter vector θ to include\nthe intercept coefficient.\n\na) Which parameters need to be learned? Define the corresponding parameter space Θ.\n\na) State the loss function for the $i$-th observation using $L2$ loss.\n\na) Now you need to optimize this risk to find the best parameters, and hence the best model, via empirical risk\nminimization. State the optimization problem formally and list the necessary steps to solve it.\n\n::: {.content-visible when-profile=\"a\"}\n\na) In classical statistics, you would estimate the parameters via maximum likelihood estimation (MLE). The\nlikelihood for the LM is given by:\n\n:::\n\n::: {.content-visible when-profile=\"b\"}\n\n::: {.panel-tabset}\n\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# from foo import bar\n```\n:::\n\n\n\n\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}