\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[utf8]{inputenc}
\pagenumbering{arabic}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{mathtools}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}

\input{../../style/common}

\tcbset{enhanced}

%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}

\font \sfbold=cmssbx10
\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}

\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
% \pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
	{\sf \bf \huge Exercise Collection -- #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\exlect}
  {\color{black} \hrule \section{Lecture exercises}}
  
\newcommand{\exexams}
  {\color{black} \hrule \section{Further exercises}}
  % rename so it is not immediately clear these are from past exams
  
\newcommand{\exinspo}
  {\color{black} \hrule \section{Ideas \& exercises from other sources}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1} 
	\noindent}
	{\vspace{0.5cm}}
	
\newenvironment{aufgabeexam}[3] % semester, first or second, question number
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1, #2, question #3}
	\noindent}
	{\vspace{1.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\color{gray} \refstepcounter{loes}\textbf{Solution \arabic{loes}:}
	\\ \noindent}
	{\bigskip}

\setcounter{secnumdepth}{0}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{ML Basics}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{ML Tasks}{

Identify which type of machine learning (supervised or unsupervised, type of
of task, learning to predict or to explain) could be used in these cases:

\begin{enumerate}[a)]

  \item When crossing the alps using the Brenner Autobahn, there is the 
  option to pay electronically in advance. When approaching the toll station, 
  the barrier automatically opens when the number plate was recognised. The 
  recognition happens automatically by a digital camera system.
  \item Diagnose whether a patient suffers from cancer or not.
  \item The owner of an internet site wants to protect her system against 
  various violations of the terms of service (bot programs, manipulation of 
  timestamps, etc.)
  \item An online shopping portal wants to determine products that are 
  automatically offered to registered customers upon login.
  \item We want to sort the contents in our news feed into different groups.
  \item We want to sort our e-mails into spam / non-spam.
  \item In a supermarket, products that are often bought together shall be 
  placed side by side to increase the sales.
  \item We want to determine our top customers (e.g., w.r.t. highest sales, 
  logistics etc.).
  \item A call center estimates the amount of customer traffic to facilitate 
  staff planning.
\end{enumerate}
}
\newpage

\loesung{

\begin{enumerate}[a)]
  \item supervised learning / multi-class classification (plate digits) / 
  learning to predict
  \item supervised / binary classification / learning to predict, perhaps also 
  learning to explain
  \item (un)supervised / outlier detection / learning to predict
  \item unsupervised / frequent pattern mining
  \item (un)supervised / classification or clustering / learning to predict
  \item supervised / binary classification / learning to predict
  \item unsupervised / clustering or association rules 
  \item not a machine learning task
  \item supervised / regression / learning to predict, perhaps also learning to 
  explain
\end{enumerate}
}

\aufgabe{Simple Regression Problem I}{

%x <- rnorm(6,1)
%x <- c(0.56, 0.22, 1.7, 0.63, 0.36,1.2)
%y <- c(160,150,175,185,165,170)
%data <- t(cbind(x,y))

Suppose we observe 6 data pairs and want to describe the underlying relationship between target $y$ and feature $\xv$.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | c | c | }
    \hline
$\xv$ & 0.56 & 0.22 & 1.7 & 0.63 & 0.36 & 1.2 \\ \hline
y & 160 & 150 & 175 & 185 & 165 & 170 \\
    \hline
  \end{tabular}
\end{center}


\begin{itemize}
\item[a)] Assume a standard linear relationship $$\yi = \beta_0 + \beta_1 \xi + \epsi$$ with iid errors $\epsi$ and calculate the least squares estimator $\hat{\bm{\beta}}$ for $\bm{\beta}=(\beta_0, \beta_1)^\top$ manually (+ calculator).


\item[b)] Assume a non-linear relationship (polynomial degree 2) $$\yi = \beta_0 + \beta_1 \xi +\beta_2 (\xi)^2 + \epsi$$ with iid errors $\epsi$ and calculate the least squares estimator $\hat{\bm{\beta}}$ for $\bm{\beta}=(\beta_0, \beta_1, \beta_2)^\top$ with R.
\end{itemize}
}
\dlz
\loesung{




\begin{enumerate}
\item[a)] We use the least squares-estimator introduced in the lecture:
\lz
$\hat{\beta} = (X^TX)^{-1}X^Ty$  with \\ $ X = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
1 & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
\vdots & \vdots & \vdots & ... & \vdots \\
1 & x_{n,1} & x_{n,2} & ... & x_{n,m} \\
\end{bmatrix}
\lz
$  \\
$ x =  \begin{bmatrix}
0.56  \\
0.22  \\
1.7  \\
0.63  \\
0.36  \\
1.2  \\
\end{bmatrix}, X =  \begin{bmatrix}
1 & 0.56  \\
1 & 0.22  \\
1 & 1.7  \\
1 & 0.63  \\
1 & 0.36  \\
1 & 1.2  \\
\end{bmatrix}
$
and
 $
y =  \begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix}
$

Then \begin{align*}
\hat{\beta} &= (X^TX)^{-1}X^Ty \\ &= \left(
\begin{bmatrix}
1 & 1 & 1 & ... & 1  \\
x_{1,1} & x_{2,1} & x_{3,1} & ... & x_{n,1} \\
\vdots & \vdots & \vdots & ... & \vdots \\
x_{1,m} & x_{2,m} & x_{3,m} & ... & x_{n,m} \\
\end{bmatrix}
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
1 & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
\vdots & \vdots & \vdots & ... & \vdots \\
1 & x_{n,1} & x_{n,2} & ... & x_{n,m} \\
\end{bmatrix} \right)^{-1}\begin{bmatrix}
1 & 1 & 1 & ... 1  \\
x_{1,1} & x_{2,1} & x_{3,1} & ... & x_{n,1} \\
\vdots & \vdots & \vdots & ... & \vdots \\
x_{1,m} & x_{2,m} & x_{3,m} & ... & x_{n,m} \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &= \left(
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
1 & 0.56  \\
1 & 0.22  \\
1 & 1.7  \\
1 & 0.63  \\
1 & 0.36  \\
1 & 1.2  \\
\end{bmatrix} \right)^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &=
\begin{bmatrix}
6 & 4.67  \\
4.67 & 5.2185   \\
\end{bmatrix}^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &=
 \begin{bmatrix}
0.5491944 & -0.4914703  \\
-0.4914703  & 0.6314394   \\
\end{bmatrix}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\
&= \begin{bmatrix}
0.2739710 & 0.4410709 & -0.2863051  & 0.23956809 & 0.3722651 & -0.04056998 \\
-0.1378643 & -0.3525536 & 0.5819766 & -0.09366351 & -0.2641521 & 0.26625693 \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\
&= \begin{bmatrix}
158.73954  \\
11.25541  \\
\end{bmatrix}
\end{align*}


Hence the linear model $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x = 158.73954 + 11.25541 x$

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0.56}\hlstd{,} \hlnum{0.22}\hlstd{,} \hlnum{1.7}\hlstd{,} \hlnum{0.63}\hlstd{,} \hlnum{0.36}\hlstd{,}\hlnum{1.2}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{160}\hlstd{,}\hlnum{150}\hlstd{,}\hlnum{175}\hlstd{,}\hlnum{185}\hlstd{,}\hlnum{165}\hlstd{,}\hlnum{170}\hlstd{)}

\hlstd{X} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{k}\hlstd{) x}\hlopt{^}\hlstd{k)}
\hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\end{alltt}
\begin{verbatim}
##           [,1]
## [1,] 158.73954
## [2,]  11.25541
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-17-1} 

}


\end{knitrout}


\item[b)]
Here  $
X =  \begin{bmatrix}
1 & 0.56 & 0.3136 \\
1 & 0.22 & 0.0484 \\
1 & 1.7 & 2.89 \\
1 & 0.63 & 0.3969 \\
1 & 0.36 & 0.1296 \\
1 & 1.2  & 1.44\\
\end{bmatrix}
$
 and $\hat{\beta} =  \begin{bmatrix}
143.51682 \\
57.59155 \\
-23.96347 \\
\end{bmatrix} $

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0.56}\hlstd{,} \hlnum{0.22}\hlstd{,} \hlnum{1.7}\hlstd{,} \hlnum{0.63}\hlstd{,} \hlnum{0.36}\hlstd{,}\hlnum{1.2}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{160}\hlstd{,}\hlnum{150}\hlstd{,}\hlnum{175}\hlstd{,}\hlnum{185}\hlstd{,}\hlnum{165}\hlstd{,}\hlnum{170}\hlstd{)}

\hlstd{X} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{2}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{k}\hlstd{) x}\hlopt{^}\hlstd{k)}
\hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\end{alltt}
\begin{verbatim}
##           [,1]
## [1,] 143.51681
## [2,]  57.59155
## [3,] -23.96347
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-19-1} 

}


\end{knitrout}


\end{enumerate}
}

\newpage
\aufgabe{Simple Regression Problem II}{

Suppose we observe 6 data pairs and want to describe the underlying relationship between target $y$ and feature $\xv$.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | c | c | }
    \hline
$\xv$ & 0.56 & 0.22 & 1.7 & 0.63 & 0.36 & 1.2 \\ \hline
y & 160 & 150 & 175 & 185 & 165 & 170 \\
    \hline
  \end{tabular}
\end{center}

\begin{itemize}
    \item[a)] For the linear model $$\fxi = \theta_0 + \theta_1 \xi$$ with L2 loss, starting from $\thetab^{[0]} = (0,0)$ calculate one step of gradient descent with a stepsize of $\alpha = 0.1$.
    \item[b)] Implement a function \texttt{grad\_desc(x, y, iterations, alpha = 0.1)} that computes \texttt{iterations} steps of gradient descent with a learning rate of \texttt{alpha} for a linear regression with L2 loss. You can initialize all model parameters at $0$.
    \item[c)] How do the parameters estimated by gradient descent differ from the parameters estimated by the analytical solution (see ex. sheet 1) for different values of \texttt{iterations}.
\end{itemize}
}
\dlz
\loesung{

\begin{enumerate}[a)]

    \item From the lecture we can retrieve the empirical risk function:

\[
R_{emp}(\theta) = \frac{1}{n}(X\theta - Y)^2
\]

A gradient descend step is given by:
\[
\theta^{[t+1]} =\theta^{[t]} - \alpha \frac{\partial}{\partial\theta}R_{emp}(\theta^{[t]})
\]

Therefore we need the derivative of $R_{emp}$:

\[
\frac{\partial}{\partial\theta}R_{emp}(\theta^{[t]}) =  \frac{2}{n}X^T[X\theta^{[t]} - y]
\]

\begin{align*}
    \frac{\partial}{\partial\theta}R_{emp}(\theta^{[0]}) &= \frac{2}{6} \begin{bmatrix}
        1 & 1 & 1 & 1 & 1 & 1 \\
        0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
        \end{bmatrix}\left(\begin{bmatrix}
            1 & 0.56  \\
            1 & 0.22  \\
            1 & 1.7  \\
            1 & 0.63  \\
            1 & 0.36  \\
            1 & 1.2  \\
            \end{bmatrix}\begin{bmatrix}0 \\ 0\end{bmatrix} - \begin{bmatrix}
                160  \\
                150  \\
                175  \\
                185  \\
                165  \\
                170  \\
                \end{bmatrix} \right) \\ &=
    -\frac{1}{3} \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 \\
    0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
    \end{bmatrix} \begin{bmatrix}
        160  \\
        150  \\
        175  \\
        185  \\
        165  \\
        170  \\
        \end{bmatrix} \\
        &=
        \begin{bmatrix} -335 \\ -266.683\end{bmatrix}
\end{align*}

\begin{align*}
    \theta^{[1]} &= \begin{bmatrix}0 \\ 0\end{bmatrix} - 0.1  \begin{bmatrix} -335 \\ -266.683\end{bmatrix} =  \begin{bmatrix} 33.5 \\ 26.6683\end{bmatrix}
\end{align*}

We can easily verify the computation in R:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{r_emp_derivative} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{theta_t}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{y}\hlstd{) \{}
  \hlnum{2} \hlopt{/} \hlkwd{nrow}\hlstd{(X)} \hlopt{*} \hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{(X} \hlopt{%*%} \hlstd{theta_t} \hlopt{-} \hlstd{y))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{grad_desc_step} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{theta_t}\hlstd{,} \hlkwc{X}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{alpha}\hlstd{) \{}
  \hlstd{theta_t} \hlopt{-} \hlstd{alpha} \hlopt{*} \hlkwd{r_emp_derivative}\hlstd{(theta_t, X, y)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Note that $X$ is a matrix with $X = [1\;x]$, as
\[
f(X) = \theta X = \theta_0 1_{n} + \theta_1 x
\]

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{small_x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0.56}\hlstd{,} \hlnum{0.22}\hlstd{,} \hlnum{1.7}\hlstd{,} \hlnum{0.63}\hlstd{,} \hlnum{0.36}\hlstd{,} \hlnum{1.2}\hlstd{)}
\hlstd{X} \hlkwb{=} \hlkwd{cbind}\hlstd{(}\hlnum{1}\hlstd{, small_x)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{160}\hlstd{,} \hlnum{150}\hlstd{,} \hlnum{175}\hlstd{,} \hlnum{185}\hlstd{,} \hlnum{165}\hlstd{,} \hlnum{170}\hlstd{)}

\hlkwd{grad_desc_step}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{), X, y,} \hlnum{0.1}\hlstd{)}
\end{alltt}
\begin{verbatim}
##             [,1]
##         33.50000
## small_x 26.66833
\end{verbatim}
\end{kframe}
\end{knitrout}

\item

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{grad_desc} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{small_x}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{iterations}\hlstd{,} \hlkwc{alpha} \hlstd{=} \hlnum{0.1}\hlstd{) \{}
  \hlstd{X} \hlkwb{=} \hlkwd{cbind}\hlstd{(}\hlnum{1}\hlstd{, small_x)}
  \hlstd{theta_t} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
  \hlcom{# Repeat for n steps}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{iterations) \{}
    \hlstd{theta_t} \hlkwb{=} \hlkwd{grad_desc_step}\hlstd{(theta_t, X, y, alpha)}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(theta_t)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\item

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{iterations} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{25}\hlstd{,} \hlnum{50}\hlstd{,} \hlnum{100}\hlstd{,} \hlnum{250}\hlstd{,} \hlnum{500}\hlstd{,} \hlnum{750}\hlstd{,} \hlnum{1000}\hlstd{)}
\hlstd{theta_hat} \hlkwb{=} \hlkwd{sapply}\hlstd{(iterations,} \hlkwa{function}\hlstd{(}\hlkwc{z}\hlstd{)} \hlkwd{grad_desc}\hlstd{(small_x, y, z))}
\hlstd{predictions} \hlkwb{=} \hlkwd{cbind}\hlstd{(iterations,} \hlkwd{t}\hlstd{(theta_hat))}
\hlkwd{colnames}\hlstd{(predictions)} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlstr{"iterations"}\hlstd{,} \hlstr{"theta_0"}\hlstd{,} \hlstr{"theta_1"}\hlstd{)}

\hlstd{predictions}
\end{alltt}
\begin{verbatim}
##      iterations  theta_0  theta_1
## [1,]          1  33.5000 26.66833
## [2,]         25 127.9945 44.67538
## [3,]         50 144.6637 26.55818
## [4,]        100 155.7887 14.46344
## [5,]        250 158.7124 11.28496
## [6,]        500 158.7395 11.25542
## [7,]        750 158.7395 11.25541
## [8,]       1000 158.7395 11.25541
\end{verbatim}
\end{kframe}
\end{knitrout}

Looking at exercise 2 of the previous sheet, one can see, that
$\hat{\theta_0} = 158.73954$ and $\hat{\theta_1} = 11.25541$. We know, that
this is a local minimum. The values from the gradient descent only slowly
approaches the local minimum. As we can see, the predicted $\hat{\theta}_{\mathrm{grad\_desc}}$ of the
\texttt{grad\_desc} function only coincides with $\hat{\theta}_{\mathrm{analytical}}$
for a large number of iterations. Since the starting value $\theta^{[0]} = (0 ,0)$
is not close to the minimum, it takes some steps to get a good approximation.

\end{enumerate}
}

\newpage

\aufgabe{Car Price Prediction}{

Imagine you work at a second-hand car dealer and are tasked with finding 
for-sale vehicles your company can acquire at a reasonable price. You decide to 
address this challenge in a data-driven manner and develop a model that predicts 
adequate market prices (in EUR) from vehicles’ properties.

\begin{enumerate}[a)]

  \item Characterize the task at hand: supervised or unsupervised? Regression or 
  classification? Learning to explain or learning to predict? Justify your 
  answers. \textbf{\textcolor{teal}{[only for lecture group B]}}
  \item How would you set up your data? Name potential features along with their 
  respective data type and state the target variable.
  \item Assume now that you have data on vehicles’ age (days), mileage (km), and 
  price (EUR). 
  Explicitly define the feature space $\Xspace$ and target space $\Yspace$.
  \item You choose to use a linear model (LM) for this task.
  % For this, you assume the targets to be conditionally independent given the 
  % features, i.e., $\yi|\xi \in \yi[j]|\xi[j]$ for all $i,j \in 
  % \{1, 2, \dots, n\}, i \neq j$, with sample size $n$.
  The LM models the target as a linear function of the features 
  with Gaussian error term.
  % : $\yv = \Xmat \thetab + \epsilon$, \\ 
  % $\epsilon \sim N(\bm{0}, \mathit{diag}(\sigma^2)), ~~ \sigma > 0$.
  % Furthermore, you have reason to believe that the effect of mileage might be 
  % non-linear, so you decide to include this quantity logarithmically (using the 
  % natural logarithm).
  
  State the hypothesis space for the corresponding model class.
  For this, assume the parameter vector $\thetab$ to include the intercept 
  coefficient.
  \item Which parameters need to be learned?
  Define the corresponding parameter space $\Theta$.
  \item State the loss function for the $i$-th observation using $L2$ loss. 
  % \item In classical statistics, you would estimate the parameters via maximum 
  % likelihood estimation (MLE). 
  % The likelihood for the LM is given by:
  % % \[
  % % \ell(\thetab) = - \frac{n}{2} \log(2 \sigma^2 \pi) - \frac{1}{2 \sigma^2} 
  % % (\yv - \Xmat \thetab)^T(\yv - \Xmat \thetab)
  % % \]
  % \begin{flalign*}
  % \LL(\thetab | \xv) &= 
  % \prodin \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-
  % \frac{1}{2 \sigma^2} \left( \yi - \thetab^T \xi  \right)^2  \right) 
  % % \\
  % % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % % \frac{1}{2 \sigma^2} \sumin \left(\yi - \thetat \xi \right)^2  \right) \\ 
  % % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % % \frac{1}{2 \sigma^2} \| \yv - \Xmat \thetab \|^2 \right)
  % \end{flalign*}
  % Describe how you can make use of the likelihood in empirical risk minimization 
  % (ERM) and write down the resulting empirical risk.
  \item Now you need to optimize this risk to find the best parameters, 
  and hence the best model, via empirical risk minimization. 
  State the optimization problem formally and list the necessary steps to solve 
  it. 

\end{enumerate}

Congratulations, you just designed your first machine learning project!

}
\dlz
\loesung{

\begin{enumerate}[a)]

  \item We face a \textbf{supervised regression} task: we definitely need 
  labeled training data to infer a relationship between cars' attributes and 
  their prices, and price in EUR is a continuous target (or quasi-continuous, 
  to be exact -- as with all other quantities, we can only measure it with 
  finite precision, but the scale is sufficiently fine-grained to assume 
  continuity). \textbf{Prediction} is definitely the goal here, however, it 
  might also be interesting to examine the explanatory contribution of each 
  feature.
  
  \item Target variable and potential features: \\
  
  \begin{tabular}{l|l|l}
    \textbf{Variable} & \textbf{Role} & \textbf{Data type} \\ \hline
    Price in EUR & Target & Numeric \\ \hline
    Age in days & Feature & Numeric \\ \hline
    Mileage in km & Feature & Numeric \\ \hline
    Brand & Feature & Categorical \\ \hline
    Accident-free y/n & Feature & Binary \\ \hline
    \dots & \dots & \dots
  \end{tabular}
  
  \item Let $x_1$ and $x_2$ measure age and mileage, respectively. 
  Both features and target are numeric and (quasi-) continuous. It is also 
  reasonable to assume non-negativity for the features, such that we 
  obtain $\Xspace = (\R_{0}^{+})^2$, with $\xi = (x_1^{(i)}, x_2^{(i)})^\top 
  \in \Xspace$ for $i = 1, 2, \dots, n$ observations. 
  As the standard LM does not impose any 
  restrictions on the target, we have $\Yspace = \R$, though we would probably 
  discard negative predictions in practice.
  
  \item We can write the hypothesis space as:
  
  \begin{flalign*}
    \Hspace = \{\fxt = \thetab^\top \xv ~|~ \thetab \in \R^3 \}
    =  \{\fxt = \theta_0 + \theta_1 x_1 + \theta_2 x_2 ~|~ 
    (\theta_0, \theta_1, \theta_2) \in \R^3 \}.
    % \Hspace &= \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \thetab^\top \xv, ~ (\theta_0, \thetab) \in \R^3 \} \\
    % &=  \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \theta_{\text{age}} x_{\text{age}} + 
    % \theta_{\text{mileage}} x_{\text{mileage}}, ~ (\theta_0, 
    % \theta_{\text{age}}, \theta_{\text{mileage}}) \in \R^3 \},
  \end{flalign*}
  
  Note the \textbf{slight abuse of notation} here: in the lecture, we first 
  define $\thetab$ to only consist of the feature coefficients, with $\xv$ 
  likewise being the plain feature vector. For the sake of simplicity, however, 
  it is more convenient to append the intercept coefficient to the vector of 
  feature coefficients. This does not change our model formulation, but we have 
  to keep in mind that it implicitly entails adding an element 1 at the first 
  position of each feature vector.
  % , i.e., $\xi := (1, x_1, x_2)^{(i)} \in 
  % \{1\} \cup \Xspace$, constituting the familiar column of ones in the design 
  % matrix $\Xmat$.
  
  \item The parameter space is included in the definition of the hypothesis 
  space and in this case given by $\Theta = \R^3$.
  
  \item Loss function for the $i$-th observation: $\Lxyit = \left( \yi - 
  \thetab^\top \xi \right)^2$.
  
  % \item The first thing to note is that both MLE and ERM are 
  % \textbf{optimization problems}, and both should lead us to the same optimum. 
  % Their opposite signs are not a problem: maximizing the likelihood is 
  % equivalent to minimizing the negative likelihood. 
  % Also, both are defined pointwise.
  % The last thing to fix is therefore the product introduced by the independence 
  % assumption in the joint likelihood of all observations (recall that we use 
  % a \textit{summed} loss in ERM), for which the logarithm is a natural remedy.
  % We can thus simply use the \textbf{negative log-likelihood (NLL)} as our loss 
  % function (and indeed, many known loss functions can be shown to correspond to 
  % certain model likelihoods).
  % 
  % Let's put these reflections to practice:
  % 
  % \begin{flalign*}
  %   L_{NLL}\left (\yi, f\left( \xi | \thetab \right) \right) 
  %   &= - \log \LL(\thetab | \xi) \\
  %   &= - \ell(\thetab | \xi) \\
  %   &= - \log \left(\frac{1}{\sqrt{2 \pi 
  %   \sigma^2}} \exp \left(- \frac{1}{2 \sigma^2} \left( \yi - \thetab^\top \xi  
  %   \right)^2  \right) \right) \\
  %   &= - \left(\log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) + 
  %   \log \left( \exp \left(- \frac{1}{2 \sigma^2} \left( \yi - \thetab^\top \xi  
  %   \right)^2  \right) \right) \right)  \\
  %   &= - \left(- \frac{1}{2}\log(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \left( 
  %   \yi - \thetab^\top \xi \right)^2 \right) \\
  %   &= \frac{1}{2}\log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left( 
  %   \yi - \thetab^\top \xi \right)^2
  % \end{flalign*}
  % 
  % \begin{flalign*}
  %   \risket &= \sumin - \ell(\thetab | \xi) \\
  %   &= \sumin L_{NLL}\left (\yi, f\left( \xi | \thetab \right) \right)  \\
  %   &= \sumin \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} 
  %   \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &= \frac{n}{2}\log(2 \pi \sigma^2) +  \frac{1}{2 \sigma^2} \sumin
  %   \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &\propto \sumin \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &= \sumin \Lxyit ~ (L2 \text{ loss})
  % \end{flalign*}  
  % 
  % As we are only interested in the feature coefficients here, we neglect all 
  % irrelevant terms that do not depend on $\thetab$ as they have no effect on
  % the solution (i.e., the $\argmin$ of $\risket$).
  % This is what the proportional sign $\propto$, often used in 
  % contexts of optimization and Bayesian statistics, means: we keep 
  % only expressions impacted by our parameter of interest because they suffice 
  % to yield the intended results or show some property of interest.
  % 
  % From this we can easily see the correspondence between MLE and ERM:
  % the $L2$ loss is proportional to the negative log-likelihood and hence, the 
  % $\argmax$ of the likelihood (using the assumption of normally distributed 
  % errors) and the $\argmin$ of the risk (using $L2$ loss) are equivalent.
  
  \item In order to find the optimal $\thetabh$, we need to solve the following 
  minimization problem: 
  
  \begin{flalign*}
    \thetabh = \argmin_{\thetab \in \Theta} \risket &= \argmin_{\thetab \in \Theta} 
    \left( \sumin \left(\yi - \thetab^\top \xi \right)^2 \right) 
    % &= \argmin_{\thetab \in \Theta} \| \yv - \Xmat \thetab \|_2^2
  \end{flalign*}  

  This is achieved in the usual manner of setting the derivative w.r.t. 
  $\thetab$ to 0 and solving for $\thetab$, yielding the familiar least-squares 
  estimator. %$\thetabh = (\Xmat^\top \Xmat)^{-1} \Xmat^\top \yv$.

\end{enumerate}
}

\dlz

\aufgabe{Credit Scoring Project}{

Imagine you work at a bank and have the job to develop a credit scoring model. This means, your model should predict whether a customer applying for a credit will be able to pay it back in the end.

\begin{enumerate}[a)]

\item Is this a supervised or unsupervised learning problem? Justify your answer.
\item How would you set up your data? Which is the target variable, what feature variables could you think of? Do you need labeled or unlabeled data? Justify all answers.
\item Is this a regression or classification task? Justify your answer.
\item Is this "learning to predict" or "learning to explain"? Justify your answer.
\item In classical statistics, you could use e.g. the logit model for this task. This means we assume that the targets are conditionally independent given the features, so $\yi|\xi \ind \yi[j]|\xi[j]$ for all $i,j = 1, \dots, n, i \ne j$, where $n$ is the sample size. We further assume that $\yi|\xi \distas{} Bin(\pi^{(i)})$, where $\pi^{(i)} = \frac{\exp(\thetab^\top\xi)}{1+\exp(\thetab^\top\xi)}$.
Looking at this from a Machine Learning perspective, write down the hypothesis space for this model. State explicitly which parameters have to be learned.
\item In classical statistics, you would estimate the parameters via Maximum Likelihood estimation. (The log-Likelihood of the Logit-Model is: $\sumin\yi\log(\pi^{(i)}) + (1-\yi)(\log(1-\pi^{(i)}))$). How could you use the model assumptions to define a reasonable loss function? Write it down explicitly.
\item Now you have to optimize this risk function to find the best parameters and hence the best model. Describe with a few sentences, how you could do this.

\end{enumerate}

Congratulations, you just designed your first Machine Learning project!

}
\newpage
\loesung{


\begin{enumerate}[a)]
  \item Supervised learning problem - the model will be learned from historical
  credit data for which payment history has been observed (knowing the ground
  truth is vital here since we need to evaluate our model's accuracy)
  \item Target variable: classes (default y/n), continuous credit
  scores, or class probabilities). Potential features: monthly income, current 
  level of indebtedness, past credit behavior, profession, residential 
  environment, age, number of kids etc. Labels: yes, since we have a supervised 
  learning problem.
  \item This is a classification problem - we want to assign our customers to
  classes \emph{default} and \emph{non-default}.
  \item (Primarily) learning to predict - we want to score future borrowers.
  \item $\Hspace = \{\pi: \Xspace \mapsto [0,1] ~|~ \pixt = s(\thetab^T \xv), \thetab \in \R^d\}$,
  where $s(z) = 1 / (1 + exp(-z))$ is the sigmoid function. Parameters to be
  learned: $\thetab$.
  \item We know that, in the optimum, (log-)likelihood is maximal. We can
  directly translate this into risk minimization by using the \emph{negative}
  log-likelihood as our empirical risk. We will just use the pointwise negative
  log-likelihood as our loss function: $$\Lpixyit = - \left(\yi \log \left(
  \pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit \right) 
  \right) \right)$$ (the so-called \emph{Bernoulli loss}). The
  empirical risk is then the sum of point-wise losses: $$\risket = -\sumin \yi
  \log \left(\pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit
  \right) \right)$$
  \item We can now solve this optimization problem via empirical risk
  minimization, which, in this case, is perfectly equivalent to ML estimation.
  Therefore, we set the first derivative of $\risket$ wrt $\thetab$ to 0 and
  solve for $\thetab$. However -- unlike linear regression -- this has no
  closed-form solution, so a numerical optimization procedure such as gradient
  descent is required.
\end{enumerate}
}

\aufgabe{Own Use Case}{

Think of a practical use case of your daily work where you would like to apply machine learning methods.

\begin{enumerate}[a)]
\item Describe your use case in a way that allows non-experts of your field to understand the main idea.
\item Is this a supervised or unsupervised learning problem? Justify your answer. (Try to think of a supervised problem to answer the next questions.)
\item How would you set up your data? Which is the target variable, what feature variables could you think of? Do you need labeled or unlabeled data? Justify all answers.
\item Is this a regression or classification task? Justify your answer.
\item Is this 'learning to predict' or 'learning to explain'? Justify your answer.
\item Which important properties should the ML algorithm fulfill for your use case? (E.g., accurate predictions, fast predictions, interpretability, scalability to large amounts of data, fast re-training with new data, ..)

\end{enumerate}
}
\dlz
\loesung{
No model solution
}

\newpage

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

\aufgabeexam{WS2020/21}{second}{1}{

\begin{tabular}{ | c | c | c |}
\hline
ID  &  $\xv$  &  $y$  \\  \hline
1   &  0.0    &  0.0  \\
2   &  1.0    &  4.0  \\
3   &  1.5    &  5.5  \\
4   &  2.5    &  7.0  \\
5   &  3.5    &  6.0  \\
6   &  5.0    &  3.0  \\
7   &  6.0    &  2.0  \\
8   &  7.0    &  3.0  \\
9   &  8.0    &  8.0  \\
\hline
\end{tabular}

\begin{centering}



\end{centering}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-13-1} 
\end{knitrout}

Now we want to train a cubic polynomial, i.e., a polynomial regression 
model with degree $d = 3$ on the data used in a). 
\begin{enumerate}
  \item[(i)] Define the hypothesis space of this model and state explicitly 
  how many parameters have to be estimated for training the model.
  \item[(ii)] Define the minimization problem that we have to optimize in 
  order to train the polynomial regression model. Use L2 loss and be as 
  explicit as possible - without plugging in the data.  
  \item[(iii)] In order to estimate the parameters of the model, it is 
  convenient to describe the model as a linear model. Compute the respective 
  design matrix using the concrete values of $\xv$ given above. Additionally, 
  state a formula for estimating the parameters using this design matrix. 
  (You do not have to derive this formula.)
\end{enumerate}
}

\newpage

\loesung{
\begin{enumerate}
  \item[(i)] $$\Hspace = \{f: \fx = \theta_0 + \theta_1 \xv + \theta_2 \xv^2 + 
  \theta_3 \xv^3 ~|~ \theta_0, \theta_1, \theta_2, \theta_3 \in \R\}$$
  The four parameters $\theta_0, \theta_1, \theta_2, \theta_3$ have to be 
  estimated
  \item[(ii)] $$\thetabh \in \argmin \limits_{\thetab \in \Theta} \risket$$ 
  This means we have to optimize the following minimization problem wrt 
  $\thetab = (\theta_0, \theta_1, \theta_2, \theta_3)$:
  $$\min \limits_{\thetab \in \Theta} \risket = \min \limits_{\thetab \in 
  \Theta} \sum_{i=1}^9(\yi - (\theta_0 + \theta_1 \xi + \theta_2 (\xi)^2 + 
  \theta_3 (\xi)^3))^2$$ 
  \item[(iii)]   $$\thetah = (X^\top X )^{-1}X^\top y$$
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{r|r|r|r}
\hline
X1 & x & x.2 & x.3\\
\hline
1 & 0.0 & 0.00 & 0.000\\
\hline
1 & 1.0 & 1.00 & 1.000\\
\hline
1 & 1.5 & 2.25 & 3.375\\
\hline
1 & 2.5 & 6.25 & 15.625\\
\hline
1 & 3.5 & 12.25 & 42.875\\
\hline
1 & 5.0 & 25.00 & 125.000\\
\hline
1 & 6.0 & 36.00 & 216.000\\
\hline
1 & 7.0 & 49.00 & 343.000\\
\hline
1 & 8.0 & 64.00 & 512.000\\
\hline
\end{tabular}

\end{knitrout}
\end{enumerate}
}

% ------------------------------------------------------------------------------

\dlz
\aufgabeexam{WS2020/21}{second}{6}{

Describe a real-life application in which classification might be useful and 
where we want to “learn to explain”. Describe the response, as well as the 
predictors. Explain your answer thoroughly.
}

\dlz
\loesung{

No model solution
}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo
\end{document}
