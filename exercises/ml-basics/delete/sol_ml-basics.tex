\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopficsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfaml}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Advanced Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}


\newcommand{\kopfdive}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Deep Dive\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}

\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{1}{ML Basics}

\loesung{Car Price Prediction}{

\begin{enumerate}[a)]

  \item We face a \textbf{supervised regression} task: we definitely need 
  labeled training data to infer a relationship between cars' attributes and 
  their prices, and price in EUR is a continuous target (or quasi-continuous, 
  to be exact -- as with all other quantities, we can only measure it with 
  finite precision, but the scale is sufficiently fine-grained to assume 
  continuity). \textbf{Prediction} is definitely the goal here, however, it 
  might also be interesting to examine the explanatory contribution of each 
  feature.
  
  \item Target variable and potential features: \\
  
  \begin{tabular}{l|l|l}
    \textbf{Variable} & \textbf{Role} & \textbf{Data type} \\ \hline
    Price in EUR & Target & Numeric \\ \hline
    Age in days & Feature & Numeric \\ \hline
    Mileage in km & Feature & Numeric \\ \hline
    Brand & Feature & Categorical \\ \hline
    Accident-free y/n & Feature & Binary \\ \hline
    \dots & \dots & \dots
  \end{tabular}
  
  \item Let $x_1$ and $x_2$ measure age and mileage, respectively. 
  Both features and target are numeric and (quasi-) continuous. It is also 
  reasonable to assume non-negativity for the features, such that we 
  obtain $\Xspace = (\R_{0}^{+})^2$, with $\xi = (x_1^{(i)}, x_2^{(i)})^\top 
  \in \Xspace$ for $i = 1, 2, \dots, n$ observations. 
  As the standard LM does not impose any 
  restrictions on the target, we have $\Yspace = \R$, though we would probably 
  discard negative predictions in practice.
  
  \item We can write the hypothesis space as:
  
  \begin{flalign*}
    \Hspace = \{\fxt = \thetab^\top \xv ~|~ \thetab \in \R^3 \}
    =  \{\fxt = \theta_0 + \theta_1 x_1 + \theta_2 x_2 ~|~ 
    (\theta_0, \theta_1, \theta_2) \in \R^3 \}.
    % \Hspace &= \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \thetab^\top \xv, ~ (\theta_0, \thetab) \in \R^3 \} \\
    % &=  \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \theta_{\text{age}} x_{\text{age}} + 
    % \theta_{\text{mileage}} x_{\text{mileage}}, ~ (\theta_0, 
    % \theta_{\text{age}}, \theta_{\text{mileage}}) \in \R^3 \},
  \end{flalign*}
  
  Note the \textbf{slight abuse of notation} here: in the lecture, we first 
  define $\thetab$ to only consist of the feature coefficients, with $\xv$ 
  likewise being the plain feature vector. For the sake of simplicity, however, 
  it is more convenient to append the intercept coefficient to the vector of 
  feature coefficients. This does not change our model formulation, but we have 
  to keep in mind that it implicitly entails adding an element 1 at the first 
  position of each feature vector.
  % , i.e., $\xi := (1, x_1, x_2)^{(i)} \in 
  % \{1\} \cup \Xspace$, constituting the familiar column of ones in the design 
  % matrix $\Xmat$.
  
  \item The parameter space is included in the definition of the hypothesis 
  space and in this case given by $\Theta = \R^3$.
  
  \item Loss function for the $i$-th observation: $\Lxyit = \left( \yi - 
  \thetab^\top \xi \right)^2$.
  
  % \item The first thing to note is that both MLE and ERM are 
  % \textbf{optimization problems}, and both should lead us to the same optimum. 
  % Their opposite signs are not a problem: maximizing the likelihood is 
  % equivalent to minimizing the negative likelihood. 
  % Also, both are defined pointwise.
  % The last thing to fix is therefore the product introduced by the independence 
  % assumption in the joint likelihood of all observations (recall that we use 
  % a \textit{summed} loss in ERM), for which the logarithm is a natural remedy.
  % We can thus simply use the \textbf{negative log-likelihood (NLL)} as our loss 
  % function (and indeed, many known loss functions can be shown to correspond to 
  % certain model likelihoods).
  % 
  % Let's put these reflections to practice:
  % 
  % \begin{flalign*}
  %   L_{NLL}\left (\yi, f\left( \xi | \thetab \right) \right) 
  %   &= - \log \LL(\thetab | \xi) \\
  %   &= - \ell(\thetab | \xi) \\
  %   &= - \log \left(\frac{1}{\sqrt{2 \pi 
  %   \sigma^2}} \exp \left(- \frac{1}{2 \sigma^2} \left( \yi - \thetab^\top \xi  
  %   \right)^2  \right) \right) \\
  %   &= - \left(\log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) + 
  %   \log \left( \exp \left(- \frac{1}{2 \sigma^2} \left( \yi - \thetab^\top \xi  
  %   \right)^2  \right) \right) \right)  \\
  %   &= - \left(- \frac{1}{2}\log(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \left( 
  %   \yi - \thetab^\top \xi \right)^2 \right) \\
  %   &= \frac{1}{2}\log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left( 
  %   \yi - \thetab^\top \xi \right)^2
  % \end{flalign*}
  % 
  % \begin{flalign*}
  %   \risket &= \sumin - \ell(\thetab | \xi) \\
  %   &= \sumin L_{NLL}\left (\yi, f\left( \xi | \thetab \right) \right)  \\
  %   &= \sumin \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} 
  %   \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &= \frac{n}{2}\log(2 \pi \sigma^2) +  \frac{1}{2 \sigma^2} \sumin
  %   \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &\propto \sumin \left(\yi - \thetab^\top \xi \right)^2 \\
  %   &= \sumin \Lxyit ~ (L2 \text{ loss})
  % \end{flalign*}  
  % 
  % As we are only interested in the feature coefficients here, we neglect all 
  % irrelevant terms that do not depend on $\thetab$ as they have no effect on
  % the solution (i.e., the $\argmin$ of $\risket$).
  % This is what the proportional sign $\propto$, often used in 
  % contexts of optimization and Bayesian statistics, means: we keep 
  % only expressions impacted by our parameter of interest because they suffice 
  % to yield the intended results or show some property of interest.
  % 
  % From this we can easily see the correspondence between MLE and ERM:
  % the $L2$ loss is proportional to the negative log-likelihood and hence, the 
  % $\argmax$ of the likelihood (using the assumption of normally distributed 
  % errors) and the $\argmin$ of the risk (using $L2$ loss) are equivalent.
  
  \item In order to find the optimal $\thetabh$, we need to solve the following 
  minimization problem: 
  
  \begin{flalign*}
    \thetabh = \argmin_{\thetab \in \Theta} \risket &= \argmin_{\thetab \in \Theta} 
    \left( \sumin \left(\yi - \thetab^\top \xi \right)^2 \right) 
    % &= \argmin_{\thetab \in \Theta} \| \yv - \Xmat \thetab \|_2^2
  \end{flalign*}  

  This is achieved in the usual manner of setting the derivative w.r.t. 
  $\thetab$ to 0 and solving for $\thetab$, yielding the familiar least-squares 
  estimator. %$\thetabh = (\Xmat^\top \Xmat)^{-1} \Xmat^\top \yv$.

\end{enumerate}
}

\dlz

\loesung{Vector Calculus}{

\begin{enumerate}[a)]
    \item In computing $\Amat \xv$ we multiply each of the $m$ rows in $\Amat$ with the sole length-$n$ column in $\xv$, leaving us with a column vector $\fx \in \R^{m \times 1}$. Thus, we have $f: \R^{n (\times 1)} \rightarrow \R^{m (\times 1)}$.
     
    The $i$-th function component $f_i(\xv)$ corresponds to multiplying the $i$-th row of $\Amat$ with $\xv$, amounting to $$f_i(\xv) = \sum_{j = 1}^n a_{ij} x_j,$$ with $a_{ij}$ the element in the $i$-row, $j$-th column of $\Amat$.
    
    \includegraphics[width=0.5\textwidth]{figure/illustration_matmult}
    
    \scriptsize{\url{https://mbernste.github.io/posts/matrix_vector_mult/}} \normalsize
    \item \phantom{foo}
    \begin{enumerate}[i)]
         \item The gradient is the row vector\footnote{
         Pertaining to one of two conventions; we use the \textit{numerator layout} here (the transposed version is called \textit{denominator layout}).
         } of partial derivatives, i.e., the derivatives of $f$ w.r.t. each dimension of $\xv$:
         $$\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} 
         = \mat{\pd{\fx}{x_1} & \dots & \pd{\fx}{x_n}}.
         $$
         Now, since $f$ is a vector-valued function, each component is itself a vector of length $m$.
         Therefore, we have $\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} \in \R^{m \times n}$, given by the collection of all partial derivatives of each function component:
         $$
         \frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} = \mat{
         \pd{f_1(\xv)}{x_1} & \cdots & \pd{f_1(\xv)}{x_n} \\
         \vdots & & \vdots \\
         \pd{f_m(\xv)}{x_1} & \cdots & \pd{f_m(\xv)}{x_n}
         }
         $$
         This matrix is also called the \emph{Jacobian} of $f$.
         \item We have $$\pd{f_i(\xv)}{x_j} = \pd{\left(\sum_{j = 1}^n a_{ij} x_j \right)}{x_j} = a_{ij}.$$
         Doing this for every element yields
         $$
         \mat{a_{11} & \cdots & a_{1n}  \\ \vdots & & \vdots \\ a_{m1} & \cdots & a_{mn}},
         $$
         and we have $\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} = \frac{\mathop{}\!\mathrm{d} \Amat \xv}{\mathop{}\!\mathrm{d} \xv} = \Amat$.
    \end{enumerate}
\end{enumerate}

For more explanations and exercises, including a useful collection of rules for calculus, we recommend the book "Mathematics for Machine Learning" (\url{https://mml-book.github.io/book/mml-book.pdf}).
}
\end{document}
