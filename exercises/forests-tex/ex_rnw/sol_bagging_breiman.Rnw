\begin{itemize}
    \item Let's first think about the expectation -- what is the stochastic part we integrate over? Since the base learner and ensemble structure is fixed for given data, the only thing random is the bootstrap sample we use as training data for the $m$-th tree, given that the total available training data originate from the data-generating process $\Pxy$, so we could write $\E_{\Dtrain^{[m]} ~|~ \Dtrain \sim \Pxy}$ (for ease of notation, we'll omit the subscripts of the expectation from now on).
    \item Next, consider the quantity we are supposed to compare to another quantity: $\E \left( \left(y - \blx \right)^2 \right)$. This looks already a lot like the $\E(Z^2)$ part of the LOTV.
    \item Since the expectation is linear, it is often worthwhile to decompose expressions of expectation and isolate the stochastic parts. Here, the only thing that changes w.r.t. $\Dtrain^{[m]}$ is the base learner models $\blx$:
    \begin{align*}
\E \left( \left(y - \blx \right)^2\right) 
&= \E \left(y^2 - 2y \blx + \left(\blx\right)^2 \right) \\
&= y^2 - 2y \E \left(\blx \right) + \E \left( \left( 
\blx \right)^2 \right),
\end{align*}
    \item Now it's time to realize two things. First: we want to arrive at $\left( y - \E \left(\blx \right) \right)^2 $ in order to be able to use the $(\E(Z))^2$ part of the LOTV. Second: the RHS of the above expression looks almost like a binomial formula -- we just need to move the square in the last term:
        \begin{align*}
y^2 - 2y \E \left(\blx \right)  + \left( \E 
\left(\blx \right) \right)^2  &= \left( y - \E \left(\blx \right) \right)^2 
\end{align*}
    \item Combine the two parts using the LOTV:    
    \begin{align*}
\E \left( \left(y - \blx \right)^2\right) &\geq \left( y - \E \left(\blx \right) \right)^2 .
\end{align*}
    \item The last missing step is to show that $ \E \left(\blx \right) = \fM$. To compute the expectation for this discrete random variable (we have a finite ensemble), we sum over all possible realizations, weighted by their probability of occurence.
    This can be further simplified given that all of the $M$ bootstrap samples were drawn with equal probability:
    \begin{align*}
    \E \left(\blx \right) &= \sum_{m = 1}^M \blx p \left(\Dtrain^{[m]} \right) \\
    &= \sum_{m = 1}^M \blx \cdot \frac{1}{M}  \\
    &= \frac{1}{M} \sum_{m = 1}^M \blx,
    \end{align*}
    which is precisely the ensemble prediction $\fM$.
    \item Putting everything together, we get 
    \begin{align*}
\E \left( \left(y - \blx \right)^2\right) &\geq \left( y - \left(\fM \right) \right)^2,
\end{align*}
    showing that the expected quadratic loss over individual base learner predictions is at least as large as the expected loss of the ensemble prediction.
\end{itemize}

\iffalse

Let's take a look at the average loss of individual base learner predictions. 
Since we are in the (theoretical) case of an infinitely large ensemble, the 
average contains an infinite sum, which can equally be viewed as the expectation 
over $\mathcal{M}$:
\begin{align*}
\E_{\mathcal{M}} \left( \left(y - \blx \right)^2\right) 
&= \E_{\mathcal{M}} \left(y^2 - 2y \blx + \left(\blx\right)^2 \right) \\
&= y^2 - 2y \E_{\mathcal{M}} \left(\blx \right) + \E_{\mathcal{M}} \left( \left( 
\blx \right)^2 \right),
\end{align*}
where we use the linearity of the expectation.

Note that the average base learner prediction is simply the prediction of 
the ensemble: $\E_{\mathcal{M}} (\blx) = \fM$.
Plugging this into the above equation and using the definition of the variance 
of a random variable $Z$ ("Verschiebungssatz"\footnote{
$\var(Z) = \E(Z^2) - (\E(Z))^2 ~ \Longleftrightarrow ~ \E(Z^2) = \var(Z) + 
(\E(Z))^2$, where $\var(Z) \geq 0$ by definition.
}), which tells us that $\E(Z^2) \geq (\E(Z))^2$, we obtain: 

\begin{align*}
\E_{\mathcal{M}} \left( \left(y - \blx \right)^2\right) &\geq 
y^2 - 2y \E_{\mathcal{M}} \left(\blx \right)  + \left( \E_{\mathcal{M}} 
\left(\blx \right) \right)^2 \\
&= \left( y - \E_{\mathcal{M}} \left(\blx \right) \right)^2 \\
&= \left( y - \fM \right)^2.
\end{align*}

The ensemble loss is thus less than or equal to the average loss of individual 
base learners.
How "sharp" this inequality is depends on how unequal both sides of 
$$\E_{\mathcal{M}} \left( \left( \blx \right)^2 \right)  \geq 
\left( \E_{\mathcal{M}} \left(\blx \right) \right)^2
$$ 
are, determined via the definition of variance by $\var \left( \blx \right)$.
In other words: the more instable the base learners (high variance), the more 
beneficial the ensembling procedure.

\fi
