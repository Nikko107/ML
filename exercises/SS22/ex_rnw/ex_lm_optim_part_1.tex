


Consider the regression learning setting, i.e., where the target space $\mathcal{Y}$ is $\mathbb{R}$
and assume that your loss function of interest is $L(y,f(\bm{x}))= 0.5 (m(y)-m(f(\bm{x})))^2,$ where $m:\mathbb{R} \to \mathbb{R}$ is a continuous strictly monotone function.
%


\begin{enumerate}
	%
	\item Consider the hypothesis space of constant models 
	%
	$\mathcal{H} = \{ f:\mathcal{X} \to \mathbb{R} \, | \,  f(\bm{x}) = \bm{\theta}  \ \forall \bm{x} \in \mathcal{X}  \}.$ 
	%
	Show that 
	%
	$$\hat{f}(\bm{x}) = m^{-1}\big(  \frac1n \sum_{i=1}^n m(\bm{x}^{(i)})  \big)$$
	%
	is the optimal constant model.
	%
	\item 
	%
	\item Given $f \in \mathcal{H}$, explain the different parts of the Bayes regret if (i) $f^\ast \in \mathcal{H}$; if (ii) $f^\ast \notin \mathcal{H}$.
	%
	\item Define the empirical risk and derive the gradients of the empirical risk.
	%
	\item Show that the empirical risk is convex in the model coefficients. Why is convexity a desirable property? Hint: Compute the Hessian matrix $\bm{H} \in \mathbb{R}^{p \times p}$ and show that $\bm{z}^\top \bm{H} \bm{z} \geq 0 \, \forall \bm{z} \in \mathbb{R}^p$, i.e., show that the Hessian is positive semi-definite (psd).  
\end{enumerate}
