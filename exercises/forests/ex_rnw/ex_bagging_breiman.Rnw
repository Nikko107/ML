In this exercise, we briefly revisit why bagging is a useful technique to 
stabilize predictions.

For a fixed observation $(\xv, y)$, show that the expected quadratic loss over individual base learner predictions $\blx$ is larger than or equal to the expected quadratic loss of the prediction $\fM$ of a size-$M$ ensemble ($M \in \N$).

You can consider all hyperparameters of the base learners and the ensemble fixed.
\\

\textit{Hint:} Use the law of total expectation ("Verschiebungssatz der Varianz"\footnote{
$\var(Z) = \E(Z^2) - (\E(Z))^2 ~ \Longleftrightarrow ~ \E(Z^2) = \var(Z) + 
(\E(Z))^2$, where $\var(Z) \geq 0$ by definition.
}) stating $\E(Z^2) \geq (\E(Z))^2$ for a random variable $Z$.
