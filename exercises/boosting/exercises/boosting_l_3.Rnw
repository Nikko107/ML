We have seen in the lecture that boosting with linear models and $L2$ loss simply approaches the closed-form OLS solution with a speed determined by the learning rate $\beta$.

\begin{itemize}
  \item This is perfectly equivalent to fitting an LM via \textbf{gradient descent}, as a gradient step in parameter space is a gradient step in function space for this specific case.
  \item Recall the parameter update for GD with learning rate $\beta$:
  $$
  \thetab^{[m+1]} \leftarrow \thetam - \beta \cdot \nabla_{\thetam}
  \riske(\thetam) = \thetam + \beta (-\Xmat^T \ydat + \Xmat^T\Xmat \thetam).
  $$
\end{itemize}

Compute the update for a boosted LM with current model $\Xmat \thetam$ (note that adding a linear base learner to an LM is equivalent to summing parameters):
  \begin{eqnarray*}
    \footnotesize
    \frac{\partial}{\partial \thetab^{[m+1]}}
    \left \| (\ydat - \Xmat \thetam) - \Xmat \thetab^{[m+1]}) \right \|^2_2
    &=& 0 \\
    -2\Xmat^T (\ydat - \Xmat \thetam) + 2\Xmat^T\Xmat
    \thetab^{[m+1]} &=& 0 \\
    \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T
    (\ydat - \Xmat \thetam) \\
    \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T \ydat
    - (\Xmat^T\Xmat)^{-1} \Xmat^T \Xmat \thetam \\
    \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T \ydat
    - \thetam % \quad \quad \textcolor{gray}{\rvert \cdot (-\Xmat^T\Xmat)}
    \\
    \thetab^{[m+1]} &=& - \Xmat^T \ydat + \Xmat^T\Xmat
    \thetam.
  \end{eqnarray*}
  $\Rightarrow \fh^{[m+1]} = \Xmat \tilde \thetab^{[m+1]} =
  \Xmat \left( \thetam + \beta (-\Xmat^T \ydat +
  \Xmat^T\Xmat \thetam) \right).$
