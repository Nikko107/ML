\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    
    \usepackage{bbm}
    % basic latex stuff
    \newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} %fontstyle for R packages
    \newcommand{\lz}{\vspace{0.5cm}} %vertical space
    \newcommand{\dlz}{\vspace{1cm}} %double vertical space
    \newcommand{\oneliner}[1] % Oneliner for important statements
    {\begin{block}{}\begin{center}\begin{Large}#1\end{Large}\end{center}\end{block}}
    
    
    %new environments
    \newenvironment{vbframe}  %frame with breaks and verbatim
    {
    	\begin{frame}[containsverbatim,allowframebreaks]
    	}
    	{
    	\end{frame}
    }
    
    \newenvironment{vframe}  %frame with verbatim without breaks (to avoid numbering one slided frames)
    {
    	\begin{frame}[containsverbatim]
    	}
    	{
    	\end{frame}
    }
    
    \newenvironment{blocki}[1]   % itemize block
    {
    	\begin{block}{#1}\begin{itemize}
    		}
    		{
    	\end{itemize}\end{block}
    }
    
    \newenvironment{fragileframe}[2]{  %fragile frame with framebreaks
    	\begin{frame}[allowframebreaks, fragile, environment = fragileframe]
    		\frametitle{#1}
    		#2}
    	{\end{frame}}
    
    
    \newcommand{\myframe}[2]{  %short for frame with framebreaks
    	\begin{frame}[allowframebreaks]
    		\frametitle{#1}
    		#2
    \end{frame}}
    
    \newcommand{\remark}[1]{
    	\textbf{Remark:} #1
    }
    
    
    \newenvironment{deleteframe}
    {
    	\begingroup
    	\usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight]{../style/color/red.png}}
    	\begin{frame}
    	}
    	{
    	\end{frame}
    	\endgroup
    }
    \newenvironment{simplifyframe}
    {
    	\begingroup
    	\usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight]{../style/color/yellow.png}}
    	\begin{frame}
    	}
    	{
    	\end{frame}
    	\endgroup
    }\newenvironment{draftframe}
    {
    	\begingroup
    	\usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight]{../style/color/green.jpg}}
    	\begin{frame}
    	}
    	{
    	\end{frame}
    	\endgroup
    }
    % https://tex.stackexchange.com/a/261480: textcolor that works in mathmode
    \makeatletter
    \renewcommand*{\@textcolor}[3]{%
    	\protect\leavevmode
    	\begingroup
    	\color#1{#2}#3%
    	\endgroup
    }
    \makeatother
    
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Ex\_Classification\_2}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    % math spaces
    \ifdefined\N                                                                
    \renewcommand{\N}{\mathds{N}} % N, naturals
    \else \newcommand{\N}{\mathds{N}} \fi 
    \newcommand{\Z}{\mathds{Z}} % Z, integers
    \newcommand{\Q}{\mathds{Q}} % Q, rationals
    \newcommand{\R}{\mathds{R}} % R, reals
    \ifdefined\C 
    \renewcommand{\C}{\mathds{C}} % C, complex
    \else \newcommand{\C}{\mathds{C}} \fi
    \newcommand{\continuous}{\mathcal{C}} % C, space of continuous functions
    \newcommand{\M}{\mathcal{M}} % machine numbers
    \newcommand{\epsm}{\epsilon_m} % maximum error
    
    % counting / finite sets
    \newcommand{\setzo}{\{0, 1\}} % set 0, 1
    \newcommand{\setmp}{\{-1, +1\}} % set -1, 1
    \newcommand{\unitint}{[0, 1]} % unit interval
    
    % basic math stuff
    \newcommand{\xt}{\tilde x} % x tilde
    \newcommand{\argmax}{\operatorname{arg\,max}} % argmax
    \newcommand{\argmin}{\operatorname{arg\,min}} % argmin
    \newcommand{\argminlim}{\mathop{\mathrm{arg\,min}}\limits} % argmax with limits
    \newcommand{\argmaxlim}{\mathop{\mathrm{arg\,max}}\limits} % argmin with limits  
    \newcommand{\sign}{\operatorname{sign}} % sign, signum
    \newcommand{\I}{\mathbb{I}} % I, indicator
    \newcommand{\order}{\mathcal{O}} % O, order
    \newcommand{\pd}[2]{\frac{\partial{#1}}{\partial #2}} % partial derivative
    \newcommand{\floorlr}[1]{\left\lfloor #1 \right\rfloor} % floor
    \newcommand{\ceillr}[1]{\left\lceil #1 \right\rceil} % ceiling
    
    % sums and products
    \newcommand{\sumin}{\sum\limits_{i=1}^n} % summation from i=1 to n
    \newcommand{\sumim}{\sum\limits_{i=1}^m} % summation from i=1 to m
    \newcommand{\sumjn}{\sum\limits_{j=1}^n} % summation from j=1 to p
    \newcommand{\sumjp}{\sum\limits_{j=1}^p} % summation from j=1 to p
    \newcommand{\sumik}{\sum\limits_{i=1}^k} % summation from i=1 to k
    \newcommand{\sumkg}{\sum\limits_{k=1}^g} % summation from k=1 to g
    \newcommand{\sumjg}{\sum\limits_{j=1}^g} % summation from j=1 to g
    \newcommand{\meanin}{\frac{1}{n} \sum\limits_{i=1}^n} % mean from i=1 to n
    \newcommand{\meanim}{\frac{1}{m} \sum\limits_{i=1}^m} % mean from i=1 to n
    \newcommand{\meankg}{\frac{1}{g} \sum\limits_{k=1}^g} % mean from k=1 to g
    \newcommand{\prodin}{\prod\limits_{i=1}^n} % product from i=1 to n
    \newcommand{\prodkg}{\prod\limits_{k=1}^g} % product from k=1 to g
    \newcommand{\prodjp}{\prod\limits_{j=1}^p} % product from j=1 to p
    
    % linear algebra
    \newcommand{\one}{\boldsymbol{1}} % 1, unitvector
    \newcommand{\zero}{\mathbf{0}} % 0-vector
    \newcommand{\id}{\boldsymbol{I}} % I, identity
    \newcommand{\diag}{\operatorname{diag}} % diag, diagonal
    \newcommand{\trace}{\operatorname{tr}} % tr, trace
    \newcommand{\spn}{\operatorname{span}} % span
    \newcommand{\scp}[2]{\left\langle #1, #2 \right\rangle} % <.,.>, scalarproduct
    \newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}} % short pmatrix command
    \newcommand{\Amat}{\mathbf{A}} % matrix A
    \newcommand{\Deltab}{\mathbf{\Delta}} % error term for vectors
    
    % basic probability + stats
    \renewcommand{\P}{\mathds{P}} % P, probability
    \newcommand{\E}{\mathds{E}} % E, expectation
    \newcommand{\var}{\mathsf{Var}} % Var, variance
    \newcommand{\cov}{\mathsf{Cov}} % Cov, covariance
    \newcommand{\corr}{\mathsf{Corr}} % Corr, correlation
    \newcommand{\normal}{\mathcal{N}} % N of the normal distribution
    \newcommand{\iid}{\overset{i.i.d}{\sim}} % dist with i.i.d superscript
    \newcommand{\distas}[1]{\overset{#1}{\sim}} % ... is distributed as ...
    
    
    % machine learning
    \newcommand{\Xspace}{\mathcal{X}} % X, input space
    \newcommand{\Yspace}{\mathcal{Y}} % Y, output space
    \newcommand{\nset}{\{1, \ldots, n\}} % set from 1 to n
    \newcommand{\pset}{\{1, \ldots, p\}} % set from 1 to p
    \newcommand{\gset}{\{1, \ldots, g\}} % set from 1 to g
    \newcommand{\Pxy}{\mathbb{P}_{xy}} % P_xy
    \newcommand{\Exy}{\mathbb{E}_{xy}} % E_xy: Expectation over random variables xy
    \newcommand{\xv}{\mathbf{x}} % vector x (bold)
    \newcommand{\xtil}{\tilde{\mathbf{x}}} % vector x-tilde (bold)
    \newcommand{\yv}{\mathbf{y}} % vector y (bold)
    \newcommand{\xy}{(\xv, y)} % observation (x, y)
    \newcommand{\xvec}{\left(x_1, \ldots, x_p\right)^\top} % (x1, ..., xp) 
    \newcommand{\Xmat}{\mathbf{X}} % Design matrix
    \newcommand{\allDatasets}{\mathds{D}} % The set of all datasets
    \newcommand{\allDatasetsn}{\mathds{D}_n}  % The set of all datasets of size n 
    \newcommand{\D}{\mathcal{D}} % D, data
    \newcommand{\Dn}{\D_n} % D_n, data of size n
    \newcommand{\Dtrain}{\mathcal{D}_{\text{train}}} % D_train, training set
    \newcommand{\Dtest}{\mathcal{D}_{\text{test}}} % D_test, test set
    \newcommand{\xyi}[1][i]{\left(\xv^{(#1)}, y^{(#1)}\right)} % (x^i, y^i), i-th observation
    \newcommand{\Dset}{\left( \xyi[1], \ldots, \xyi[n]\right)} % {(x1,y1)), ..., (xn,yn)}, data
    \newcommand{\defAllDatasetsn}{(\Xspace \times \Yspace)^n} % Def. of the set of all datasets of size n 
    \newcommand{\defAllDatasets}{\bigcup_{n \in \N}(\Xspace \times \Yspace)^n} % Def. of the set of all datasets 
    \newcommand{\xdat}{\left\{ \xv^{(1)}, \ldots, \xv^{(n)}\right\}} % {x1, ..., xn}, input data
    \newcommand{\yvec}{\left(y^{(1)}, \hdots, y^{(n)}\right)^\top} % (y1, ..., yn), vector of outcomes
    \renewcommand{\xi}[1][i]{\xv^{(#1)}} % x^i, i-th observed value of x
    \newcommand{\yi}[1][i]{y^{(#1)}} % y^i, i-th observed value of y 
    \newcommand{\xivec}{\left(x^{(i)}_1, \ldots, x^{(i)}_p\right)^\top} % (x1^i, ..., xp^i), i-th observation vector
    \newcommand{\xj}{\xv_j} % x_j, j-th feature
    \newcommand{\xjvec}{\left(x^{(1)}_j, \ldots, x^{(n)}_j\right)^\top} % (x^1_j, ..., x^n_j), j-th feature vector
    \newcommand{\phiv}{\mathbf{\phi}} % Basis transformation function phi
    \newcommand{\phixi}{\mathbf{\phi}^{(i)}} % Basis transformation of xi: phi^i := phi(xi)
    
    %%%%%% ml - models general
    \newcommand{\lamv}{\bm{\lambda}} % lambda vector, hyperconfiguration vector
    \newcommand{\Lam}{\bm{\Lambda}}	 % Lambda, space of all hpos
    % Inducer / Inducing algorithm
    \newcommand{\preimageInducer}{\left(\defAllDatasets\right)\times\Lam} % Set of all datasets times the hyperparameter space
    \newcommand{\preimageInducerShort}{\allDatasets\times\Lam} % Set of all datasets times the hyperparameter space
    % Inducer / Inducing algorithm
    \newcommand{\ind}{\mathcal{I}} % Inducer, inducing algorithm, learning algorithm 
    
    % continuous prediction function f
    \newcommand{\ftrue}{f_{\text{true}}}  % True underlying function (if a statistical model is assumed)
    \newcommand{\ftruex}{\ftrue(\xv)} % True underlying function (if a statistical model is assumed)
    \newcommand{\fx}{f(\xv)} % f(x), continuous prediction function
    \newcommand{\fdomains}{f: \Xspace \rightarrow \R^g} % f with domain and co-domain
    \newcommand{\Hspace}{\mathcal{H}} % hypothesis space where f is from
    \newcommand{\fbayes}{f^{\ast}} % Bayes-optimal model
    \newcommand{\fxbayes}{f^{\ast}(\xv)} % Bayes-optimal model
    \newcommand{\fkx}[1][k]{f_{#1}(\xv)} % f_j(x), discriminant component function
    \newcommand{\fh}{\hat{f}} % f hat, estimated prediction function
    \newcommand{\fxh}{\fh(\xv)} % fhat(x)
    \newcommand{\fxt}{f(\xv ~|~ \thetab)} % f(x | theta)
    \newcommand{\fxi}{f\left(\xv^{(i)}\right)} % f(x^(i))
    \newcommand{\fxih}{\hat{f}\left(\xv^{(i)}\right)} % f(x^(i))
    \newcommand{\fxit}{f\left(\xv^{(i)} ~|~ \thetab\right)} % f(x^(i) | theta)
    \newcommand{\fhD}{\fh_{\D}} % fhat_D, estimate of f based on D
    \newcommand{\fhDtrain}{\fh_{\Dtrain}} % fhat_Dtrain, estimate of f based on D
    \newcommand{\fhDnlam}{\fh_{\Dn, \lamv}} %model learned on Dn with hp lambda
    \newcommand{\fhDlam}{\fh_{\D, \lamv}} %model learned on D with hp lambda
    \newcommand{\fhDnlams}{\fh_{\Dn, \lamv^\ast}} %model learned on Dn with optimal hp lambda 
    \newcommand{\fhDlams}{\fh_{\D, \lamv^\ast}} %model learned on D with optimal hp lambda 
    
    % discrete prediction function h
    \newcommand{\hx}{h(\xv)} % h(x), discrete prediction function
    \newcommand{\hh}{\hat{h}} % h hat
    \newcommand{\hxh}{\hat{h}(\xv)} % hhat(x)
    \newcommand{\hxt}{h(\xv | \thetab)} % h(x | theta)
    \newcommand{\hxi}{h\left(\xi\right)} % h(x^(i))
    \newcommand{\hxit}{h\left(\xi ~|~ \thetab\right)} % h(x^(i) | theta)
    \newcommand{\hbayes}{h^{\ast}} % Bayes-optimal classification model
    \newcommand{\hxbayes}{h^{\ast}(\xv)} % Bayes-optimal classification model
    
    % yhat
    \newcommand{\yh}{\hat{y}} % yhat for prediction of target
    \newcommand{\yih}{\hat{y}^{(i)}} % yhat^(i) for prediction of ith targiet
    \newcommand{\resi}{\yi- \yih}
    
    % theta
    \newcommand{\thetah}{\hat{\theta}} % theta hat
    \newcommand{\thetab}{\bm{\theta}} % theta vector
    \newcommand{\thetabh}{\bm{\hat\theta}} % theta vector hat
    \newcommand{\thetat}[1][t]{\thetab^{[#1]}} % theta^[t] in optimization
    \newcommand{\thetatn}[1][t]{\thetab^{[#1 +1]}} % theta^[t+1] in optimization
    \newcommand{\thetahDnlam}{\thetabh_{\Dn, \lamv}} %theta learned on Dn with hp lambda
    \newcommand{\thetahDlam}{\thetabh_{\D, \lamv}} %theta learned on D with hp lambda
    \newcommand{\mint}{\min_{\thetab \in \Theta}} % min problem theta
    \newcommand{\argmint}{\argmin_{\thetab \in \Theta}} % argmin theta
    
    % densities + probabilities
    % pdf of x 
    \newcommand{\pdf}{p} % p
    \newcommand{\pdfx}{p(\xv)} % p(x)
    \newcommand{\pixt}{\pi(\xv~|~ \thetab)} % pi(x|theta), pdf of x given theta
    \newcommand{\pixit}{\pi\left(\xi ~|~ \thetab\right)} % pi(x^i|theta), pdf of x given theta
    \newcommand{\pixii}{\pi\left(\xi\right)} % pi(x^i), pdf of i-th x 
    
    % pdf of (x, y)
    \newcommand{\pdfxy}{p(\xv,y)} % p(x, y)
    \newcommand{\pdfxyt}{p(\xv, y ~|~ \thetab)} % p(x, y | theta)
    \newcommand{\pdfxyit}{p\left(\xi, \yi ~|~ \thetab\right)} % p(x^(i), y^(i) | theta)
    
    % pdf of x given y
    \newcommand{\pdfxyk}[1][k]{p(\xv | y= #1)} % p(x | y = k)
    \newcommand{\lpdfxyk}[1][k]{\log p(\xv | y= #1)} % log p(x | y = k)
    \newcommand{\pdfxiyk}[1][k]{p\left(\xi | y= #1 \right)} % p(x^i | y = k)
    
    % prior probabilities
    \newcommand{\pik}[1][k]{\pi_{#1}} % pi_k, prior
    \newcommand{\lpik}[1][k]{\log \pi_{#1}} % log pi_k, log of the prior
    \newcommand{\pit}{\pi(\thetab)} % Prior probability of parameter theta
    
    % posterior probabilities
    \newcommand{\post}{\P(y = 1 ~|~ \xv)} % P(y = 1 | x), post. prob for y=1
    \newcommand{\postk}[1][k]{\P(y = #1 ~|~ \xv)} % P(y = k | y), post. prob for y=k
    \newcommand{\pidomains}{\pi: \Xspace \rightarrow \unitint} % pi with domain and co-domain
    \newcommand{\pibayes}{\pi^{\ast}} % Bayes-optimal classification model
    \newcommand{\pixbayes}{\pi^{\ast}(\xv)} % Bayes-optimal classification model
    \newcommand{\pix}{\pi(\xv)} % pi(x), P(y = 1 | x)
    \newcommand{\pikx}[1][k]{\pi_{#1}(\xv)} % pi_k(x), P(y = k | x)
    \newcommand{\pikxt}[1][k]{\pi_{#1}(\xv ~|~ \thetab)} % pi_k(x | theta), P(y = k | x, theta)
    \newcommand{\pixh}{\hat \pi(\xv)} % pi(x) hat, P(y = 1 | x) hat
    \newcommand{\pikxh}[1][k]{\hat \pi_{#1}(\xv)} % pi_k(x) hat, P(y = k | x) hat
    \newcommand{\pixih}{\hat \pi(\xi)} % pi(x^(i)) with hat
    \newcommand{\pikxih}[1][k]{\hat \pi_{#1}(\xi)} % pi_k(x^(i)) with hat
    \newcommand{\pdfygxt}{p(y ~|~\xv, \thetab)} % p(y | x, theta)
    \newcommand{\pdfyigxit}{p\left(\yi ~|~\xi, \thetab\right)} % p(y^i |x^i, theta)
    \newcommand{\lpdfygxt}{\log \pdfygxt } % log p(y | x, theta)
    \newcommand{\lpdfyigxit}{\log \pdfyigxit} % log p(y^i |x^i, theta)
    
    % probababilistic
    \newcommand{\bayesrulek}[1][k]{\frac{\P(\xv | y= #1) \P(y= #1)}{\P(\xv)}} % Bayes rule
    \newcommand{\muk}{\bm{\mu_k}} % mean vector of class-k Gaussian (discr analysis) 
    
    % residual and margin
    \newcommand{\eps}{\epsilon} % residual, stochastic
    \newcommand{\epsi}{\epsilon^{(i)}} % epsilon^i, residual, stochastic
    \newcommand{\epsh}{\hat{\epsilon}} % residual, estimated
    \newcommand{\yf}{y \fx} % y f(x), margin
    \newcommand{\yfi}{\yi \fxi} % y^i f(x^i), margin
    \newcommand{\Sigmah}{\hat \Sigma} % estimated covariance matrix
    \newcommand{\Sigmahj}{\hat \Sigma_j} % estimated covariance matrix for the j-th class
    
    % ml - loss, risk, likelihood
    \newcommand{\Lyf}{L\left(y, f\right)} % L(y, f), loss function
    \newcommand{\Lxy}{L\left(y, \fx\right)} % L(y, f(x)), loss function
    \newcommand{\Lxyi}{L\left(\yi, \fxi\right)} % loss of observation
    \newcommand{\Lxyt}{L\left(y, \fxt\right)} % loss with f parameterized
    \newcommand{\Lxyit}{L\left(\yi, \fxit\right)} % loss of observation with f parameterized
    \newcommand{\Lxym}{L\left(\yi, f\left(\bm{\tilde{x}}^{(i)} ~|~ \thetab\right)\right)} % loss of observation with f parameterized
    \newcommand{\Lpixy}{L\left(y, \pix\right)} % loss in classification
    \newcommand{\Lpixyi}{L\left(\yi, \pixii\right)} % loss of observation in classification
    \newcommand{\Lpixyt}{L\left(y, \pixt\right)} % loss with pi parameterized
    \newcommand{\Lpixyit}{L\left(\yi, \pixit\right)} % loss of observation with pi parameterized
    \newcommand{\Lhxy}{L\left(y, \hx\right)} % L(y, h(x)), loss function on discrete classes
    \newcommand{\Lr}{L\left(r\right)} % L(r), loss defined on residual (reg) / margin (classif)
    \newcommand{\lone}{|y - \fx|} % L1 loss
    \newcommand{\ltwo}{\left(y - \fx\right)^2} % L2 loss
    \newcommand{\lbernoullimp}{\ln(1 + \exp(-y \cdot \fx))} % Bernoulli loss for -1, +1 encoding
    \newcommand{\lbernoullizo}{- y \cdot \fx + \log(1 + \exp(\fx))} % Bernoulli loss for 0, 1 encoding
    \newcommand{\lcrossent}{- y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)} % cross-entropy loss
    \newcommand{\lbrier}{\left(\pix - y \right)^2} % Brier score
    \newcommand{\risk}{\mathcal{R}} % R, risk
    \newcommand{\riskbayes}{\mathcal{R}^\ast}
    \newcommand{\riskf}{\risk(f)} % R(f), risk
    \newcommand{\riskdef}{\E_{y|\xv}\left(\Lxy \right)} % risk def (expected loss)
    \newcommand{\riskt}{\mathcal{R}(\thetab)} % R(theta), risk
    \newcommand{\riske}{\mathcal{R}_{\text{emp}}} % R_emp, empirical risk w/o factor 1 / n
    \newcommand{\riskeb}{\bar{\mathcal{R}}_{\text{emp}}} % R_emp, empirical risk w/ factor 1 / n
    \newcommand{\riskef}{\riske(f)} % R_emp(f)
    \newcommand{\risket}{\mathcal{R}_{\text{emp}}(\thetab)} % R_emp(theta)
    \newcommand{\riskr}{\mathcal{R}_{\text{reg}}} % R_reg, regularized risk
    \newcommand{\riskrt}{\mathcal{R}_{\text{reg}}(\thetab)} % R_reg(theta)
    \newcommand{\riskrf}{\riskr(f)} % R_reg(f)
    \newcommand{\riskrth}{\hat{\mathcal{R}}_{\text{reg}}(\thetab)} % hat R_reg(theta)
    \newcommand{\risketh}{\hat{\mathcal{R}}_{\text{emp}}(\thetab)} % hat R_emp(theta)
    \newcommand{\LL}{\mathcal{L}} % L, likelihood
    \newcommand{\LLt}{\mathcal{L}(\thetab)} % L(theta), likelihood
    \newcommand{\LLtx}{\mathcal{L}(\thetab | \xv)} % L(theta|x), likelihood
    \newcommand{\logl}{\ell} % l, log-likelihood
    \newcommand{\loglt}{\logl(\thetab)} % l(theta), log-likelihood
    \newcommand{\logltx}{\logl(\thetab | \xv)} % l(theta|x), log-likelihood
    \newcommand{\errtrain}{\text{err}_{\text{train}}} % training error
    \newcommand{\errtest}{\text{err}_{\text{test}}} % test error
    \newcommand{\errexp}{\overline{\text{err}_{\text{test}}}} % avg training error
    
    % lm
    \newcommand{\thx}{\thetab^\top \xv} % linear model
    \newcommand{\olsest}{(\Xmat^\top \Xmat)^{-1} \Xmat^\top \yv} % OLS estimator in LM 
    
    	\newcommand{\kopf}[2]{
    	\hrule
    	\vspace{.15cm}
    	\begin{minipage}{\textwidth}
    		%akwardly i had to put \" here to make it compile correctly
    		{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
    			\url{https://slds-lmu.github.io/i2ml/} \hfill #2}
    	\end{minipage}
    	\vspace{.05cm}
    	\hrule
    	\vspace{1cm}}
    
    \geometry{verbose,tmargin=0.5in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
  	\kopf{4}{Supervised Classification}
    
    

    
    \hypertarget{init-import-packages}{%
\subsection*{Init: Import packages}\label{init-import-packages}}



    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k+kn}{import} \PY{n}{CategoricalNB} \PY{c+c1}{\PYZsh{} import Naive Bayes Classifier for categroial distributed features}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{OrdinalEncoder}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{norm}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k+kn}{import} \PY{n}{LinearDiscriminantAnalysis} \PY{k}{as} \PY{n}{LDA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k+kn}{import} \PY{n}{QuadraticDiscriminantAnalysis} \PY{k}{as} \PY{n}{QDA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k+kn}{import} \PY{n}{GaussianNB}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{inspection} \PY{k+kn}{import} \PY{n}{DecisionBoundaryDisplay}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{solution-1-naive-bayes}{%
\subsection*{Solution 1: Naive Bayes}\label{solution-1-naive-bayes}}

    \hypertarget{a}{%
\subsubsection*{a)}\label{a}}


	
	When using the naive Bayes classifier, the features $\xv := 
	(x_\text{Color},x_\text{Form},x_\text{Origin})$ are assumed to be 
	conditionally independent of each other, given the category $y = k \in 
	\{\text{yes}, \text{no}\}$, s.t.
	
	$$ \P(\xv ~|~ y = k) =
	\P((x_\text{Color}, x_\text{Form}, x_\text{Origin}) ~|~ y = k) = 
	\P(x_\text{Color} ~|~ y = k) \cdot \P(x_\text{Form} ~|~ y = k) \cdot 
	\P(x_\text{Origin} ~|~ y = k).$$
	
	Recall Bayes' theorem:
	
	$$\pikx = \postk = \bayesrulek.$$
	
	As the denominator is constant across all classes, the following holds for the 
	posterior probabilities:
	\begin{align*} 
		\pikx \propto & ~ \underbrace{\pik \cdot \P(x_\text{Color} ~|~ y = k)
			\cdot \P(x_\text{Form} ~|~ y = k) \cdot \P(x_\text{Origin} ~|~ y = k)}_{=: 
			\alpha_k(x)} \\
		\iff & \exists c \in \mathbb{R}: \pikx = c \cdot \alpha_k(\xv),
	\end{align*}
	where $\pik = \P(y = k)$ is the prior probability of class $k$ and $c$ is 
	the normalizing constant.
	
	From this and since the posterior probabilities need to sum up to 1, 
	we know that 
	\begin{align*}
		1 = ~ c \cdot \alpha_\text{yes}(\xv) +  c \cdot \alpha_\text{no}(\xv)
		~\iff c = \frac{1}{\alpha_\text{yes}(\xv) + \alpha_\text{no}(\xv)}.
	\end{align*}
	This means that, in order to compute $\pi_\text{yes}(\xv)$, the scores 
	$\alpha_\text{yes}(\xv)$ and $\alpha_\text{no}(\xv)$ are needed.
	\\
	
	Now we want to estimate for a new fruit the posterior probability 
	$\hat{\pi}_{yes}((\text{yellow}, \text{round}, \text{imported}))$.
	
	Obviously, we do not know the \emph{true} prior probability and the 
	\emph{true} conditional densities. 
	Here -- since the target and the features are 
	categorical -- we use a categorical distribution, i.e., the 
	simplest distribution over a $g$-way event that is fully specified by the 
	individual probabilities for each class (which must of course sum to 1).
	This is a generalization of the Bernoulli distribution to the multi-class 
	case.
	We can estimate the distribution parameters via the relative frequencies 
	encountered in the data:
	\begin{align*}
		\hat{\alpha}_\text{yes}(\xv_{\ast}) = 
		& \;  \hat{\pi}_{yes} \cdot 
		% \hat{\P}(\text{yellow} ~|~ y = \text{yes})\cdot \hat{\P}(\text{round} ~|~ y = 
		% \text{yes}) \cdot \hat{\P}(\text{imported} ~|~ y = \text{yes}) \\
		\hat{\P}(x_\text{Color} = \text{yellow} ~|~ y = \text{yes}) \cdot 
		\hat{\P}(x_\text{Form} = \text{round} ~|~ y = \text{yes}) \cdot 
		\hat{\P}(x_\text{Origin} = \text{imported} ~|~ y = \text{yes}) \\
		= & \; \frac{3}{8} \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot 1 = \frac{1}{24} 
		\approx 0.042, \\
		\hat{\alpha}_\text{no}(\xv_{\ast}) = & \;  \hat{\pi}_{no} \cdot 
		% \hat{p}(\text{yellow}|y = \text{no})\cdot \hat{p}(\text{round}|y = \text{no}) 
		% \cdot \hat{p}(\text{imported}|y = \text{no}) \\
		\hat{\P}(x_\text{Color} = \text{yellow} ~|~ y = \text{no})\cdot 
		\hat{\P}(x_\text{Form} = \text{round} ~|~ y = \text{no}) \cdot 
		\hat{\P}(x_\text{Origin} = \text{imported} ~|~ y = \text{no}) \\
		= & \; \frac{5}{8} \cdot \frac{2}{5} \cdot \frac{3}{5} \cdot \frac{2}{5} = 
		\frac{3}{50} = 0.060.
	\end{align*}
	At this stage we can already see that the predicted label is "no", since 
	$\hat{\alpha}_\text{no}(\xv_{\ast}) = 0.060 > \frac{1}{24} = 
	\hat{\alpha}_\text{yes}(\xv_{\ast})$ -- that is, if we threshold at 0.5 for 
	predicting \enquote{yes}.
	
	With the above we can compute the posterior probability
	$$\hat{\pi}_\text{yes}(\xv_{\ast}) = \frac{\hat{\alpha}_\text{yes}(
		\xv_{\ast})}{\hat{\alpha}_\text{yes}(\xv_{\ast}) + 
		\hat{\alpha}_\text{no}(\xv_{\ast})} \approx 0.410 < 0.5,$$
	and check our calculations against the corresponding \texttt{Python} results:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Write dictionary for pandas Data Frame to save Bananas data}
\PY{n}{dic\PYZus{}bananas} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ID}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{]}\PY{p}{,} 
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{brown}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{brown}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Form}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oblong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{round}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oblong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oblong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{round}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{round}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oblong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{round}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Origin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imported}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{domestic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imported}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imported}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{domestic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imported}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{domestic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imported}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bananas}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
\PY{n}{data\PYZus{}banana} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{dic\PYZus{}bananas}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{data\PYZus{}banana}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
   ID   Color    Form    Origin Bananas
0   1  yellow  oblong  imported     yes
1   2  yellow   round  domestic      no
2   3  yellow  oblong  imported      no
3   4   brown  oblong  imported     yes
4   5   brown   round  domestic      no
5   6   green   round  imported     yes
6   7   green  oblong  domestic      no
7   8     red   round  imported      no
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Transform your data with an ordial Encoder to get required input}
\PY{n}{enc} \PY{o}{=} \PY{n}{OrdinalEncoder}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}Initialize Encoder}
\PY{n}{enc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data\PYZus{}banana}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Color}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Form}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Origin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bananas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Fit encoder on needed coulumns}
\PY{c+c1}{\PYZsh{}actually transform data and save in old data frame}
\PY{n}{data\PYZus{}banana}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Color}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Form}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Origin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bananas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{enc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{data\PYZus{}banana}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Color}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Form}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Origin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bananas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{p}{)} 

\PY{c+c1}{\PYZsh{} split the data into inputs and outputs}
\PY{n}{X} \PY{o}{=} \PY{n}{data\PYZus{}banana}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\PY{n}{y} \PY{o}{=} \PY{n}{data\PYZus{}banana}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{o}{.}\PY{n}{values}

\PY{n}{x\PYZus{}new} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{3.}\PY{p}{,}\PY{l+m+mf}{1.}\PY{p}{,}\PY{l+m+mf}{1.}\PY{p}{]}\PY{p}{)}
\PY{n}{x\PYZus{}new} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x\PYZus{}new}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}reshape needed for predict function}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{y}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{x\PYZus{}new}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[3. 0. 1.]
 [3. 1. 0.]
 [3. 0. 1.]
 [0. 0. 1.]
 [0. 1. 0.]
 [1. 1. 1.]
 [1. 0. 0.]
 [2. 1. 1.]]
[1. 0. 0. 1. 0. 1. 0. 0.]
[[3. 1. 1.]]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} initializaing the NB}
\PY{n}{classifer} \PY{o}{=} \PY{n}{CategoricalNB}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} alpha = 0 for no smoothing towards uniform distribution!!}

\PY{c+c1}{\PYZsh{} training the model}
\PY{n}{classifer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}

\PY{c+c1}{\PYZsh{} testing the model}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{classifer}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}new}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Prdection (0 = no, 1 = yes)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Prediction is \PYZdq{}not Banana\PYZdq{}}

\PY{n}{y\PYZus{}prop} \PY{o}{=} \PY{n}{classifer}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{x\PYZus{}new}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Propabilities for (no \PYZhy{} yes)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}prop}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Prdection (0 = no, 1 = yes) [0.]
Propabilities for (no - yes) [[0.59016393 0.40983607]]
    \end{Verbatim}

%    \begin{Verbatim}[commandchars=\\\{\}]
%C:\textbackslash{}Users\textbackslash{}st\_sc\textbackslash{}anaconda3\textbackslash{}envs\textbackslash{}I2ML\_env\textbackslash{}lib\textbackslash{}site-
%packages\textbackslash{}sklearn\textbackslash{}naive\_bayes.py:591: UserWarning: alpha too small will result in
%numeric errors, setting alpha = 1.0e-10
%  warnings.warn(
%    \end{Verbatim}

\subsubsection*{b)}
Before, we only had categorical features and could use the empirical 
frequencies as our parameters in a categorical distribution.
For the distribution of a numerical feature, given the the category, we need 
to define a probability distribution with continuous support.
A popular choice is to use Gaussian distributions.
For example, for the information $x_\text{Length}$ we could assume that 
$$\P(x_\text{Length} ~|~ y = \text{yes}) \sim \normal(\mu_\text{yes}, 
\sigma^2_\text{yes})$$ and $$\P(x_\text{Length} ~|~ y = \text{no}) \sim 
\normal(\mu_\text{no}, \sigma^2_\text{no}).$$ 
In order to fully specify these normal distributions we need to estimate 
their parameters $\mu_\text{yes}, \mu_\text{no}, \sigma^2_\text{yes}, 
\sigma^2_\text{no}$ from the data via the usual estimators 
(empirical mean and empirical variance with bias correction).


    \hypertarget{solution-2-discriminant-analysis}{%
\subsection*{Solution 2: Discriminant
Analysis}\label{solution-2-discriminant-analysis}}

\subsubsection*{a) (i)}
As the data seem to be pretty symmetric conditional on the 
respective class, we estimate the class means to lie roughly in the middle 
of the data clusters: $\hat \mu_1 = 1$, $\hat \mu_2 = 7$, $\hat \mu_3 = 4$.

    \hypertarget{a-ii}{%
\subsubsection*{(ii)}\label{a-ii}}

    We see that the variances in classes 1 and 2 are similar and also much
smaller than in class 3. Therefore, the densities could look roughly
like this:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot differences}

\PY{c+c1}{\PYZsh{}x\PYZhy{}axis ranges from \PYZhy{}3 and 3 with .001 steps}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}

\PY{c+c1}{\PYZsh{}plot normal distribution with mean 0 and standard deviation 1}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class 3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} title \PYZam{} label axes}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distcriminant Analysis:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Density for all classes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{estimated density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{prop}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{10}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    \subsubsection*{(iii)}
    Since LDA assumes constant variances across 
    all classes (also if this does not reflect the data situation), all 
    densities would have the same shape and only differ in location. 
    
    \subsubsection*{(iv)}
    
    As we have already noted, the assumption of equal class variances is 
    not justified here, but LDA is confined to equivariant distributions. 
    Therefore, the more flexible QDA is preferable in this case.
    Note, however, that the Gaussian distributions both variants of discriminant 
    analysis use might not be perfectly appropriate, as the data seems to 
    be more uniformly distributed (conditional on the classes).
    
    \subsubsection*{b)}
    
    The prediction for $\xv_{\ast 1}$ will probably be $\hat z_{\ast 1} = 3$ 
    because the density of class 3 has much larger variance and will therefore 
    overshoot the density of class 1. 
    For $\xv_{\ast 2}$ the case is clear and we have 
    $\hat z_{\ast 2} = 2$.
    
    \hypertarget{exercise-3-decision-boundaries}{%
\subsection*{Exercise 3: Decision
Boundaries}\label{exercise-3-decision-boundaries}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} reading the CSV file}
\PY{n}{cassini} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cassini\PYZus{}data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{43}\PY{p}{)}
\PY{n}{cassini}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x.2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{cassini}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x.2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot data by group}
\PY{n}{groups} \PY{o}{=} \PY{n}{cassini}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{group} \PY{o+ow}{in} \PY{n}{groups}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{group}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x.1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{group}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x.2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{name}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatter Plot:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Cassini Data Set, Color by Classes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Train all 3 models}
\PY{n}{X\PYZus{}cass} \PY{o}{=} \PY{n}{cassini}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\PY{n}{y\PYZus{}cass} \PY{o}{=} \PY{n}{cassini}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{values}

\PY{c+c1}{\PYZsh{} LDA}
\PY{n}{lda} \PY{o}{=} \PY{n}{LDA}\PY{p}{(}\PY{p}{)}
\PY{n}{lda}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{,} \PY{n}{y\PYZus{}cass}\PY{p}{)}

\PY{c+c1}{\PYZsh{} QDA}
\PY{n}{qda} \PY{o}{=} \PY{n}{QDA}\PY{p}{(}\PY{p}{)}
\PY{n}{qda}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{,} \PY{n}{y\PYZus{}cass}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Naive Bayes}
\PY{c+c1}{\PYZsh{} Use Gaussian Naive Bayes}
\PY{n}{gnb} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{n}{var\PYZus{}smoothing}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} no smoothing wanted here}
\PY{n}{gnb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{,} \PY{n}{y\PYZus{}cass}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
GaussianNB(var\_smoothing=0)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot Decision Boundaries }
\PY{k}{def} \PY{n+nf}{plot\PYZus{}dec\PYZus{}bound}\PY{p}{(}\PY{n}{obj}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} }
\PY{l+s+sd}{    Method to produce Decision Boundary Plots }
\PY{l+s+sd}{    Input: trained classifier object}
\PY{l+s+sd}{    Output: \PYZhy{}}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{feature\PYZus{}1}\PY{p}{,} \PY{n}{feature\PYZus{}2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}
        \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{grid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{feature\PYZus{}1}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{feature\PYZus{}2}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}

    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{obj}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{grid}\PY{p}{)}\PY{p}{,} \PY{n}{feature\PYZus{}1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
    \PY{n}{display} \PY{o}{=} \PY{n}{DecisionBoundaryDisplay}\PY{p}{(}\PY{n}{xx0}\PY{o}{=}\PY{n}{feature\PYZus{}1}\PY{p}{,} \PY{n}{xx1}\PY{o}{=}\PY{n}{feature\PYZus{}2}\PY{p}{,} \PY{n}{response}\PY{o}{=}\PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{display}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{)}

    \PY{n}{display}\PY{o}{.}\PY{n}{ax\PYZus{}}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}cass}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}cass}\PY{p}{,} \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
    

\PY{n}{class\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{n}{lda}\PY{p}{,} \PY{n}{qda}\PY{p}{,} \PY{n}{gnb}\PY{p}{]}

\PY{k}{for} \PY{n}{obj} \PY{o+ow}{in} \PY{n}{class\PYZus{}list}\PY{p}{:}
    \PY{n}{plot\PYZus{}dec\PYZus{}bound}\PY{p}{(}\PY{n}{obj}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see how LDA, with its confinement to linear decision boundaries, is
not able to classify the data very well. QDA and NB, on the other hand,
get the shape of the boundaries right. It also becomes obvious that NB
is a quadratic classifier just like QDA - their decision surfaces look
pretty much alike.

    \hypertarget{extra-confusion-matrix-for-multiclass-classifier}{%
\subsection*{Extra: Confusion Matrix for Multiclass
Classifier}\label{extra-confusion-matrix-for-multiclass-classifier}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot confusion matrix for multiclass}
\PY{c+c1}{\PYZsh{} LDA}
\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}cass}\PY{p}{,} \PY{n}{lda}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}cass}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{annot} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,} \PY{n}{xticklabels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y\PYZus{}cass}\PY{p}{)}\PY{p}{,} \PY{n}{yticklabels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y\PYZus{}cass}\PY{p}{)}\PY{p}{,} \PY{n}{cmap} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{summer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Labels}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Labels}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
\end{document}
