We consider the \texttt{BostonHousing} data for which we would like to predict
the nitric oxides concentration (\texttt{nox}) from the distance to a number of 
firms (\texttt{dis}).

<<echo=TRUE, message=FALSE, fig.height=4, fig.width=8>>=

library(mlbench)
data(BostonHousing)
data_pollution <- data.frame(dis = BostonHousing$dis, nox = BostonHousing$nox)
data_pollution <- data_pollution[order(data_pollution$dis), ]
head(data_pollution)

ggplot2::ggplot(data_pollution, ggplot2::aes(x = dis, y = nox)) +
  ggplot2::geom_point() +
  ggplot2::theme_classic()

@

\begin{enumerate}[a)]

  \item Use the first ten observations as training data to compute a linear 
  model with \texttt{mlr3} and evaluate the performance of your learner on the 
  remaining data using MSE.
  
  \item What might be disadvantageous about the train-test split in a)?
  
  \item Now, sample your training observations from the data set at random. 
  Use a share of 0.1 through 0.9, in 0.1 steps, of observations for training 
  and repeat this procedure ten times.
  Afterwards, plot the resulting test errors (in terms of MSE) in a suitable 
  manner. \\
  (Hint: \texttt{rsmp} is a convenient function for splitting data -- 
  you will want to choose the "holdout" strategy. Afterwards, \texttt{resample}
  can be used to repeatedly fit the learner.)
  
  \item Interpret the findings from c).
  
  \item After this empirical experiment we take a look at the mathematical 
  background of the bias-variance trade-off in choosing the split ratio 
  (with no specific assumption about the kind of learner used).
  Consider the expected quadratic error between predictions 
  $\hat y$ and target values $y$, given the training data:
  $$\E_{\Pxy} ((\hat y - y)^2 ~|~ \xv)$$
  and first show that
  $$\E_{\Pxy} ((\hat y - y)^2 ~|~ \xv) = \left(\hat y - \E_{\Pxy}(y ~|~ \xv) 
  \right)^2 + \var_{\Pxy}(y ~|~ \xv).$$
  
  We then go one step further and treat our prediction $\hat y$ as a 
  random variable whose value depends on how training and test observations are
  sampled.
  This sampling process we denote by $\mathcal{S}$.
  Building on the previous exercise, show that
  $$\E_{\mathcal{S}, \Pxy} ((\hat y - y)^2 ~|~ \xv) =
  \left(\E_{\mathcal{S}}(\hat y) - \E_{\Pxy}(y ~|~ \xv) \right)^2 +
  \var_{\mathcal{S}}(\hat y) + \var_{\Pxy}(y ~|~ \xv)$$
  and identify the components that represent bias and error, respectively.
  Can you imagine what the remaining term stands for?
  
\end{enumerate}