We consider the \texttt{BostonHousing} data, for which we would like to predict
the nitric oxides concentration (\texttt{nox}) from the distance to a number of 
firms (\texttt{dis}).

<<echo=TRUE, message=FALSE, fig.height=4, fig.width=8>>=

library(mlbench)
data(BostonHousing)
data_pollution <- data.frame(dis = BostonHousing$dis, nox = BostonHousing$nox)
data_pollution <- data_pollution[order(data_pollution$dis), ]
head(data_pollution)

ggplot2::ggplot(data_pollution, ggplot2::aes(x = dis, y = nox)) +
  ggplot2::geom_point() +
  ggplot2::theme_classic()

@

\begin{enumerate}[a)]
  \item Use the first 20 observations as training data to compute a linear 
  model with \texttt{mlr3} and evaluate the performance of your learner on the 
  remaining data using MSE.
  \item What might be disadvantageous about the train-test split in a)?
  \item Now, sample your training observations from the data set at random. 
  Use a share of 0.1 through 0.9, in 0.1 steps, of observations for training 
  and repeat this procedure ten times.
  Afterwards, plot the resulting test errors (in terms of MSE) in a suitable 
  manner.
  
  (Hint: \texttt{rsmp} is a convenient function for splitting data -- 
  you will want to choose the "holdout" strategy. Afterwards, \texttt{resample}
  can be used to repeatedly fit the learner.)
  \item Interpret the findings from c).
\end{enumerate}