---
title: "Exercise 1 -- ML Basics"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

## Exercise 1: Car Price Prediction

::: {.callout-note title="Learning goals" icon=false}
TBD
:::

Imagine you work at a second-hand car dealer and are tasked with finding 
for-sale vehicles your company can acquire at a reasonable price. You decide to 
address this challenge in a data-driven manner and develop a model that predicts 
adequate market prices (in EUR) from vehicles’ properties.

1. Characterize the task at hand: supervised or unsupervised? Regression or 
   classification? Learning to explain or learning to predict? Justify your 
   answers. **[only for lecture group B]**
2. How would you set up your data? Name potential features along with their 
   respective data type and state the target variable.
3. Assume now that you have data on vehicles’ age (days), mileage (km), and 
   price (EUR). 
   Explicitly define the feature space $\mathcal{X}$ and target space $\mathcal{Y}$.
4. You choose to use a linear model (LM) for this task.
   The LM models the target as a linear function of the features 
   with Gaussian error term.
   
   State the hypothesis space for the corresponding model class.
   For this, assume the parameter vector $\theta$ to include the intercept 
   coefficient.
5. Which parameters need to be learned?
   Define the corresponding parameter space $\Theta$.
6. State the loss function for the $i$-th observation using $L2$ loss. 
7. Now you need to optimize this risk to find the best parameters, 
   and hence the best model, via empirical risk minimization. 
   State the optimization problem formally and list the necessary steps to solve 
   it. 

Congratulations, you just designed your first machine learning project!

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
<!-- ::: {.panel-tabset} -->
1. We face a **supervised regression** task: we definitely need 
   labeled training data to infer a relationship between cars' attributes and 
   their prices, and price in EUR is a continuous target (or quasi-continuous, 
   to be exact -- as with all other quantities, we can only measure it with 
   finite precision, but the scale is sufficiently fine-grained to assume 
   continuity). **Prediction** is definitely the goal here, however, it 
   might also be interesting to examine the explanatory contribution of each 
   feature.
  
2. Target variable and potential features: 
  
   | **Variable**        | **Role**   | **Data type** |
   |---------------------|------------|---------------|
   | Price in EUR        | Target     | Numeric       |
   | Age in days         | Feature    | Numeric       |
   | Mileage in km       | Feature    | Numeric       |
   | Brand               | Feature    | Categorical   |
   | Accident-free y/n   | Feature    | Binary        |
   | ...                 | ...        | ...           |
  
3. Let $x_1$ and $x_2$ measure age and mileage, respectively. 
   Both features and target are numeric and (quasi-) continuous. It is also 
   reasonable to assume non-negativity for the features, such that we 
   obtain $\mathcal{X} = (\mathbb{R}_{0}^{+})^2$, with $\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})^\top 
   \in \mathcal{X}$ for $i = 1, 2, \dots, n$ observations. 
   As the standard LM does not impose any 
   restrictions on the target, we have $\mathcal{Y} = \mathbb{R}$, though we would probably 
   discard negative predictions in practice.
  
4. We can write the hypothesis space as:
   
   $$\mathcal{H} = \{f(\mathbf{x}) = \theta^\top \mathbf{x} ~|~ \theta \in \mathbb{R}^3 \}
   =  \{f(\mathbf{x}) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 ~|~ 
   (\theta_0, \theta_1, \theta_2) \in \mathbb{R}^3 \}.$$
   
   Note the **slight abuse of notation** here: in the lecture, we first 
   define $\theta$ to only consist of the feature coefficients, with $\mathbf{x}$ 
   likewise being the plain feature vector. For the sake of simplicity, however, 
   it is more convenient to append the intercept coefficient to the vector of 
   feature coefficients. This does not change our model formulation, but we
</details> 
:::

## Exercise 2: Vector Calculus [only for lecture group A]

::: {.callout-note title="Learning goals" icon=false}
TBD
:::

Consider the following function performing matrix-vector multiplication: $f(\mathbf{x}) = \mathbf{A} \mathbf{x}$, where $\mathbf{A} \in \mathbb{R}^{m \times n}$, $\mathbf{x} \in \mathbb{R}^{n \times 1}$.

1. What is the dimension of $f(\mathbf{x})$? Explicitly state the calculation for the $i$-th component of $f(\mathbf{x})$.
2. Now, consider the gradient (derivative generalized to multivariate functions) $\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}}$ (a.k.a. $\nabla_{\mathbf{x}} f(\mathbf{x})$).
   a. What is the dimension of $\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}}$?
   b. Compute $\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}}$.
  
::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
1. In computing $\mathbf{A} \mathbf{x}$ we multiply each of the $m$ rows in $\mathbf{A}$ with the sole length-$n$ column in $\mathbf{x}$, leaving us with a column vector $f(\mathbf{x}) \in \mathbb{R}^{m \times 1}$. Thus, we have $f: \mathbb{R}^{n (\times 1)} \rightarrow \mathbb{R}^{m (\times 1)}$.
   
   The $i$-th function component $f_i(\mathbf{x})$ corresponds to multiplying the $i$-th row of $\mathbf{A}$ with $\mathbf{x}$, amounting to $$f_i(\mathbf{x}) = \sum_{j = 1}^n a_{ij} x_j,$$ with $a_{ij}$ the element in the $i$-row, $j$-th column of $\mathbf{A}$.
   
2. 
   a. The gradient is the row vector[^1] of partial derivatives, i.e., the derivatives of $f$ w.r.t. each dimension of $\mathbf{x}$:
      $$\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}} 
      = \begin{bmatrix}\frac{\partial f(\mathbf{x})}{\partial x_1} & \dots & \frac{\partial f(\mathbf{x})}{\partial x_n}\end{bmatrix}.
      $$
      Now, since $f$ is a vector-valued function, each component is itself a vector of length $m$.
      Therefore, we have $\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}} \in \mathbb{R}^{m \times n}$, given by the collection of all partial derivatives of each function component:
      $$
      \frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}} = \begin{bmatrix}
      \frac{\partial f_1(\mathbf{x})}{\partial x_1} & \cdots & \frac{\partial f_1(\mathbf{x})}{\partial x_n} \\
      \vdots & & \vdots \\
      \frac{\partial f_m(\mathbf{x})}{\partial x_1} & \cdots & \frac{\partial f_m(\mathbf{x})}{\partial x_n}
      \end{bmatrix}
      $$
      This matrix is also called the *Jacobian* of $f$.
   b. We have $$\frac{\partial f_i(\mathbf{x})}{\partial x_j} = \frac{\partial \left(\sum_{j = 1}^n a_{ij} x_j \right)}{\partial x_j} = a_{ij}.$$
      Doing this for every element yields
      $$
      \begin{bmatrix}a_{11} & \cdots & a_{1n}  \\ \vdots & & \vdots \\ a_{m1} & \cdots & a_{mn}\end{bmatrix},
      $$
      and we have $\frac{\mathrm{d} f(\mathbf{x})}{\mathrm{d} \mathbf{x}} = \frac{\mathrm{d} \mathbf{A} \mathbf{x}}{\mathrm{d} \mathbf{x}} = \mathbf{A}$.

[^1]: Pertaining to one of two conventions; we use the *numerator layout* here (the transposed version is called *denominator layout*).

For more explanations and exercises, including a useful collection of rules for calculus, we recommend the book "Mathematics for Machine Learning" ([https://mml-book.github.io/book/mml-book.pdf](https://mml-book.github.io/book/mml-book.pdf)).
</details> 
:::
