\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}


\input{../../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopficsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfaml}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Advanced Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}


\newcommand{\kopfdive}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Deep Dive\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[2]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Supervised Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/i2ml/} \hfill #2}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\\noindent}
	{\bigskip}

\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../../latex-math/basic-math.tex}
\input{../../../latex-math/basic-ml.tex}

\kopf{2}{Supervised Regression}

\aufgabe{HRO in \texttt{sklearn}}{

Throughout the lecture, we will frequently use the \texttt{Python} package 
\texttt{sklearn} and its descendants, providing an integrated ecosystem for all 
common machine learning tasks.
Let's recap the HRO principle and see how it is reflected in \texttt{sklearn}.
An overview of the most important objects and their usage, illustrated with 
numerous examples, can be found at 
\url{https://scikit-learn.org/stable/index.html}.

\begin{enumerate}[a)]
  \item How are the key concepts (i.e., hypothesis space, risk and optimization) 
  you learned about in the lecture videos implemented in \texttt{sklearn}?
  \item Have a look at the function \texttt{from sklearn.datasets import load\_iris}. What attributes does this resulting \texttt{utils.Bunch} object store?
  \item Pick an \texttt{sklearn} module for classification or regression of your choice. What are the different settings for this learner? \\ 
  (Hint: Import the specific module and use \texttt{get\_params()} to see all available 
  settings.)
\end{enumerate}
}

\dlz

\aufgabe{Loss Functions for Regression Tasks}{

In this exercise, we will examine loss functions for regression tasks 
somewhat more in depth.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\includegraphics[width=\maxwidth]{figure/unnamed-chunk-5-1} \hfill{}


\end{knitrout}

\begin{enumerate}[a)]
  \item Consider the above linear regression task. How will the model 
  parameters be affected by adding the new outlier point (orange) if you use
  \begin{enumerate}[i)]
    \item $L1$ loss
    \item $L2$ loss
  \end{enumerate}
  in the empirical risk? (You do not need to actually compute the 
  parameter values.)
\end{enumerate}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\includegraphics[width=\maxwidth]{figure/unnamed-chunk-6-1} \hfill{}


\end{knitrout}

\begin{itemize}
  \item[b)] The second plot visualizes another loss function popular in 
  regression tasks, the so-called \textit{Huber loss} (depending on 
  $\epsilon > 0$; here: $\epsilon = 5$). 
  Describe how the Huber loss deals with residuals as compared to $L1$ and $L2$ 
  loss.
  Can you guess its definition? 
  % \item[c)] \textcolor{orange}{Zu schwer?}
  % Show that $\mathit{median}(y)$ is the optimal constant prediction $c$
  % when using $L1$ loss. \\
  % Hint:
  % \begin{itemize}
  %   \item Employing the law of total expectation, we can find $c$ via
  %   $$\argmin_c \E\left[|y - c|\right] = \argmin_c
  %   \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y =
  %   \argmin_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty
  %   (y - c)~p(y)~\text{d}y .$$
  %   \item Setting the derivative of this expression, found by application
  %   of Leibniz's rule, to zero yields
  %   $$0 \overset{^\text{!}}{=} \int_{-\infty}^c  ~p(y)~\text{d}y -
  %   \int_c^\infty ~p(y)~\text{d}y.$$
  % \end{itemize}
  \item[c)] Derive the least-squares estimator, i.e., the solution to the linear 
  model when using $L2$ loss, analytically via
  $$\thetabh = \argmint \| \yv - \Xmat \thetab \|_2^2.$$
\end{itemize}

}

\dlz

\aufgabe{Polynomial Regression}{

Assume the following (noisy) data-generating process from which we have 
observed 50 realizations: $$y = -3 + 5 \cdot 
\sin(0.4 \pi x) + \epsilon$$ with $\epsilon \, \sim \mathcal{N}(0, 1)$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\includegraphics[width=\maxwidth]{figure/unnamed-chunk-7-1} \hfill{}


\end{knitrout}

\begin{enumerate}[a)]
  \item We decide to model the data with a cubic polynomial (including intercept 
  term). State the corresponding hypothesis space.
  \item Demonstrate that this hypothesis space is simply a parameterized family 
  of curves by plotting in \texttt{Python} curves for 3 different models belonging to 
  the considered model class.
  \item State the empirical risk w.r.t. $\thetab$ for a member of the hypothesis 
  space. Use $L2$ loss and be as explicit as possible.
  \item We can minimize this risk using gradient descent. In order to make 
  this somewhat easier, we will denote the transformed feature matrix, 
  containing $x$ to the power from 0 to 3, by $\tilde \Xmat$, such that we can 
  express our model by $\tilde \Xmat \thetab$ (note that the model is still 
  linear in its parameters, even if $\Xmat$ has been transformed in a non-linear 
  manner!). Derive the gradient of the empirical risk w.r.t $\thetab$.
  \item Using the result from d), state the calculation to update the current 
  parameter $\thetat$.
  \item You will not be able to fit the data perfectly with a cubic polynomial.
  Describe the advantages and disadvantages that a more 
  flexible model class would have. 
  Would you opt for a more flexible learner?
\end{enumerate}
}

\dlz

\aufgabe{Predicting \texttt{abalone}}{

We want to predict the age of an abalone using its longest shell measurement and 
its weight.

See \url{https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/} for more details.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
import pandas as pd

url = \hlstr{"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"}
abalone = \hlkwd{pd.read_csv}(url, sep=\hlstr{","}, 
      names=[\hlstr{"sex"}, \hlstr{"longest_shell"}, \hlstr{"diameter"}, \hlstr{"height"}, \hlstr{"whole_weight"}, 
             \hlstr{"shucked_weight"}, \hlstr{"visceral_weight"}, \hlstr{"shell_weight"}, \hlstr{"rings"}])

abalone = abalone[[\hlstr{"longest_shell"}, \hlstr{"whole_weight"}, \hlstr{"rings"}]]
\end{alltt}
\end{kframe}
\end{knitrout}



\begin{itemize}
  \item[a)] Plot \texttt{LongestShell} and \texttt{WholeWeight} on the $x$- 
  and $y$-axis, respectively, and color points according to \texttt{Rings}.
\end{itemize}

Using \texttt{sklearn}:

\begin{itemize}
  \item[b)] Initiate a linear regression learner (for this you will need to import 
  the \texttt{from sklearn.linear\_model import LinearRegression} extension package first) and use it to train a 
  linear model on the \texttt{abalone} data. 
  \item[c)] Compare the fitted and observed targets visually. \\
  (Hint: use \texttt{import matplotlib.pyplot as plt}.)
  \item[d)] Assess the model's training loss in terms of MAE. \\
  (Hint: The MAE metric is retrieved by calling \texttt{from sklearn.metrics import mean\_absolute\_error}.)
\end{itemize}

\vspace{1cm}
\includegraphics[width=0.5\textwidth]{figure/abalone}

\scriptsize{\url{https://en.wikipedia.org/wiki/Abalone#/media/File:LivingAbalone.JPG}}
}
\end{document}
